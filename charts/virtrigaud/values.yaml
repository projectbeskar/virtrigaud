# Default values for virtrigaud
# This is a YAML-formatted file.

# Global configuration
global:
  # Image registry for all images
  imageRegistry: ghcr.io
  # Image pull policy
  imagePullPolicy: IfNotPresent
  # Image pull secrets
  imagePullSecrets: []

# Manager configuration
manager:
  # Image configuration
  image:
    repository: projectbeskar/virtrigaud/manager
    tag: "v0.2.0"
    pullPolicy: IfNotPresent

  # Replica count for manager
  replicaCount: 1

  # Resource limits and requests
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Security context (secure defaults)
  securityContext:
    runAsNonRoot: true
    runAsUser: 65532
    runAsGroup: 65532
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    seccompProfile:
      type: RuntimeDefault

  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 65532
    runAsGroup: 65532
    fsGroup: 65532
    fsGroupChangePolicy: "OnRootMismatch"
    seccompProfile:
      type: RuntimeDefault

  # Service account
  serviceAccount:
    create: true
    annotations: {}
    name: ""

  # Node selector
  nodeSelector: {}

  # Tolerations
  tolerations: []

  # Affinity
  affinity: {}

  # Environment variables
  env:
    - name: VIRTRIGAUD_LOG_LEVEL
      value: "info"
    - name: VIRTRIGAUD_LOG_FORMAT
      value: "json"
    - name: VIRTRIGAUD_TRACING_ENABLED
      value: "false"

  # Additional volumes
  volumes: []

  # Additional volume mounts
  volumeMounts: []

# Provider configuration
providers:
  # Libvirt provider
  libvirt:
    enabled: true
    replicaCount: 1
    image:
      repository: projectbeskar/virtrigaud/provider-libvirt
      tag: "v0.2.0"
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 256Mi

  # vSphere provider  
  vsphere:
    enabled: true
    replicaCount: 1
    image:
      repository: projectbeskar/virtrigaud/provider-vsphere
      tag: "v0.2.0"
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 256Mi

  # Proxmox provider
  proxmox:
    enabled: false
    replicaCount: 1
    image:
      repository: projectbeskar/virtrigaud/provider-proxmox
      tag: "v0.2.0"
    resources:
      limits:
        cpu: 1000m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 256Mi
    
    # ⚠️  REQUIRED: Proxmox VE server configuration
    # The Proxmox provider requires connection to an active Proxmox VE server.
    # Configure the environment variables below with your Proxmox server details.
    # 
    # For detailed setup instructions, see: docs/providers/proxmox.md
    #
    # Example configuration:
    # env:
    #   - name: PVE_ENDPOINT
    #     value: "https://your-proxmox-server.example.com:8006"
    #   - name: PVE_USERNAME
    #     value: "root@pam"  # or "user@realm"
    #   - name: PVE_PASSWORD
    #     value: "your-password"
    #   - name: PVE_TOKEN_ID      # Alternative to password authentication
    #     value: "user@realm!tokenid"
    #   - name: PVE_TOKEN_SECRET  # Use with PVE_TOKEN_ID
    #     value: "token-secret-value"
    #   - name: PVE_INSECURE_SKIP_VERIFY  # For self-signed certificates
    #     value: "true"
    env: []

# RBAC configuration
rbac:
  # Enable RBAC
  create: true
  # Scope: namespace or cluster (cluster required for CRDs)
  scope: cluster
  # Additional rules
  additionalRules: []

# Security configuration
security:
  # Pod Security Profile: strict, baseline, or privileged
  podSecurityProfile: strict
  
  # Network policies
  networkPolicies:
    enabled: true
    # Ingress rules
    ingress:
      # Allow manager to provider communication
      managerToProvider: true
      # Allow external access to webhooks
      webhookIngress: true
    # Egress rules  
    egress:
      # Allow provider to hypervisor communication
      providerToHypervisor: true
      # Allow DNS resolution
      dns: true
      # Allow access to Kubernetes API
      kubernetesAPI: true

  # TLS/mTLS configuration
  tls:
    # Enable TLS for provider gRPC
    enabled: true
    # Enable mutual TLS
    mTLS: true
    # Certificate source: cert-manager, manual, or self-signed
    source: self-signed
    # Certificate secret names (when source is manual)
    secrets:
      manager: virtrigaud-manager-tls
      providers: virtrigaud-provider-tls
    # cert-manager configuration (when source is cert-manager)
    certManager:
      issuer: virtrigaud-ca-issuer
      issuerKind: ClusterIssuer

# Webhook configuration
webhooks:
  # Enable admission webhooks
  enabled: false
  
  # Conversion webhook for CRD versioning
  conversion:
    enabled: true
    
  # Validating webhook
  validating:
    enabled: true
    failurePolicy: Fail
    
  # Mutating webhook  
  mutating:
    enabled: true
    failurePolicy: Fail

  # Certificate configuration
  certificates:
    # Certificate source: cert-manager, manual, or self-signed
    source: self-signed
    # Certificate secret name
    secretName: virtrigaud-webhook-certs
    # cert-manager configuration
    certManager:
      issuer: virtrigaud-ca-issuer
      issuerKind: ClusterIssuer

# Observability configuration
observability:
  # Prometheus monitoring
  prometheus:
    enabled: false
    # Create ServiceMonitor
    serviceMonitor:
      enabled: false
      interval: 30s
      scrapeTimeout: 10s
      labels: {}
      
    # Create PrometheusRule for alerts
    prometheusRule:
      enabled: false
      labels: {}
      
  # Grafana dashboards
  grafana:
    enabled: false
    # Create ConfigMaps with dashboards
    dashboards:
      enabled: false
      labels:
        grafana_dashboard: "1"
        
  # OpenTelemetry tracing
  tracing:
    enabled: false
    endpoint: ""
    samplingRatio: 0.1

# Autoscaling
autoscaling:
  # Manager autoscaling
  manager:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    
  # Provider autoscaling  
  providers:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # Custom metrics for provider scaling
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: virtrigaud_provider_tasks_inflight
          target:
            type: AverageValue
            averageValue: "10"

# Configuration
config:
  # Feature gates
  featureGates: []
  
  # Worker configuration
  workers:
    perKind: 2
    maxInflightTasks: 100
    
  # Retry configuration
  retry:
    maxAttempts: 5
    baseDelay: 500ms
    maxDelay: 30s
    
  # Circuit breaker configuration
  circuitBreaker:
    failureThreshold: 10
    resetTimeout: 60s
    
  # Rate limiting
  rateLimit:
    qps: 10
    burst: 20

# CRD installation
crds:
  # Install CRDs
  install: true
  # Keep CRDs on uninstall
  keep: false

# Testing
testing:
  # Conformance testing
  conformance:
    enabled: false
    # Test configuration
    config:
      providers: ["libvirt", "vsphere"]
      skipTests: []
      
  # Load testing  
  loadTesting:
    enabled: false
    # Load test configuration
    config:
      vmCount: 10
      concurrency: 2
      duration: 300s

# CRD Upgrade configuration
# Automatically upgrades CRDs during helm upgrade (not just install)
crdUpgrade:
  # Enable automatic CRD upgrade via Helm hooks
  # When enabled, CRDs will be applied during both 'helm install' and 'helm upgrade'
  enabled: true
  
  # Image for kubectl container that applies CRDs  
  # Using our own kubectl image built from Alpine with kubectl and shell
  # This image is built and published as part of the VirtRigaud release process
  # Note: During development, this defaults to the current version, but in production
  #       it will be updated to match the release tag
  image:
    repository: ghcr.io/projectbeskar/virtrigaud/kubectl
    tag: "v0.2.0"  # Updated during release to match VirtRigaud version
    pullPolicy: IfNotPresent
  
  # Job configuration
  backoffLimit: 3  # Number of retries before considering job failed
  ttlSecondsAfterFinished: 300  # Cleanup job after 5 minutes
  waitSeconds: 5  # Wait time after applying CRDs for them to be established
  
  # Resources for the CRD upgrade job
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
