<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>VirtRigaud Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="Kubernetes operator for managing virtual machines across multiple hypervisors">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item affix "><a href="index.html">Introduction</a></li><li class="chapter-item affix "><li class="part-title">Getting Started</li><li class="chapter-item "><a href="getting-started/quickstart.html"><strong aria-hidden="true">1.</strong> 15-Minute Quickstart</a></li><li class="chapter-item "><a href="install-helm-only.html"><strong aria-hidden="true">2.</strong> Installation Guide</a></li><li class="chapter-item "><a href="HELM_CRD_UPGRADES.html"><strong aria-hidden="true">3.</strong> Helm CRD Upgrades</a></li><li class="chapter-item affix "><li class="part-title">Core Documentation</li><li class="chapter-item "><a href="CRDs.html"><strong aria-hidden="true">4.</strong> Custom Resource Definitions</a></li><li class="chapter-item "><a href="EXAMPLES.html"><strong aria-hidden="true">5.</strong> Examples</a></li><li class="chapter-item "><a href="PROVIDERS.html"><strong aria-hidden="true">6.</strong> Provider Documentation</a></li><li class="chapter-item "><a href="PROVIDERS_CAPABILITIES.html"><strong aria-hidden="true">7.</strong> Provider Capabilities Matrix</a></li><li class="chapter-item affix "><li class="part-title">Provider-Specific Guides</li><li class="chapter-item "><a href="providers/vsphere.html"><strong aria-hidden="true">8.</strong> vSphere Provider</a></li><li class="chapter-item "><a href="providers/libvirt.html"><strong aria-hidden="true">9.</strong> Libvirt Provider</a></li><li class="chapter-item "><a href="providers/proxmox.html"><strong aria-hidden="true">10.</strong> Proxmox VE Provider</a></li><li class="chapter-item "><a href="providers/tutorial.html"><strong aria-hidden="true">11.</strong> Provider Tutorial</a></li><li class="chapter-item "><a href="providers/versioning.html"><strong aria-hidden="true">12.</strong> Provider Versioning</a></li><li class="chapter-item affix "><li class="part-title">Advanced Features</li><li class="chapter-item "><a href="ADVANCED_LIFECYCLE.html"><strong aria-hidden="true">13.</strong> VM Lifecycle Management</a></li><li class="chapter-item "><a href="NESTED_VIRTUALIZATION.html"><strong aria-hidden="true">14.</strong> Nested Virtualization</a></li><li class="chapter-item "><a href="GRACEFUL_SHUTDOWN.html"><strong aria-hidden="true">15.</strong> Graceful Shutdown</a></li><li class="chapter-item "><a href="REMOTE_PROVIDERS.html"><strong aria-hidden="true">16.</strong> Remote Providers</a></li><li class="chapter-item affix "><li class="part-title">Operations & Administration</li><li class="chapter-item "><a href="OBSERVABILITY.html"><strong aria-hidden="true">17.</strong> Observability</a></li><li class="chapter-item "><a href="SECURITY.html"><strong aria-hidden="true">18.</strong> Security</a></li><li class="chapter-item "><a href="RESILIENCE.html"><strong aria-hidden="true">19.</strong> Resilience</a></li><li class="chapter-item "><a href="UPGRADE.html"><strong aria-hidden="true">20.</strong> Upgrade Guide</a></li><li class="chapter-item "><a href="VSPHERE_HARDWARE_VERSION.html"><strong aria-hidden="true">21.</strong> vSphere Hardware Versions</a></li><li class="chapter-item affix "><li class="part-title">Security Configuration</li><li class="chapter-item "><a href="providers/security/bearer-token.html"><strong aria-hidden="true">22.</strong> Bearer Token Authentication</a></li><li class="chapter-item "><a href="providers/security/mtls.html"><strong aria-hidden="true">23.</strong> mTLS Configuration</a></li><li class="chapter-item "><a href="providers/security/external-secrets.html"><strong aria-hidden="true">24.</strong> External Secrets</a></li><li class="chapter-item "><a href="providers/security/network-policies.html"><strong aria-hidden="true">25.</strong> Network Policies</a></li><li class="chapter-item affix "><li class="part-title">API Reference</li><li class="chapter-item "><a href="CLI.html"><strong aria-hidden="true">26.</strong> CLI Tools Reference</a></li><li class="chapter-item "><a href="api-reference/cli.html"><strong aria-hidden="true">27.</strong> CLI API Reference</a></li><li class="chapter-item "><a href="api-reference/metrics.html"><strong aria-hidden="true">28.</strong> Metrics Catalog</a></li><li class="chapter-item "><a href="catalog.html"><strong aria-hidden="true">29.</strong> Provider Catalog</a></li><li class="chapter-item affix "><li class="part-title">Development</li><li class="chapter-item "><a href="TESTING_WORKFLOWS_LOCALLY.html"><strong aria-hidden="true">30.</strong> Testing Workflows Locally</a></li><li class="chapter-item "><a href="../../CONTRIBUTING.html"><strong aria-hidden="true">31.</strong> Contributing</a></li><li class="chapter-item affix "><li class="part-title">Examples</li><li class="chapter-item "><a href="examples/index.html"><strong aria-hidden="true">32.</strong> Example README</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">VirtRigaud Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/projectbeskar/virtrigaud" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="virtrigaud-documentation"><a class="header" href="#virtrigaud-documentation">VirtRigaud Documentation</a></h1>
<p>Welcome to the VirtRigaud documentation. VirtRigaud is a Kubernetes operator for managing virtual machines across multiple hypervisors including vSphere, Libvirt/KVM, and Proxmox VE.</p>
<h2 id="quick-navigation"><a class="header" href="#quick-navigation">Quick Navigation</a></h2>
<h3 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h3>
<ul>
<li><a href="getting-started/quickstart.html">15-Minute Quickstart</a> - Get up and running quickly</li>
<li><a href="install-helm-only.html">Installation Guide</a> - Helm installation instructions</li>
<li><a href="HELM_CRD_UPGRADES.html">Helm CRD Upgrades</a> - Managing CRD updates</li>
</ul>
<h3 id="core-documentation"><a class="header" href="#core-documentation">Core Documentation</a></h3>
<ul>
<li><a href="CRDs.html">Custom Resource Definitions</a> - Complete API reference</li>
<li><a href="EXAMPLES.html">Examples</a> - Practical configuration examples</li>
<li><a href="CLOUD_INIT.html">Cloud-Init Configuration</a> - UserData and MetaData guide</li>
<li><a href="PROVIDERS.html">Provider Documentation</a> - Provider development guide</li>
<li><a href="PROVIDERS_CAPABILITIES.html">Provider Capabilities Matrix</a> - Feature comparison</li>
</ul>
<h3 id="provider-specific-guides"><a class="header" href="#provider-specific-guides">Provider-Specific Guides</a></h3>
<ul>
<li><a href="providers/vsphere.html">vSphere Provider</a> - VMware vCenter/ESXi integration</li>
<li><a href="providers/libvirt.html">Libvirt Provider</a> - KVM/QEMU virtualization</li>
<li><a href="providers/proxmox.html">Proxmox VE Provider</a> - Proxmox Virtual Environment</li>
<li><a href="providers/tutorial.html">Provider Tutorial</a> - Build your own provider</li>
<li><a href="providers/versioning.html">Provider Versioning</a> - Version management</li>
</ul>
<h3 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h3>
<ul>
<li><a href="ADVANCED_LIFECYCLE.html">VM Lifecycle Management</a> - Advanced VM operations</li>
<li><a href="VM_ADOPTION.html">VM Adoption</a> - Onboard existing VMs into VirtRigaud</li>
<li><a href="NESTED_VIRTUALIZATION.html">Nested Virtualization</a> - Run hypervisors in VMs</li>
<li><a href="GRACEFUL_SHUTDOWN.html">Graceful Shutdown</a> - Proper VM shutdown handling</li>
<li><a href="ADVANCED_LIFECYCLE.html#snapshots">VM Snapshots</a> - Backup and restore</li>
<li><a href="REMOTE_PROVIDERS.html">Remote Providers</a> - Provider architecture</li>
</ul>
<h3 id="operations--administration"><a class="header" href="#operations--administration">Operations &amp; Administration</a></h3>
<ul>
<li><a href="OBSERVABILITY.html">Observability</a> - Monitoring and metrics</li>
<li><a href="SECURITY.html">Security</a> - Security best practices</li>
<li><a href="RESILIENCE.html">Resilience</a> - High availability and fault tolerance</li>
<li><a href="UPGRADE.html">Upgrade Guide</a> - Version upgrade procedures</li>
<li><a href="VSPHERE_HARDWARE_VERSION.html">vSphere Hardware Versions</a> - Hardware compatibility</li>
</ul>
<h3 id="security-configuration"><a class="header" href="#security-configuration">Security Configuration</a></h3>
<ul>
<li><a href="providers/security/bearer-token.html">Bearer Token Authentication</a></li>
<li><a href="providers/security/mtls.html">mTLS Configuration</a></li>
<li><a href="providers/security/external-secrets.html">External Secrets</a></li>
<li><a href="providers/security/network-policies.html">Network Policies</a></li>
</ul>
<h3 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h3>
<ul>
<li><a href="CLI.html">CLI Tools Reference</a> - Command-line interface guide</li>
<li><a href="api-reference/cli.html">CLI API Reference</a> - Detailed CLI documentation</li>
<li><a href="api-reference/metrics.html">Metrics Catalog</a> - Available metrics</li>
<li><a href="catalog.html">Provider Catalog</a> - Available providers</li>
</ul>
<h3 id="development"><a class="header" href="#development">Development</a></h3>
<ul>
<li><a href="TESTING_WORKFLOWS_LOCALLY.html">Testing Workflows Locally</a> - Local CI/CD testing</li>
<li><a href="../CONTRIBUTING.html">Contributing</a> - Contribution guidelines</li>
<li><a href="../DEVELOPMENT.html">Development Guide</a> - Developer setup</li>
</ul>
<h3 id="examples-directory"><a class="header" href="#examples-directory">Examples Directory</a></h3>
<ul>
<li><a href="examples/README.html">Example README</a> - Overview of all examples</li>
<li><a href="examples/">Complete Examples</a> - Working configuration files</li>
<li><a href="examples/advanced/">Advanced Examples</a> - Complex scenarios</li>
<li><a href="examples/security/">Security Examples</a> - Security configurations</li>
</ul>
<h2 id="version-information"><a class="header" href="#version-information">Version Information</a></h2>
<p>This documentation covers <strong>VirtRigaud v0.2.3</strong>.</p>
<h3 id="recent-changes"><a class="header" href="#recent-changes">Recent Changes</a></h3>
<ul>
<li><strong>v0.2.3</strong>: Provider feature parity - Reconfigure, Clone, TaskStatus, ConsoleURL</li>
<li><strong>v0.2.2</strong>: Nested virtualization, TPM support, snapshot management</li>
<li><strong>v0.2.1</strong>: Critical fixes and documentation updates</li>
<li><strong>v0.2.0</strong>: Production-ready vSphere and Libvirt providers</li>
</ul>
<p>See <a href="../CHANGELOG.html">CHANGELOG.md</a> for complete version history.</p>
<h2 id="provider-status"><a class="header" href="#provider-status">Provider Status</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Status</th><th>Maturity</th><th>Documentation</th></tr></thead><tbody>
<tr><td>vSphere</td><td>Production Ready</td><td>Stable</td><td><a href="providers/vsphere.html">Guide</a></td></tr>
<tr><td>Libvirt/KVM</td><td>Production Ready</td><td>Stable</td><td><a href="providers/libvirt.html">Guide</a></td></tr>
<tr><td>Proxmox VE</td><td>Production Ready</td><td>Beta</td><td><a href="providers/proxmox.html">Guide</a></td></tr>
<tr><td>Mock</td><td>Complete</td><td>Testing</td><td><a href="PROVIDERS.html">PROVIDERS.md</a></td></tr>
</tbody></table>
</div>
<h2 id="support"><a class="header" href="#support">Support</a></h2>
<ul>
<li><strong>GitHub Issues</strong>: <a href="https://github.com/projectbeskar/virtrigaud/issues">github.com/projectbeskar/virtrigaud/issues</a></li>
<li><strong>Discussions</strong>: <a href="https://github.com/projectbeskar/virtrigaud/discussions">github.com/projectbeskar/virtrigaud/discussions</a></li>
<li><strong>Slack</strong>: #virtrigaud on Kubernetes Slack</li>
</ul>
<h2 id="quick-links"><a class="header" href="#quick-links">Quick Links</a></h2>
<ul>
<li><a href="../README.html">Main README</a> - Project overview</li>
<li><a href="../CHANGELOG.html">CHANGELOG</a> - Version history</li>
<li><a href="../CONTRIBUTING.html">Contributing</a> - How to contribute</li>
<li><a href="../LICENSE">License</a> - Apache License 2.0</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="15-minute-quickstart"><a class="header" href="#15-minute-quickstart">15-Minute Quickstart</a></h1>
<p>This guide will get you up and running with VirtRigaud in 15 minutes using both vSphere and Libvirt providers.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li>Kubernetes cluster (1.24+)</li>
<li>kubectl configured</li>
<li>Helm 3.x</li>
<li>Access to a vSphere environment (optional)</li>
<li>Access to a Libvirt/KVM host (optional)</li>
</ul>
<h2 id="api-support"><a class="header" href="#api-support">API Support</a></h2>
<p><strong>Default API</strong>: v1beta1 - The recommended stable API for all new deployments.</p>
<p><strong>Legacy API</strong>: v1beta1 - Served for compatibility but deprecated. See the <a href="getting-started/../upgrade/">upgrade guide</a> for migration instructions.</p>
<p>All resources support seamless conversion between API versions via webhooks.</p>
<h2 id="step-1-install-virtrigaud"><a class="header" href="#step-1-install-virtrigaud">Step 1: Install VirtRigaud</a></h2>
<h3 id="using-helm-recommended"><a class="header" href="#using-helm-recommended">Using Helm (Recommended)</a></h3>
<pre><code class="language-bash"># Add the VirtRigaud Helm repository
helm repo add virtrigaud https://projectbeskar.github.io/virtrigaud
helm repo update

# Install with default settings (CRDs included automatically)
helm install virtrigaud virtrigaud/virtrigaud \
  --namespace virtrigaud-system \
  --create-namespace

# Or install with specific providers enabled
helm install virtrigaud virtrigaud/virtrigaud \
  --namespace virtrigaud-system \
  --create-namespace \
  --set providers.vsphere.enabled=true \
  --set providers.libvirt.enabled=true

# To skip CRDs if already installed separately
helm install virtrigaud virtrigaud/virtrigaud \
  --namespace virtrigaud-system \
  --create-namespace \
  --skip-crds
</code></pre>
<h3 id="using-kustomize"><a class="header" href="#using-kustomize">Using Kustomize</a></h3>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud

# Apply base installation
kubectl apply -k deploy/kustomize/base

# Or apply with overlays
kubectl apply -k deploy/kustomize/overlays/standard
</code></pre>
<h2 id="step-2-verify-installation"><a class="header" href="#step-2-verify-installation">Step 2: Verify Installation</a></h2>
<pre><code class="language-bash"># Check that the manager is running
kubectl get pods -n virtrigaud-system

# Check CRDs are installed
kubectl get crds | grep virtrigaud

# Verify API conversion is working (v1beta1 &lt;-&gt; v1beta1)
kubectl get crd virtualmachines.infra.virtrigaud.io -o yaml | yq '.spec.conversion'

# Check manager logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-manager
</code></pre>
<h2 id="step-3-configure-a-provider"><a class="header" href="#step-3-configure-a-provider">Step 3: Configure a Provider</a></h2>
<h3 id="option-a-vsphere-provider"><a class="header" href="#option-a-vsphere-provider">Option A: vSphere Provider</a></h3>
<p>Create a secret with vSphere credentials:</p>
<pre><code class="language-bash">kubectl create secret generic vsphere-credentials \
  --namespace default \
  --from-literal=endpoint=https://vcenter.example.com \
  --from-literal=username=administrator@vsphere.local \
  --from-literal=password=your-password \
  --from-literal=insecure=false
</code></pre>
<p>Create a vSphere provider:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-prod
  namespace: default
spec:
  type: vsphere
  endpoint: https://vcenter.example.com
  credentialSecretRef:
    name: vsphere-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.3"
    service:
      port: 9090
  defaults:
    datastore: "datastore1"
    cluster: "cluster1"
    folder: "virtrigaud-vms"
</code></pre>
<h3 id="option-b-libvirt-provider"><a class="header" href="#option-b-libvirt-provider">Option B: Libvirt Provider</a></h3>
<p>Create a secret with Libvirt connection details:</p>
<pre><code class="language-bash">kubectl create secret generic libvirt-credentials \
  --namespace default \
  --from-literal=uri=qemu+ssh://root@libvirt-host.example.com/system \
  --from-literal=username=root \
  --from-literal=privateKey="$(cat ~/.ssh/id_rsa)"
</code></pre>
<p>Create a Libvirt provider:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-lab
  namespace: default
spec:
  type: libvirt
  endpoint: qemu+ssh://root@libvirt-host.example.com/system
  credentialSecretRef:
    name: libvirt-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.0"
    service:
      port: 9090
  defaults:
    defaultStoragePool: "default"
    defaultNetwork: "default"
</code></pre>
<p>Apply the provider configuration:</p>
<pre><code class="language-bash">kubectl apply -f provider.yaml
</code></pre>
<blockquote>
<p>üí° <strong>Behind the scenes</strong>: VirtRigaud automatically converts your Provider resource into the appropriate command-line arguments, environment variables, and secret mounts for the provider pod. See the <a href="getting-started/../REMOTE_PROVIDERS.html#configuration-flow-provider-resource--provider-pod">configuration flow documentation</a> for complete details.</p>
</blockquote>
<h2 id="step-4-create-a-vm-class"><a class="header" href="#step-4-create-a-vm-class">Step 4: Create a VM Class</a></h2>
<p>Define resource templates for your VMs:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: small
  namespace: default
spec:
  cpu: 2
  memoryMiB: 2048
  disks:
  - name: root
    sizeGiB: 20
    type: thin
  networks:
  - name: default
    type: "VM Network"  # vSphere network name
</code></pre>
<pre><code class="language-bash">kubectl apply -f vmclass.yaml
</code></pre>
<h2 id="step-5-create-a-vm-image"><a class="header" href="#step-5-create-a-vm-image">Step 5: Create a VM Image</a></h2>
<p>Define the base image for your VMs:</p>
<h3 id="vsphere-image-ova"><a class="header" href="#vsphere-image-ova">vSphere Image (OVA)</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-20-04
  namespace: virtrigaud-system
spec:
  source:
    vsphere:
      ovaURL: "https://cloud-images.ubuntu.com/releases/20.04/ubuntu-20.04-server-cloudimg-amd64.ova"
      checksum: "sha256:abc123..."
      datastore: "datastore1"
      folder: "vm-templates"
  prepare:
    onMissing: Import
    timeout: "30m"
</code></pre>
<h3 id="libvirt-image-qcow2"><a class="header" href="#libvirt-image-qcow2">Libvirt Image (qcow2)</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-20-04
  namespace: virtrigaud-system
spec:
  source:
    libvirt:
      qcow2URL: "https://cloud-images.ubuntu.com/releases/20.04/ubuntu-20.04-server-cloudimg-amd64.img"
      checksum: "sha256:def456..."
      storagePool: "default"
  prepare:
    onMissing: Import
    timeout: "30m"
</code></pre>
<pre><code class="language-bash">kubectl apply -f vmimage.yaml
</code></pre>
<h2 id="step-6-create-your-first-vm"><a class="header" href="#step-6-create-your-first-vm">Step 6: Create Your First VM</a></h2>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: my-first-vm
  namespace: default
spec:
  providerRef:
    name: vsphere-prod  # or libvirt-lab
    namespace: default
  classRef:
    name: small
    namespace: default
  imageRef:
    name: ubuntu-20-04
    namespace: default
  powerState: "On"
  userData:
    cloudInit:
      inline: |
        #cloud-config
        users:
          - name: ubuntu
            sudo: ALL=(ALL) NOPASSWD:ALL
            ssh_authorized_keys:
              - ssh-rsa AAAAB3... your-public-key
        packages:
          - curl
          - vim
  networks:
  - name: default
    networkRef:
      name: default-network
      namespace: default
</code></pre>
<pre><code class="language-bash">kubectl apply -f vm.yaml
</code></pre>
<h2 id="step-7-monitor-vm-creation"><a class="header" href="#step-7-monitor-vm-creation">Step 7: Monitor VM Creation</a></h2>
<pre><code class="language-bash"># Watch VM status
kubectl get vm my-first-vm -w

# Check detailed status
kubectl describe vm my-first-vm

# View events
kubectl get events --field-selector involvedObject.name=my-first-vm

# Check provider logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-provider-vsphere
</code></pre>
<h2 id="step-8-access-your-vm"><a class="header" href="#step-8-access-your-vm">Step 8: Access Your VM</a></h2>
<pre><code class="language-bash"># Get VM IP address
kubectl get vm my-first-vm -o jsonpath='{.status.ips[0]}'

# Get console URL (if supported)
kubectl get vm my-first-vm -o jsonpath='{.status.consoleURL}'

# SSH to the VM (once it has an IP)
ssh ubuntu@&lt;vm-ip&gt;
</code></pre>
<h2 id="step-9-try-advanced-operations"><a class="header" href="#step-9-try-advanced-operations">Step 9: Try Advanced Operations</a></h2>
<h3 id="create-a-snapshot"><a class="header" href="#create-a-snapshot">Create a Snapshot</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSnapshot
metadata:
  name: my-vm-snapshot
  namespace: default
spec:
  vmRef:
    name: my-first-vm
  nameHint: "pre-update-snapshot"
  memory: true
</code></pre>
<h3 id="clone-the-vm"><a class="header" href="#clone-the-vm">Clone the VM</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClone
metadata:
  name: my-vm-clone
  namespace: default
spec:
  sourceRef:
    name: my-first-vm
  target:
    name: cloned-vm
    classRef:
      name: small
      namespace: default
  linked: true
</code></pre>
<h3 id="scale-with-vmset"><a class="header" href="#scale-with-vmset">Scale with VMSet</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSet
metadata:
  name: web-servers
  namespace: default
spec:
  replicas: 3
  template:
    spec:
      providerRef:
        name: vsphere-prod
        namespace: default
      classRef:
        name: small
        namespace: default
      imageRef:
        name: ubuntu-20-04
        namespace: default
      powerState: "On"
</code></pre>
<h2 id="step-10-clean-up"><a class="header" href="#step-10-clean-up">Step 10: Clean Up</a></h2>
<pre><code class="language-bash"># Delete VM
kubectl delete vm my-first-vm

# Delete snapshots and clones
kubectl delete vmsnapshot my-vm-snapshot
kubectl delete vmclone my-vm-clone
kubectl delete vmset web-servers

# Uninstall VirtRigaud (optional)
helm uninstall virtrigaud -n virtrigaud-system
kubectl delete namespace virtrigaud-system
</code></pre>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li>Browse <a href="getting-started/../examples/">Complete Examples</a> for production-ready configurations</li>
<li>Explore the <a href="getting-started/../user-guides/virtual-machines.html">VM Lifecycle Guide</a></li>
<li>Learn about <a href="getting-started/../user-guides/networking.html">Advanced Networking</a></li>
<li>Set up <a href="getting-started/../admin-guides/monitoring.html">Monitoring and Observability</a></li>
<li>Configure <a href="getting-started/../admin-guides/security.html">Security and RBAC</a></li>
<li>Read the <a href="getting-started/../REMOTE_PROVIDERS.html">Remote Providers Documentation</a></li>
<li>Read the <a href="getting-started/../developer/provider-guide.html">Provider Development Guide</a></li>
</ul>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<p>If you encounter issues:</p>
<ol>
<li>Check the <a href="getting-started/../admin-guides/troubleshooting.html">Troubleshooting Guide</a></li>
<li>Verify your provider credentials and connectivity</li>
<li>Check the manager and provider logs</li>
<li>Ensure your Kubernetes cluster meets the requirements</li>
<li>File an issue on <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-only-installation--verify-conversion"><a class="header" href="#helm-only-installation--verify-conversion">Helm-only Installation &amp; Verify Conversion</a></h1>
<p>This guide covers installing virtrigaud using only Helm (without pre-applying CRDs via Kustomize) and verifying that API conversion is working correctly.</p>
<h2 id="helm-only-install"><a class="header" href="#helm-only-install">Helm-only Install</a></h2>
<p>VirtRigaud can be installed using only Helm, which will automatically install all required CRDs including conversion webhook configuration.</p>
<h3 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h3>
<ul>
<li>Kubernetes cluster (1.26+)</li>
<li>Helm 3.8+</li>
<li><code>kubectl</code> configured to access your cluster</li>
</ul>
<h3 id="installation"><a class="header" href="#installation">Installation</a></h3>
<pre><code class="language-bash"># Add the virtrigaud Helm repository
helm repo add virtrigaud https://projectbeskar.github.io/virtrigaud
helm repo update

# Or install directly from source
git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud

# Install virtrigaud with CRDs
helm install virtrigaud charts/virtrigaud \
  --namespace virtrigaud \
  --create-namespace \
  --wait \
  --timeout 10m
</code></pre>
<h1 id="or-install-directly-from-source"><a class="header" href="#or-install-directly-from-source">Or install directly from source</a></h1>
<p>git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud
helm install virtrigaud charts/virtrigaud <br />
‚Äìnamespace virtrigaud <br />
‚Äìcreate-namespace <br />
‚Äìwait <br />
‚Äìtimeout 10m</p>
<pre><code>
### Skip CRDs (if already installed)

If you need to install the chart without CRDs (e.g., they're managed separately):

```bash
helm install virtrigaud charts/virtrigaud \
  --namespace virtrigaud \
  --create-namespace \
  --skip-crds \
  --wait
</code></pre>
<h2 id="verify-conversion"><a class="header" href="#verify-conversion">Verify Conversion</a></h2>
<p>After installation, verify that API conversion is working correctly.</p>
<h3 id="check-crd-conversion-configuration"><a class="header" href="#check-crd-conversion-configuration">Check CRD Conversion Configuration</a></h3>
<pre><code class="language-bash"># Verify all CRDs have conversion webhook configuration
kubectl get crd virtualmachines.infra.virtrigaud.io -o yaml | yq '.spec.conversion'
</code></pre>
<p>Expected output:</p>
<pre><code class="language-yaml">strategy: Webhook
webhook:
  clientConfig:
    service:
      name: virtrigaud-webhook
      namespace: virtrigaud
      path: /convert
  conversionReviewVersions:
  - v1
</code></pre>
<h3 id="check-api-versions"><a class="header" href="#check-api-versions">Check API Versions</a></h3>
<p>Verify that both v1beta1 and v1beta1 versions are available:</p>
<pre><code class="language-bash"># Check available versions for VirtualMachine CRD
kubectl get crd virtualmachines.infra.virtrigaud.io -o jsonpath='{.spec.versions[*].name}' | tr ' ' '\n'
</code></pre>
<p>Expected output:</p>
<pre><code>v1beta1
v1beta1
</code></pre>
<h3 id="verify-storage-version"><a class="header" href="#verify-storage-version">Verify Storage Version</a></h3>
<p>Confirm that v1beta1 is set as the storage version:</p>
<pre><code class="language-bash"># Check storage version
kubectl get crd virtualmachines.infra.virtrigaud.io -o jsonpath='{.spec.versions[?(@.storage==true)].name}'
</code></pre>
<p>Expected output:</p>
<pre><code>v1beta1
</code></pre>
<h3 id="test-conversion"><a class="header" href="#test-conversion">Test Conversion</a></h3>
<p>Create resources using different API versions and verify conversion works:</p>
<pre><code class="language-bash"># Create a VM using v1beta1 API
cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: test-vm-alpha
  namespace: default
spec:
  providerRef:
    name: test-provider
  classRef:
    name: small
  imageRef:
    name: ubuntu-22
  powerState: "On"
EOF

# Read it back as v1beta1
kubectl get vm test-vm-alpha -o yaml | grep "apiVersion:"
# Should show: apiVersion: infra.virtrigaud.io/v1beta1

# Create a VM using v1beta1 API
cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: test-vm-beta
  namespace: default
spec:
  providerRef:
    name: test-provider
  classRef:
    name: small
  imageRef:
    name: ubuntu-22
  powerState: On
EOF

# Clean up test resources
kubectl delete vm test-vm-alpha test-vm-beta
</code></pre>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="conversion-webhook-missing"><a class="header" href="#conversion-webhook-missing">Conversion Webhook Missing</a></h3>
<p>If the conversion webhook is missing or not configured:</p>
<pre><code class="language-bash"># Check if webhook service exists
kubectl get svc virtrigaud-webhook -n virtrigaud

# Check webhook pod logs
kubectl logs -l app.kubernetes.io/name=virtrigaud -n virtrigaud

# Verify webhook certificate
kubectl get secret virtrigaud-webhook-certs -n virtrigaud
</code></pre>
<h3 id="conversion-webhook-failing"><a class="header" href="#conversion-webhook-failing">Conversion Webhook Failing</a></h3>
<p>If conversion is failing:</p>
<pre><code class="language-bash"># Check conversion webhook logs
kubectl logs -l app.kubernetes.io/name=virtrigaud -n virtrigaud | grep conversion

# Test webhook connectivity
kubectl get --raw "/api/v1/namespaces/virtrigaud/services/virtrigaud-webhook:webhook/proxy/convert"

# Check webhook certificate validity
kubectl get secret virtrigaud-webhook-certs -n virtrigaud -o yaml
</code></pre>
<h3 id="api-version-issues"><a class="header" href="#api-version-issues">API Version Issues</a></h3>
<p>If certain API versions aren‚Äôt working:</p>
<pre><code class="language-bash"># List all available APIs
kubectl api-resources | grep virtrigaud

# Check specific CRD status
kubectl describe crd virtualmachines.infra.virtrigaud.io

# Verify controller is running
kubectl get pods -l app.kubernetes.io/name=virtrigaud -n virtrigaud
</code></pre>
<h2 id="integration-with-gitops"><a class="header" href="#integration-with-gitops">Integration with GitOps</a></h2>
<h3 id="argocd"><a class="header" href="#argocd">ArgoCD</a></h3>
<pre><code class="language-yaml">apiVersion: argoproj.io/v1beta1
kind: Application
metadata:
  name: virtrigaud
spec:
  source:
    chart: virtrigaud
    repoURL: https://projectbeskar.github.io/virtrigaud
    targetRevision: "1.0.0"
    helm:
      values: |
        manager:
          image:
            repository: ghcr.io/projectbeskar/virtrigaud/manager
            tag: v1.0.0
</code></pre>
<h3 id="flux"><a class="header" href="#flux">Flux</a></h3>
<pre><code class="language-yaml">apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: virtrigaud
spec:
  chart:
    spec:
      chart: virtrigaud
      sourceRef:
        kind: HelmRepository
        name: virtrigaud
      version: "1.0.0"
  values:
    manager:
      image:
        repository: ghcr.io/projectbeskar/virtrigaud/manager
        tag: v1.0.0
</code></pre>
<h2 id="migration-from-kustomize-to-helm"><a class="header" href="#migration-from-kustomize-to-helm">Migration from Kustomize to Helm</a></h2>
<p>If you‚Äôre currently using Kustomize for CRD management and want to switch to Helm:</p>
<ol>
<li>
<p><strong>Backup existing resources:</strong></p>
<pre><code class="language-bash">kubectl get vms,providers,vmclasses -A -o yaml &gt; virtrigaud-backup.yaml
</code></pre>
</li>
<li>
<p><strong>Uninstall Kustomize-managed CRDs (optional):</strong></p>
<pre><code class="language-bash">kubectl delete -k config/default
</code></pre>
</li>
<li>
<p><strong>Install via Helm:</strong></p>
<pre><code class="language-bash">helm install virtrigaud charts/virtrigaud --namespace virtrigaud --create-namespace
</code></pre>
</li>
<li>
<p><strong>Restore resources:</strong></p>
<pre><code class="language-bash">kubectl apply -f virtrigaud-backup.yaml
</code></pre>
</li>
</ol>
<p>The conversion webhook will handle any necessary API version transformations automatically.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automatic-crd-upgrades-in-virtrigaud-helm-chart"><a class="header" href="#automatic-crd-upgrades-in-virtrigaud-helm-chart">Automatic CRD Upgrades in VirtRigaud Helm Chart</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>VirtRigaud Helm chart now supports <strong>automatic CRD upgrades</strong> during <code>helm upgrade</code>. This eliminates the need for manual CRD management and provides a seamless upgrade experience.</p>
<h2 id="the-problem"><a class="header" href="#the-problem">The Problem</a></h2>
<p>By default, Helm has a limitation:</p>
<ul>
<li>CRDs are installed during <code>helm install</code></li>
<li>CRDs are <strong>NOT upgraded</strong> during <code>helm upgrade</code></li>
</ul>
<p>This means users had to manually apply CRD updates before upgrading, which was:</p>
<ul>
<li>Error-prone</li>
<li>Easy to forget</li>
<li>Breaks GitOps workflows</li>
<li>Causes version drift between chart and CRDs</li>
</ul>
<h2 id="the-solution"><a class="header" href="#the-solution">The Solution</a></h2>
<p>VirtRigaud uses <strong>Helm Hooks</strong> with a Kubernetes Job to automatically apply CRDs during both install and upgrade:</p>
<h3 id="kubectl-image"><a class="header" href="#kubectl-image">kubectl Image</a></h3>
<p>VirtRigaud builds and publishes its own <code>kubectl</code> image as part of the release process. This image:</p>
<ul>
<li>Based on Alpine Linux for minimal size (~50MB)</li>
<li>Includes kubectl 1.32.0 binary from official Kubernetes releases</li>
<li>Includes bash and shell for scripting support</li>
<li>Runs as non-root user (UID 65532)</li>
<li>Verified with SHA256 checksums</li>
<li>Signed with Cosign and includes SBOM</li>
<li>Security scanned but uses official kubectl binary (vulnerabilities tracked upstream)</li>
</ul>
<p>The image is automatically built and tagged to match each VirtRigaud release version, ensuring version consistency across all components.</p>
<p><strong>Image Location</strong>: <code>ghcr.io/projectbeskar/virtrigaud/kubectl:&lt;version&gt;</code></p>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<ol>
<li><strong>Pre-Upgrade Hook</strong>: Before the main upgrade starts, a Job is created</li>
<li><strong>CRD Application</strong>: The Job applies all CRDs using <code>kubectl apply --server-side</code></li>
<li><strong>Safe Upgrades</strong>: Server-side apply handles conflicts gracefully</li>
<li><strong>Automatic Cleanup</strong>: Job is deleted after successful completion</li>
</ol>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<pre><code>helm upgrade virtrigaud
    ‚Üì
[Pre-Upgrade Hook -10]
    ‚Üì
ConfigMap with CRDs created
    ‚Üì
[Pre-Upgrade Hook -5]
    ‚Üì
ServiceAccount + RBAC created
    ‚Üì
[Pre-Upgrade Hook 0]
    ‚Üì
Job applies CRDs via kubectl
    ‚Üì
[Standard Helm Resources]
    ‚Üì
Manager &amp; Providers deployed
    ‚Üì
[Hook Cleanup]
    ‚Üì
Job &amp; Hook resources deleted
</code></pre>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<h3 id="enabled-by-default"><a class="header" href="#enabled-by-default">Enabled by Default</a></h3>
<p>No configuration needed - just works:</p>
<pre><code class="language-bash">helm upgrade virtrigaud virtrigaud/virtrigaud -n virtrigaud-system
</code></pre>
<h3 id="server-side-apply"><a class="header" href="#server-side-apply">Server-Side Apply</a></h3>
<p>Uses <code>kubectl apply --server-side</code> for:</p>
<ul>
<li>Safe conflict resolution</li>
<li>Field management</li>
<li>No ownership conflicts</li>
</ul>
<h3 id="gitops-compatible"><a class="header" href="#gitops-compatible">GitOps Compatible</a></h3>
<p>Works seamlessly with:</p>
<ul>
<li><strong>ArgoCD</strong>: Helm hooks execute properly</li>
<li><strong>Flux</strong>: Compatible with HelmRelease CRD upgrades</li>
<li><strong>Terraform</strong>: Helm provider handles hooks</li>
</ul>
<h3 id="configurable"><a class="header" href="#configurable">Configurable</a></h3>
<p>Customize the upgrade behavior:</p>
<pre><code class="language-yaml">crdUpgrade:
  enabled: true  # Enable/disable automatic upgrades
  
  image:
    repository: ghcr.io/projectbeskar/virtrigaud/kubectl  # VirtRigaud kubectl image
    tag: "v0.2.0"  # Auto-updated to match release version
  
  backoffLimit: 3
  ttlSecondsAfterFinished: 300
  waitSeconds: 5
  
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
</code></pre>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="standard-upgrade-automatic-crds"><a class="header" href="#standard-upgrade-automatic-crds">Standard Upgrade (Automatic CRDs)</a></h3>
<pre><code class="language-bash"># CRDs are automatically upgraded
helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system
</code></pre>
<h3 id="disable-automatic-crd-upgrade"><a class="header" href="#disable-automatic-crd-upgrade">Disable Automatic CRD Upgrade</a></h3>
<pre><code class="language-bash"># Disable if you manage CRDs separately
helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system \
  --set crdUpgrade.enabled=false
</code></pre>
<h3 id="manual-crd-management"><a class="header" href="#manual-crd-management">Manual CRD Management</a></h3>
<pre><code class="language-bash"># Apply CRDs manually before upgrade
kubectl apply -f charts/virtrigaud/crds/

# Then upgrade without CRD management
helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system \
  --set crdUpgrade.enabled=false
</code></pre>
<h3 id="skip-crds-entirely"><a class="header" href="#skip-crds-entirely">Skip CRDs Entirely</a></h3>
<pre><code class="language-bash"># Skip CRDs during upgrade (for external CRD management)
helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system \
  --skip-crds \
  --set crdUpgrade.enabled=false
</code></pre>
<h2 id="gitops-integration"><a class="header" href="#gitops-integration">GitOps Integration</a></h2>
<h3 id="argocd-1"><a class="header" href="#argocd-1">ArgoCD</a></h3>
<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: virtrigaud
spec:
  source:
    chart: virtrigaud
    targetRevision: 0.2.2
    helm:
      values: |
        crdUpgrade:
          enabled: true  # Automatic upgrades work!
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
</code></pre>
<p><strong>Note</strong>: ArgoCD executes Helm hooks properly, so CRDs will be upgraded automatically.</p>
<h3 id="flux-1"><a class="header" href="#flux-1">Flux</a></h3>
<pre><code class="language-yaml">apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: virtrigaud
spec:
  chart:
    spec:
      chart: virtrigaud
      version: 0.2.2
  values:
    crdUpgrade:
      enabled: true  # Automatic upgrades work!
  install:
    crds: CreateReplace
  upgrade:
    crds: CreateReplace
</code></pre>
<p><strong>Note</strong>: Flux‚Äôs <code>crds: CreateReplace</code> works alongside our hook-based upgrades for maximum compatibility.</p>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="check-crd-upgrade-job"><a class="header" href="#check-crd-upgrade-job">Check CRD Upgrade Job</a></h3>
<pre><code class="language-bash"># View job status
kubectl get jobs -n virtrigaud-system -l app.kubernetes.io/component=crd-upgrade

# View job logs
kubectl logs -n virtrigaud-system -l app.kubernetes.io/component=crd-upgrade

# View job details
kubectl describe job -n virtrigaud-system -l app.kubernetes.io/component=crd-upgrade
</code></pre>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<h4 id="1-rbac-permissions"><a class="header" href="#1-rbac-permissions">1. RBAC Permissions</a></h4>
<p><strong>Symptom</strong>: Job fails with ‚Äúforbidden‚Äù errors</p>
<p><strong>Solution</strong>: Ensure the ServiceAccount has CRD permissions:</p>
<pre><code class="language-bash">kubectl get clusterrole -l app.kubernetes.io/component=crd-upgrade
kubectl describe clusterrole &lt;role-name&gt;
</code></pre>
<h4 id="2-image-pull-failures"><a class="header" href="#2-image-pull-failures">2. Image Pull Failures</a></h4>
<p><strong>Symptom</strong>: Job fails to start, ImagePullBackOff</p>
<p><strong>Solution</strong>: Check image configuration:</p>
<pre><code class="language-yaml">crdUpgrade:
  image:
    repository: ghcr.io/projectbeskar/virtrigaud/kubectl
    tag: "v0.2.2-rc1"  # Use matching VirtRigaud version
    pullPolicy: IfNotPresent
</code></pre>
<h4 id="3-crd-conflicts"><a class="header" href="#3-crd-conflicts">3. CRD Conflicts</a></h4>
<p><strong>Symptom</strong>: Apply errors about field conflicts</p>
<p><strong>Solution</strong>: Server-side apply handles this automatically, but you can force:</p>
<pre><code class="language-bash">kubectl apply --server-side=true --force-conflicts -f charts/virtrigaud/crds/
</code></pre>
<h4 id="4-job-not-cleaning-up"><a class="header" href="#4-job-not-cleaning-up">4. Job Not Cleaning Up</a></h4>
<p><strong>Symptom</strong>: Old jobs remain after upgrade</p>
<p><strong>Solution</strong>: Adjust TTL or manually clean:</p>
<pre><code class="language-bash">kubectl delete jobs -n virtrigaud-system -l app.kubernetes.io/component=crd-upgrade
</code></pre>
<h3 id="debug-mode"><a class="header" href="#debug-mode">Debug Mode</a></h3>
<p>Enable verbose logging:</p>
<pre><code class="language-bash">helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system \
  --debug
</code></pre>
<h2 id="migration-guide"><a class="header" href="#migration-guide">Migration Guide</a></h2>
<h3 id="migrating-from-manual-crd-management"><a class="header" href="#migrating-from-manual-crd-management">Migrating from Manual CRD Management</a></h3>
<p>If you were previously managing CRDs manually:</p>
<ol>
<li>
<p><strong>Enable automatic upgrades</strong>:</p>
<pre><code class="language-bash">helm upgrade virtrigaud virtrigaud/virtrigaud \
  -n virtrigaud-system \
  --set crdUpgrade.enabled=true
</code></pre>
</li>
<li>
<p><strong>Verify CRDs are upgraded</strong>:</p>
<pre><code class="language-bash">kubectl get crd -l app.kubernetes.io/name=virtrigaud
</code></pre>
</li>
<li>
<p><strong>Remove manual steps from your upgrade process</strong></p>
</li>
</ol>
<h3 id="migrating-to-external-crd-management"><a class="header" href="#migrating-to-external-crd-management">Migrating to External CRD Management</a></h3>
<p>If you want to manage CRDs externally (e.g., separate Helm chart):</p>
<ol>
<li>
<p><strong>Disable automatic upgrades</strong>:</p>
<pre><code class="language-yaml">crdUpgrade:
  enabled: false
</code></pre>
</li>
<li>
<p><strong>Extract CRDs</strong>:</p>
<pre><code class="language-bash">helm show crds virtrigaud/virtrigaud &gt; my-crds.yaml
</code></pre>
</li>
<li>
<p><strong>Manage CRDs separately</strong>:</p>
<pre><code class="language-bash">kubectl apply -f my-crds.yaml
</code></pre>
</li>
</ol>
<h2 id="technical-details"><a class="header" href="#technical-details">Technical Details</a></h2>
<h3 id="hook-weights"><a class="header" href="#hook-weights">Hook Weights</a></h3>
<p>The upgrade process uses weighted hooks for proper ordering:</p>
<div class="table-wrapper"><table><thead><tr><th>Weight</th><th>Resource</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>-10</code></td><td>ConfigMap</td><td>Store CRD content</td></tr>
<tr><td><code>-5</code></td><td>RBAC</td><td>Create permissions</td></tr>
<tr><td><code>0</code></td><td>Job</td><td>Apply CRDs</td></tr>
</tbody></table>
</div>
<h3 id="resource-requirements"><a class="header" href="#resource-requirements">Resource Requirements</a></h3>
<p>The CRD upgrade job is lightweight:</p>
<pre><code class="language-yaml">resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 50m
    memory: 64Mi
</code></pre>
<h3 id="security"><a class="header" href="#security">Security</a></h3>
<ul>
<li>Runs as non-root user (65532)</li>
<li>Read-only root filesystem</li>
<li>No privilege escalation</li>
<li>Minimal RBAC (only CRD permissions)</li>
<li>Automatic cleanup after completion</li>
</ul>
<h3 id="compatibility"><a class="header" href="#compatibility">Compatibility</a></h3>
<ul>
<li><strong>Kubernetes</strong>: 1.25+</li>
<li><strong>Helm</strong>: 3.8+</li>
<li><strong>kubectl</strong>: 1.24+ (in Job image)</li>
</ul>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li><strong>Use Automatic Upgrades</strong>: Enable by default for best UX</li>
<li><strong>Monitor Job Logs</strong>: Check logs during first upgrade</li>
<li><strong>Test in Dev First</strong>: Verify upgrades in non-production</li>
<li><strong>Backup CRDs</strong>: Keep backups before major upgrades</li>
<li><strong>Review Changelogs</strong>: Check for breaking CRD changes</li>
</ol>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<h3 id="q-will-this-delete-my-existing-resources"><a class="header" href="#q-will-this-delete-my-existing-resources">Q: Will this delete my existing resources?</a></h3>
<p><strong>A</strong>: No. CRD upgrades are additive and preserve existing Custom Resources.</p>
<h3 id="q-what-happens-if-the-job-fails"><a class="header" href="#q-what-happens-if-the-job-fails">Q: What happens if the job fails?</a></h3>
<p><strong>A</strong>: Helm upgrade will fail, leaving your cluster in the previous state. Fix the issue and retry.</p>
<h3 id="q-can-i-use-this-with-argocd"><a class="header" href="#q-can-i-use-this-with-argocd">Q: Can I use this with ArgoCD?</a></h3>
<p><strong>A</strong>: Yes! ArgoCD properly executes Helm hooks.</p>
<h3 id="q-does-this-work-with-flux"><a class="header" href="#q-does-this-work-with-flux">Q: Does this work with Flux?</a></h3>
<p><strong>A</strong>: Yes! Flux HelmRelease handles hooks correctly.</p>
<h3 id="q-how-do-i-roll-back"><a class="header" href="#q-how-do-i-roll-back">Q: How do I roll back?</a></h3>
<p><strong>A</strong>: Use <code>helm rollback</code>. CRDs are not rolled back (Kubernetes limitation).</p>
<h3 id="q-can-i-customize-the-kubectl-image"><a class="header" href="#q-can-i-customize-the-kubectl-image">Q: Can I customize the kubectl image?</a></h3>
<p><strong>A</strong>: Yes, via <code>crdUpgrade.image.repository</code> and <code>crdUpgrade.image.tag</code>. The default uses the official Kubernetes kubectl image from <code>registry.k8s.io</code>.</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://helm.sh/docs/topics/charts_hooks/">Helm Hooks Documentation</a></li>
<li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Kubernetes CRD Documentation</a></li>
<li><a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-Side Apply</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-resource-definitions-crds"><a class="header" href="#custom-resource-definitions-crds">Custom Resource Definitions (CRDs)</a></h1>
<p>This document describes all the Custom Resource Definitions (CRDs) provided by virtrigaud.</p>
<h2 id="virtualmachine"><a class="header" href="#virtualmachine">VirtualMachine</a></h2>
<p>The <code>VirtualMachine</code> CRD represents a virtual machine instance.</p>
<h3 id="spec"><a class="header" href="#spec">Spec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>providerRef</code></td><td><code>ObjectRef</code></td><td>Yes</td><td>Reference to the Provider resource</td></tr>
<tr><td><code>classRef</code></td><td><code>ObjectRef</code></td><td>Yes</td><td>Reference to the VMClass resource</td></tr>
<tr><td><code>imageRef</code></td><td><code>ObjectRef</code></td><td>Yes</td><td>Reference to the VMImage resource</td></tr>
<tr><td><code>networks</code></td><td><code>[]VMNetworkRef</code></td><td>No</td><td>Network attachments</td></tr>
<tr><td><code>disks</code></td><td><code>[]DiskSpec</code></td><td>No</td><td>Additional disks</td></tr>
<tr><td><code>userData</code></td><td><code>UserData</code></td><td>No</td><td>Cloud-init configuration</td></tr>
<tr><td><code>metaData</code></td><td><code>MetaData</code></td><td>No</td><td>Cloud-init metadata configuration</td></tr>
<tr><td><code>placement</code></td><td><code>Placement</code></td><td>No</td><td>Placement hints</td></tr>
<tr><td><code>powerState</code></td><td><code>string</code></td><td>No</td><td>Desired power state (On/Off)</td></tr>
<tr><td><code>tags</code></td><td><code>[]string</code></td><td>No</td><td>Tags for organization</td></tr>
</tbody></table>
</div>
<h3 id="status"><a class="header" href="#status">Status</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><code>id</code></td><td><code>string</code></td><td>Provider-specific VM identifier</td></tr>
<tr><td><code>powerState</code></td><td><code>string</code></td><td>Current power state</td></tr>
<tr><td><code>ips</code></td><td><code>[]string</code></td><td>Assigned IP addresses</td></tr>
<tr><td><code>consoleURL</code></td><td><code>string</code></td><td>Console access URL</td></tr>
<tr><td><code>conditions</code></td><td><code>[]Condition</code></td><td>Status conditions</td></tr>
<tr><td><code>observedGeneration</code></td><td><code>int64</code></td><td>Last observed generation</td></tr>
<tr><td><code>lastTaskRef</code></td><td><code>string</code></td><td>Reference to last async task</td></tr>
<tr><td><code>provider</code></td><td><code>map[string]string</code></td><td>Provider-specific details</td></tr>
</tbody></table>
</div>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: demo-web-01
spec:
  providerRef:
    name: vsphere-prod
  classRef:
    name: small
  imageRef:
    name: ubuntu-22-template
  networks:
    - name: app-net
      ipPolicy: dhcp
  powerState: On
</code></pre>
<h2 id="vmclass"><a class="header" href="#vmclass">VMClass</a></h2>
<p>The <code>VMClass</code> CRD defines resource allocation for virtual machines.</p>
<h3 id="spec-1"><a class="header" href="#spec-1">Spec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>cpu</code></td><td><code>int32</code></td><td>Yes</td><td>Number of virtual CPUs</td></tr>
<tr><td><code>memoryMiB</code></td><td><code>int32</code></td><td>Yes</td><td>Memory in MiB</td></tr>
<tr><td><code>firmware</code></td><td><code>string</code></td><td>No</td><td>Firmware type (BIOS/UEFI)</td></tr>
<tr><td><code>diskDefaults</code></td><td><code>DiskDefaults</code></td><td>No</td><td>Default disk settings</td></tr>
<tr><td><code>guestToolsPolicy</code></td><td><code>string</code></td><td>No</td><td>Guest tools policy</td></tr>
<tr><td><code>extraConfig</code></td><td><code>map[string]string</code></td><td>No</td><td>Provider-specific configuration</td></tr>
</tbody></table>
</div>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: small
spec:
  cpu: 2
  memoryMiB: 4096
  firmware: UEFI
  diskDefaults:
    type: thin
    sizeGiB: 40
</code></pre>
<h2 id="vmimage"><a class="header" href="#vmimage">VMImage</a></h2>
<p>The <code>VMImage</code> CRD defines base templates/images for virtual machines.</p>
<h3 id="spec-2"><a class="header" href="#spec-2">Spec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>vsphere</code></td><td><code>VSphereImageSpec</code></td><td>No</td><td>vSphere-specific configuration</td></tr>
<tr><td><code>libvirt</code></td><td><code>LibvirtImageSpec</code></td><td>No</td><td>Libvirt-specific configuration</td></tr>
<tr><td><code>prepare</code></td><td><code>ImagePrepare</code></td><td>No</td><td>Image preparation options</td></tr>
</tbody></table>
</div>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-22-template
spec:
  vsphere:
    templateName: "tmpl-ubuntu-22.04-cloudimg"
  libvirt:
    url: "https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img"
    format: qcow2
</code></pre>
<h2 id="vmnetworkattachment"><a class="header" href="#vmnetworkattachment">VMNetworkAttachment</a></h2>
<p>The <code>VMNetworkAttachment</code> CRD defines network configurations.</p>
<h3 id="spec-3"><a class="header" href="#spec-3">Spec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>vsphere</code></td><td><code>VSphereNetworkSpec</code></td><td>No</td><td>vSphere-specific network config</td></tr>
<tr><td><code>libvirt</code></td><td><code>LibvirtNetworkSpec</code></td><td>No</td><td>Libvirt-specific network config</td></tr>
<tr><td><code>ipPolicy</code></td><td><code>string</code></td><td>No</td><td>IP assignment policy</td></tr>
<tr><td><code>macAddress</code></td><td><code>string</code></td><td>No</td><td>Static MAC address</td></tr>
</tbody></table>
</div>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMNetworkAttachment
metadata:
  name: app-net
spec:
  vsphere:
    portgroup: "PG-App"
  ipPolicy: dhcp
</code></pre>
<h2 id="provider"><a class="header" href="#provider">Provider</a></h2>
<p>The <code>Provider</code> CRD configures hypervisor connection details.</p>
<h3 id="spec-4"><a class="header" href="#spec-4">Spec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>type</code></td><td><code>string</code></td><td>Yes</td><td>Provider type (vsphere/libvirt/etc)</td></tr>
<tr><td><code>endpoint</code></td><td><code>string</code></td><td>Yes</td><td>Provider endpoint URI</td></tr>
<tr><td><code>credentialSecretRef</code></td><td><code>ObjectRef</code></td><td>Yes</td><td>Secret containing credentials</td></tr>
<tr><td><code>insecureSkipVerify</code></td><td><code>bool</code></td><td>No</td><td>Skip TLS verification</td></tr>
<tr><td><code>defaults</code></td><td><code>ProviderDefaults</code></td><td>No</td><td>Default placement settings</td></tr>
<tr><td><code>rateLimit</code></td><td><code>RateLimit</code></td><td>No</td><td>API rate limiting</td></tr>
</tbody></table>
</div>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-prod
spec:
  type: vsphere
  endpoint: https://vcenter.example.com
  credentialSecretRef:
    name: vsphere-creds
  defaults:
    datastore: datastore1
    cluster: compute-cluster-a
</code></pre>
<h2 id="common-types"><a class="header" href="#common-types">Common Types</a></h2>
<h3 id="objectref"><a class="header" href="#objectref">ObjectRef</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>name</code></td><td><code>string</code></td><td>Yes</td><td>Object name</td></tr>
<tr><td><code>namespace</code></td><td><code>string</code></td><td>No</td><td>Object namespace</td></tr>
</tbody></table>
</div>
<h3 id="diskspec"><a class="header" href="#diskspec">DiskSpec</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>sizeGiB</code></td><td><code>int32</code></td><td>Yes</td><td>Disk size in GiB</td></tr>
<tr><td><code>type</code></td><td><code>string</code></td><td>No</td><td>Disk type</td></tr>
<tr><td><code>name</code></td><td><code>string</code></td><td>No</td><td>Disk name</td></tr>
</tbody></table>
</div>
<h3 id="userdata"><a class="header" href="#userdata">UserData</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>cloudInit</code></td><td><code>CloudInitConfig</code></td><td>No</td><td>Cloud-init configuration</td></tr>
</tbody></table>
</div>
<h3 id="metadata"><a class="header" href="#metadata">MetaData</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>inline</code></td><td><code>string</code></td><td>No</td><td>Inline cloud-init metadata in YAML format</td></tr>
<tr><td><code>secretRef</code></td><td><code>ObjectRef</code></td><td>No</td><td>Secret containing cloud-init metadata</td></tr>
</tbody></table>
</div>
<h3 id="cloudinitconfig"><a class="header" href="#cloudinitconfig">CloudInitConfig</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr></thead><tbody>
<tr><td><code>secretRef</code></td><td><code>ObjectRef</code></td><td>No</td><td>Secret containing cloud-init data</td></tr>
<tr><td><code>inline</code></td><td><code>string</code></td><td>No</td><td>Inline cloud-init configuration</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="examples"><a class="header" href="#examples">Examples</a></h1>
<p>This document provides practical examples for using VirtRigaud with the Remote provider architecture.</p>
<h2 id="quick-start-examples"><a class="header" href="#quick-start-examples">Quick Start Examples</a></h2>
<p>All VirtRigaud providers now run as Remote providers. Here are the essential examples to get started:</p>
<h3 id="basic-provider-setup"><a class="header" href="#basic-provider-setup">Basic Provider Setup</a></h3>
<ul>
<li><strong><a href="examples/provider-vsphere.yaml">vSphere Provider</a></strong> - Basic vSphere provider configuration</li>
<li><strong><a href="examples/provider-libvirt.yaml">LibVirt Provider</a></strong> - Basic LibVirt provider configuration</li>
</ul>
<h3 id="complete-working-examples"><a class="header" href="#complete-working-examples">Complete Working Examples</a></h3>
<ul>
<li><strong><a href="examples/complete-example.yaml">Complete vSphere Setup</a></strong> - End-to-end vSphere VM creation</li>
<li><strong><a href="examples/vsphere-advanced-example.yaml">Advanced vSphere Setup</a></strong> - Production-ready vSphere configuration</li>
<li><strong><a href="examples/libvirt-complete-example.yaml">LibVirt Complete Setup</a></strong> - End-to-end LibVirt VM creation</li>
<li><strong><a href="examples/multi-provider-example.yaml">Multi-Provider Setup</a></strong> - Using multiple providers together</li>
</ul>
<h3 id="individual-resource-examples"><a class="header" href="#individual-resource-examples">Individual Resource Examples</a></h3>
<ul>
<li><strong><a href="examples/vmclass-small.yaml">VMClass</a></strong> - VM resource allocation template</li>
<li><strong><a href="examples/vmimage-ubuntu.yaml">VMImage</a></strong> - VM image/template definition</li>
<li><strong><a href="examples/vmnetwork-app.yaml">VMNetworkAttachment</a></strong> - Network configuration</li>
<li><strong><a href="examples/vm-ubuntu-small.yaml">Simple VM</a></strong> - Basic virtual machine</li>
</ul>
<h3 id="advanced-examples"><a class="header" href="#advanced-examples">Advanced Examples</a></h3>
<ul>
<li><strong><a href="examples/security/">Security Configuration</a></strong> - RBAC, network policies, external secrets</li>
<li><strong><a href="examples/advanced/">Advanced Operations</a></strong> - Snapshots, reconfiguration, lifecycle management</li>
</ul>
<h2 id="example-directory-structure"><a class="header" href="#example-directory-structure">Example Directory Structure</a></h2>
<pre><code>docs/examples/
‚îú‚îÄ‚îÄ provider-*.yaml          # Provider configurations
‚îú‚îÄ‚îÄ complete-example.yaml    # Full working setup
‚îú‚îÄ‚îÄ *-advanced-example.yaml  # Production configurations
‚îú‚îÄ‚îÄ vm*.yaml                 # Individual resource definitions
‚îú‚îÄ‚îÄ advanced/                # Advanced operations
‚îú‚îÄ‚îÄ security/                # Security configurations
‚îî‚îÄ‚îÄ secrets/                 # Credential examples
</code></pre>
<h2 id="key-changes-from-previous-versions"><a class="header" href="#key-changes-from-previous-versions">Key Changes from Previous Versions</a></h2>
<h3 id="remote-only-architecture"><a class="header" href="#remote-only-architecture">Remote-Only Architecture</a></h3>
<p>All providers now run as separate pods with the Remote runtime:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: my-provider
spec:
  type: vsphere  # or libvirt, proxmox
  endpoint: https://vcenter.example.com
  credentialSecretRef:
    name: provider-creds
  runtime:
    mode: Remote              # Required - only mode supported
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.3"
    service:
      port: 9090
</code></pre>
<h3 id="current-api-schema-v023"><a class="header" href="#current-api-schema-v023">Current API Schema (v0.2.3)</a></h3>
<ul>
<li><strong>VMClass</strong>: Standard Kubernetes resource quantities (<code>cpus: 4</code>, <code>memory: "4Gi"</code>)</li>
<li><strong>VMImage</strong>: Provider-specific source configurations</li>
<li><strong>VMNetworkAttachment</strong>: Network provider abstractions</li>
<li><strong>VirtualMachine</strong>: Declarative power state management</li>
</ul>
<h3 id="configuration-management"><a class="header" href="#configuration-management">Configuration Management</a></h3>
<p>Providers receive configuration through:</p>
<ul>
<li><strong>Endpoint</strong>: Environment variable <code>PROVIDER_ENDPOINT</code></li>
<li><strong>Credentials</strong>: Mounted secret files in <code>/etc/virtrigaud/credentials/</code></li>
<li><strong>Runtime</strong>: Managed automatically by the provider controller</li>
</ul>
<h2 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h2>
<ol>
<li><strong>Choose your provider</strong> from the basic examples above</li>
<li><strong>Create credentials secret</strong> (see <code>examples/secrets/</code>)</li>
<li><strong>Apply provider configuration</strong> with required <code>runtime</code> section</li>
<li><strong>Define VM resources</strong> (VMClass, VMImage, VMNetworkAttachment)</li>
<li><strong>Create VirtualMachine</strong> referencing your resources</li>
</ol>
<p>For detailed setup instructions, see:</p>
<ul>
<li><a href="getting-started/quickstart.html">Getting Started Guide</a></li>
<li><a href="REMOTE_PROVIDERS.html">Remote Providers Documentation</a></li>
<li><a href="providers/">Provider-Specific Guides</a></li>
</ul>
<h2 id="need-help"><a class="header" href="#need-help">Need Help?</a></h2>
<ul>
<li>Check the <a href="REMOTE_PROVIDERS.html">Remote Providers documentation</a> for architecture details</li>
<li>Review <a href="providers/">provider-specific guides</a> for setup instructions</li>
<li>Look at <a href="examples/">complete examples</a> for working configurations</li>
<li>See <a href="getting-started/quickstart.html#troubleshooting">troubleshooting tips</a> for common issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provider-development-guide"><a class="header" href="#provider-development-guide">Provider Development Guide</a></h1>
<p>This document explains how to implement a new provider for VirtRigaud.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Providers are responsible for implementing VM lifecycle operations on specific hypervisor platforms. VirtRigaud uses a Remote Provider architecture where each provider runs as an independent gRPC service, communicating with the manager controller.</p>
<h2 id="provider-interface"><a class="header" href="#provider-interface">Provider Interface</a></h2>
<p>All providers must implement the <code>contracts.Provider</code> interface:</p>
<pre><code class="language-go">type Provider interface {
    // Validate ensures the provider session/credentials are healthy
    Validate(ctx context.Context) error

    // Create creates a new VM if it doesn't exist (idempotent)
    Create(ctx context.Context, req CreateRequest) (CreateResponse, error)

    // Delete removes a VM (idempotent)
    Delete(ctx context.Context, id string) (taskRef string, err error)

    // Power performs a power operation on the VM
    Power(ctx context.Context, id string, op PowerOp) (taskRef string, err error)

    // Reconfigure modifies VM resources
    Reconfigure(ctx context.Context, id string, desired CreateRequest) (taskRef string, err error)

    // Describe returns the current state of the VM
    Describe(ctx context.Context, id string) (DescribeResponse, error)

    // IsTaskComplete checks if an async task is complete
    IsTaskComplete(ctx context.Context, taskRef string) (done bool, err error)
}
</code></pre>
<h2 id="implementation-steps"><a class="header" href="#implementation-steps">Implementation Steps</a></h2>
<h3 id="1-create-provider-package"><a class="header" href="#1-create-provider-package">1. Create Provider Package</a></h3>
<p>Create a new package under <code>internal/providers/</code> for your provider:</p>
<pre><code>internal/providers/yourprovider/
‚îú‚îÄ‚îÄ provider.go      # Main provider implementation
‚îú‚îÄ‚îÄ session.go       # Connection/session management
‚îú‚îÄ‚îÄ tasks.go         # Async task handling
‚îú‚îÄ‚îÄ converter.go     # Type conversions
‚îú‚îÄ‚îÄ network.go       # Network operations
‚îî‚îÄ‚îÄ storage.go       # Storage operations
</code></pre>
<h3 id="2-implement-the-provider"><a class="header" href="#2-implement-the-provider">2. Implement the Provider</a></h3>
<pre><code class="language-go">package yourprovider

import (
    "context"
    "github.com/projectbeskar/virtrigaud/api/v1beta1"
    "github.com/projectbeskar/virtrigaud/internal/providers/contracts"
)

type Provider struct {
    config   *v1beta1.Provider
    client   YourProviderClient
}

func NewProvider(ctx context.Context, provider *v1beta1.Provider) (contracts.Provider, error) {
    // Initialize your provider client
    // Parse credentials from secret
    // Establish connection
    return &amp;Provider{
        config: provider,
        client: client,
    }, nil
}

func (p *Provider) Validate(ctx context.Context) error {
    // Check connection health
    // Validate credentials
    return nil
}

// Implement other interface methods...
</code></pre>
<h3 id="3-create-provider-grpc-server"><a class="header" href="#3-create-provider-grpc-server">3. Create Provider gRPC Server</a></h3>
<p>Create a gRPC server for your provider:</p>
<pre><code class="language-go">// cmd/provider-yourprovider/main.go
package main

import (
    "context"
    "log"
    "net"
    
    "google.golang.org/grpc"
    "github.com/projectbeskar/virtrigaud/pkg/grpc/provider"
    "github.com/projectbeskar/virtrigaud/internal/providers/yourprovider"
)

func main() {
    lis, err := net.Listen("tcp", ":9090")
    if err != nil {
        log.Fatal(err)
    }
    
    s := grpc.NewServer()
    provider.RegisterProviderServer(s, &amp;yourprovider.GRPCServer{})
    
    log.Println("Provider server listening on :9090")
    if err := s.Serve(lis); err != nil {
        log.Fatal(err)
    }
}
</code></pre>
<h3 id="4-handle-credentials"><a class="header" href="#4-handle-credentials">4. Handle Credentials</a></h3>
<p>Providers should read credentials from Kubernetes secrets. Common credential fields:</p>
<ul>
<li><code>username</code> / <code>password</code>: Basic authentication</li>
<li><code>token</code>: API token authentication</li>
<li><code>tls.crt</code> / <code>tls.key</code>: TLS client certificates</li>
</ul>
<p>Example:</p>
<pre><code class="language-go">func (p *Provider) getCredentials(ctx context.Context) (*Credentials, error) {
    secret := &amp;corev1.Secret{}
    err := p.client.Get(ctx, types.NamespacedName{
        Name:      p.config.Spec.CredentialSecretRef.Name,
        Namespace: p.config.Namespace,
    }, secret)
    if err != nil {
        return nil, err
    }

    return &amp;Credentials{
        Username: string(secret.Data["username"]),
        Password: string(secret.Data["password"]),
    }, nil
}
</code></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>Use the provided error types for consistent error handling:</p>
<pre><code class="language-go">import "github.com/projectbeskar/virtrigaud/internal/providers/contracts"

// For not found errors
return contracts.NewNotFoundError("VM not found", err)

// For retryable errors
return contracts.NewRetryableError("Connection timeout", err)

// For validation errors
return contracts.NewInvalidSpecError("Invalid CPU count", nil)
</code></pre>
<h2 id="asynchronous-operations"><a class="header" href="#asynchronous-operations">Asynchronous Operations</a></h2>
<p>For long-running operations, return a task reference:</p>
<pre><code class="language-go">func (p *Provider) Create(ctx context.Context, req CreateRequest) (CreateResponse, error) {
    taskID, err := p.client.CreateVMAsync(...)
    if err != nil {
        return CreateResponse{}, err
    }

    return CreateResponse{
        ID:      vmID,
        TaskRef: taskID,
    }, nil
}

func (p *Provider) IsTaskComplete(ctx context.Context, taskRef string) (bool, error) {
    task, err := p.client.GetTask(taskRef)
    if err != nil {
        return false, err
    }
    return task.IsComplete(), nil
}
</code></pre>
<h2 id="type-conversions"><a class="header" href="#type-conversions">Type Conversions</a></h2>
<p>Convert between CRD types and provider-specific types:</p>
<pre><code class="language-go">func (p *Provider) convertVMClass(class contracts.VMClass) YourProviderVMSpec {
    return YourProviderVMSpec{
        CPUs:   class.CPU,
        Memory: class.MemoryMiB * 1024 * 1024, // Convert to bytes
        // ... other conversions
    }
}
</code></pre>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>Create unit tests for your provider:</p>
<pre><code class="language-go">func TestProvider_Create(t *testing.T) {
    provider := &amp;Provider{
        client: &amp;mockClient{},
    }

    req := contracts.CreateRequest{
        Name: "test-vm",
        // ... populate request
    }

    resp, err := provider.Create(context.Background(), req)
    assert.NoError(t, err)
    assert.NotEmpty(t, resp.ID)
}
</code></pre>
<h2 id="provider-specific-crd-fields"><a class="header" href="#provider-specific-crd-fields">Provider-Specific CRD Fields</a></h2>
<p>Update the CRD types to include provider-specific fields:</p>
<pre><code class="language-go">// In VMImage types
type YourProviderImageSpec struct {
    ImageID   string `json:"imageId,omitempty"`
    Checksum  string `json:"checksum,omitempty"`
}

// In VMNetworkAttachment types
type YourProviderNetworkSpec struct {
    NetworkID string `json:"networkId,omitempty"`
    VLAN      int32  `json:"vlan,omitempty"`
}
</code></pre>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<ol>
<li><strong>Idempotency</strong>: All operations should be idempotent</li>
<li><strong>Error Classification</strong>: Use appropriate error types</li>
<li><strong>Resource Cleanup</strong>: Ensure proper cleanup in Delete operations</li>
<li><strong>Logging</strong>: Use structured logging with context</li>
<li><strong>Timeouts</strong>: Respect context timeouts</li>
<li><strong>Rate Limiting</strong>: Implement client-side rate limiting</li>
<li><strong>Retry Logic</strong>: Handle transient failures gracefully</li>
</ol>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<p>See the existing providers for reference:</p>
<ul>
<li><code>internal/providers/vsphere/</code> - vSphere implementation</li>
<li><code>internal/providers/libvirt/</code> - Libvirt implementation (production ready)</li>
</ul>
<h2 id="provider-configuration"><a class="header" href="#provider-configuration">Provider Configuration</a></h2>
<p>Each provider type should support these configuration options:</p>
<ul>
<li>Connection endpoints</li>
<li>Authentication credentials</li>
<li>Default placement settings</li>
<li>Rate limiting configuration</li>
<li>Provider-specific options</li>
</ul>
<p>Example Provider spec:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: my-provider
spec:
  type: yourprovider
  endpoint: https://api.yourprovider.com
  credentialSecretRef:
    name: provider-creds
  defaults:
    region: us-west-2
    zone: us-west-2a
  rateLimit:
    qps: 10
    burst: 20
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provider-capabilities-matrix"><a class="header" href="#provider-capabilities-matrix">Provider Capabilities Matrix</a></h1>
<p>This document provides a comprehensive overview of VirtRigaud provider capabilities as of v0.2.3.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>VirtRigaud supports multiple hypervisor platforms through a provider architecture. Each provider implements the core VirtRigaud API while supporting platform-specific features and capabilities.</p>
<h2 id="core-provider-interface"><a class="header" href="#core-provider-interface">Core Provider Interface</a></h2>
<p>All providers implement these core operations:</p>
<ul>
<li><strong>Validate</strong>: Test provider connectivity and credentials</li>
<li><strong>Create</strong>: Create new virtual machines</li>
<li><strong>Delete</strong>: Remove virtual machines and cleanup resources</li>
<li><strong>Power</strong>: Control VM power state (On/Off/Reboot)</li>
<li><strong>Describe</strong>: Query VM state and properties</li>
<li><strong>GetCapabilities</strong>: Report provider-specific capabilities</li>
</ul>
<h2 id="provider-status-1"><a class="header" href="#provider-status-1">Provider Status</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Status</th><th>Implementation</th><th>Maturity</th></tr></thead><tbody>
<tr><td><strong>vSphere</strong></td><td>‚úÖ Production Ready</td><td>govmomi-based</td><td>Stable</td></tr>
<tr><td><strong>Libvirt/KVM</strong></td><td>‚úÖ Production Ready</td><td>virsh-based</td><td>Stable</td></tr>
<tr><td><strong>Proxmox VE</strong></td><td>‚úÖ Production Ready</td><td>REST API-based</td><td>Beta</td></tr>
<tr><td><strong>Mock</strong></td><td>‚úÖ Complete</td><td>In-memory simulation</td><td>Testing</td></tr>
</tbody></table>
</div>
<h2 id="comprehensive-capability-matrix"><a class="header" href="#comprehensive-capability-matrix">Comprehensive Capability Matrix</a></h2>
<h3 id="core-operations"><a class="header" href="#core-operations">Core Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>VM Create</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>All providers support VM creation</td></tr>
<tr><td><strong>VM Delete</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>With resource cleanup</td></tr>
<tr><td><strong>Power On/Off</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Basic power management</td></tr>
<tr><td><strong>Reboot</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Graceful and forced restart</td></tr>
<tr><td><strong>Suspend</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Memory state preservation</td></tr>
<tr><td><strong>Describe</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>VM state and properties</td></tr>
<tr><td><strong>Reconfigure</strong></td><td>‚úÖ</td><td>‚ö†Ô∏è</td><td>‚úÖ</td><td>‚úÖ</td><td>CPU/Memory/Disk changes (Libvirt requires restart)</td></tr>
<tr><td><strong>TaskStatus</strong></td><td>‚úÖ</td><td>N/A</td><td>‚úÖ</td><td>‚úÖ</td><td>Async operation tracking</td></tr>
<tr><td><strong>ConsoleURL</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚ö†Ô∏è</td><td>‚úÖ</td><td>Remote console access (Proxmox planned)</td></tr>
</tbody></table>
</div>
<h3 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>CPU Configuration</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Cores, sockets, threading</td></tr>
<tr><td><strong>Memory Allocation</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Static memory sizing</td></tr>
<tr><td><strong>Hot CPU Add</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Online CPU expansion</td></tr>
<tr><td><strong>Hot Memory Add</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Online memory expansion</td></tr>
<tr><td><strong>Resource Reservations</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Guaranteed resources</td></tr>
<tr><td><strong>Resource Limits</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Resource capping</td></tr>
</tbody></table>
</div>
<h3 id="storage-operations"><a class="header" href="#storage-operations">Storage Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Disk Creation</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Virtual disk provisioning</td></tr>
<tr><td><strong>Disk Expansion</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Online disk growth</td></tr>
<tr><td><strong>Multiple Disks</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Multi-disk VMs</td></tr>
<tr><td><strong>Thin Provisioning</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Space-efficient disks</td></tr>
<tr><td><strong>Thick Provisioning</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Pre-allocated storage</td></tr>
<tr><td><strong>Storage Policies</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Policy-based placement</td></tr>
<tr><td><strong>Storage Pools</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Organized storage management</td></tr>
</tbody></table>
</div>
<h3 id="network-configuration"><a class="header" href="#network-configuration">Network Configuration</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Basic Networking</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Single network interface</td></tr>
<tr><td><strong>Multiple NICs</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Multi-interface VMs</td></tr>
<tr><td><strong>VLAN Support</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Network segmentation</td></tr>
<tr><td><strong>Static IP</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Fixed IP assignment</td></tr>
<tr><td><strong>DHCP</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Dynamic IP assignment</td></tr>
<tr><td><strong>Bridge Networks</strong></td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Direct host bridging</td></tr>
<tr><td><strong>Distributed Switches</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚ùå</td><td>‚úÖ</td><td>Advanced vSphere networking</td></tr>
</tbody></table>
</div>
<h3 id="vm-lifecycle"><a class="header" href="#vm-lifecycle">VM Lifecycle</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Template Deployment</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Deploy from templates</td></tr>
<tr><td><strong>Clone Operations</strong></td><td>‚úÖ Complete</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Full VM duplication with snapshot support</td></tr>
<tr><td><strong>Linked Clones</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>COW-based clones with automatic snapshot creation</td></tr>
<tr><td><strong>Full Clones</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Independent copies</td></tr>
<tr><td><strong>VM Reconfiguration</strong></td><td>‚úÖ Complete</td><td>‚ö†Ô∏è Restart Required</td><td>‚úÖ</td><td>‚úÖ</td><td>Online resource modification</td></tr>
</tbody></table>
</div>
<h3 id="snapshot-operations"><a class="header" href="#snapshot-operations">Snapshot Operations</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Create Snapshots</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Point-in-time captures</td></tr>
<tr><td><strong>Delete Snapshots</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Snapshot cleanup</td></tr>
<tr><td><strong>Revert Snapshots</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Restore VM state</td></tr>
<tr><td><strong>Memory Snapshots</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Include RAM state</td></tr>
<tr><td><strong>Quiesced Snapshots</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Consistent filesystem</td></tr>
<tr><td><strong>Snapshot Trees</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Hierarchical snapshots</td></tr>
</tbody></table>
</div>
<h3 id="image-management"><a class="header" href="#image-management">Image Management</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>OVA/OVF Import</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Standard VM formats</td></tr>
<tr><td><strong>Cloud Image Download</strong></td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Remote image fetch</td></tr>
<tr><td><strong>Content Libraries</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚ùå</td><td>‚úÖ</td><td>Centralized image management</td></tr>
<tr><td><strong>Image Conversion</strong></td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Format transformation</td></tr>
<tr><td><strong>Image Caching</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Performance optimization</td></tr>
</tbody></table>
</div>
<h3 id="guest-operating-system"><a class="header" href="#guest-operating-system">Guest Operating System</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Cloud-Init</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Guest initialization</td></tr>
<tr><td><strong>Guest Tools</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Enhanced guest integration</td></tr>
<tr><td><strong>Guest Agent</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Runtime guest communication</td></tr>
<tr><td><strong>Guest Customization</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>OS-specific customization</td></tr>
<tr><td><strong>Guest Monitoring</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Resource usage tracking</td></tr>
</tbody></table>
</div>
<h3 id="advanced-features-1"><a class="header" href="#advanced-features-1">Advanced Features</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>High Availability</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Automatic failover</td></tr>
<tr><td><strong>DRS/Load Balancing</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚ùå</td><td>‚úÖ</td><td>Resource optimization</td></tr>
<tr><td><strong>Fault Tolerance</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚ùå</td><td>‚úÖ</td><td>Zero-downtime protection</td></tr>
<tr><td><strong>vMotion/Migration</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Live VM migration</td></tr>
<tr><td><strong>Resource Pools</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Hierarchical resource mgmt</td></tr>
<tr><td><strong>Affinity Rules</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>VM placement policies</td></tr>
</tbody></table>
</div>
<h3 id="monitoring--observability"><a class="header" href="#monitoring--observability">Monitoring &amp; Observability</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Capability</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Performance Metrics</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>CPU, memory, disk, network</td></tr>
<tr><td><strong>Event Logging</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>Operation audit trail</td></tr>
<tr><td><strong>Health Checks</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ</td><td>VM and guest health</td></tr>
<tr><td><strong>Alerting</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Threshold-based notifications</td></tr>
<tr><td><strong>Historical Data</strong></td><td>‚úÖ</td><td>‚ùå</td><td>‚úÖ</td><td>‚úÖ</td><td>Performance history</td></tr>
<tr><td><strong>Console URL Generation</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚ö†Ô∏è</td><td>‚úÖ</td><td>Web/VNC console access (Proxmox planned)</td></tr>
<tr><td><strong>Guest Agent Integration</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ Complete</td><td>‚úÖ</td><td>IP detection and guest info</td></tr>
</tbody></table>
</div>
<h2 id="provider-specific-features"><a class="header" href="#provider-specific-features">Provider-Specific Features</a></h2>
<h3 id="vsphere-exclusive"><a class="header" href="#vsphere-exclusive">vSphere Exclusive</a></h3>
<ul>
<li><strong>vCenter Integration</strong>: Full vCenter Server and ESXi support</li>
<li><strong>Content Library</strong>: Centralized template and ISO management</li>
<li><strong>Distributed Resource Scheduler (DRS)</strong>: Automatic load balancing</li>
<li><strong>vMotion</strong>: Live migration between hosts</li>
<li><strong>High Availability (HA)</strong>: Automatic VM restart on host failure</li>
<li><strong>Fault Tolerance</strong>: Zero-downtime VM protection</li>
<li><strong>Storage vMotion</strong>: Live storage migration</li>
<li><strong>vSAN Integration</strong>: Hyper-converged storage</li>
<li><strong>NSX Integration</strong>: Software-defined networking</li>
<li><strong>Hot Reconfiguration</strong>: Online CPU/memory/disk changes with hot-add support</li>
<li><strong>TaskStatus Tracking</strong>: Real-time async operation monitoring via govmomi</li>
<li><strong>Clone Operations</strong>: Full and linked clones with automatic snapshot handling</li>
<li><strong>Web Console URLs</strong>: Direct vSphere web client console access</li>
</ul>
<h3 id="libvirtkvm-exclusive"><a class="header" href="#libvirtkvm-exclusive">Libvirt/KVM Exclusive</a></h3>
<ul>
<li><strong>Virsh Integration</strong>: Command-line management</li>
<li><strong>QEMU Guest Agent</strong>: Advanced guest OS integration</li>
<li><strong>KVM Optimization</strong>: Native Linux virtualization</li>
<li><strong>Bridge Networking</strong>: Direct host network bridging</li>
<li><strong>Storage Pool Flexibility</strong>: Multiple storage backend support</li>
<li><strong>Cloud Image Support</strong>: Direct cloud image deployment</li>
<li><strong>Host Device Passthrough</strong>: Hardware device assignment</li>
<li><strong>Reconfiguration Support</strong>: CPU/memory/disk changes via virsh (restart required)</li>
<li><strong>VNC Console Access</strong>: Direct VNC console URL generation for remote viewers</li>
</ul>
<h3 id="proxmox-ve-exclusive"><a class="header" href="#proxmox-ve-exclusive">Proxmox VE Exclusive</a></h3>
<ul>
<li><strong>Web UI Integration</strong>: Built-in management interface</li>
<li><strong>Container Support</strong>: LXC container management</li>
<li><strong>Backup Integration</strong>: Built-in backup and restore</li>
<li><strong>Cluster Management</strong>: Multi-node cluster support</li>
<li><strong>ZFS Integration</strong>: Advanced filesystem features</li>
<li><strong>Ceph Integration</strong>: Distributed storage</li>
<li><strong>Guest Agent IP Detection</strong>: Accurate IP address extraction via QEMU guest agent</li>
<li><strong>Hot-plug Reconfiguration</strong>: Online CPU/memory/disk modifications</li>
<li><strong>Complete CRD Integration</strong>: Full Kubernetes custom resource support</li>
</ul>
<h3 id="mock-provider-features"><a class="header" href="#mock-provider-features">Mock Provider Features</a></h3>
<ul>
<li><strong>Testing Scenarios</strong>: Configurable failure modes</li>
<li><strong>Performance Simulation</strong>: Controllable operation delays</li>
<li><strong>Sample Data</strong>: Pre-populated demonstration VMs</li>
<li><strong>Development Support</strong>: Full API coverage for testing</li>
</ul>
<h2 id="supported-disk-types"><a class="header" href="#supported-disk-types">Supported Disk Types</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Disk Formats</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>vSphere</strong></td><td>thin, thick, eagerZeroedThick</td><td>vSphere native formats</td></tr>
<tr><td><strong>Libvirt</strong></td><td>qcow2, raw, vmdk</td><td>QEMU-supported formats</td></tr>
<tr><td><strong>Proxmox</strong></td><td>qcow2, raw, vmdk</td><td>Proxmox storage formats</td></tr>
<tr><td><strong>Mock</strong></td><td>thin, thick, raw, qcow2</td><td>Simulated formats</td></tr>
</tbody></table>
</div>
<h2 id="supported-network-types"><a class="header" href="#supported-network-types">Supported Network Types</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Network Types</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>vSphere</strong></td><td>distributed, standard, vlan</td><td>vSphere networking</td></tr>
<tr><td><strong>Libvirt</strong></td><td>virtio, e1000, rtl8139</td><td>QEMU network adapters</td></tr>
<tr><td><strong>Proxmox</strong></td><td>virtio, e1000, rtl8139</td><td>Proxmox network models</td></tr>
<tr><td><strong>Mock</strong></td><td>bridge, nat, distributed</td><td>Simulated network types</td></tr>
</tbody></table>
</div>
<h2 id="provider-images"><a class="header" href="#provider-images">Provider Images</a></h2>
<p>All provider images are available from the GitHub Container Registry:</p>
<ul>
<li><strong>vSphere</strong>: <code>ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.3</code></li>
<li><strong>Libvirt</strong>: <code>ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.3</code></li>
<li><strong>Proxmox</strong>: <code>ghcr.io/projectbeskar/virtrigaud/provider-proxmox:v0.2.3</code></li>
<li><strong>Mock</strong>: <code>ghcr.io/projectbeskar/virtrigaud/provider-mock:v0.2.3</code></li>
</ul>
<h2 id="choosing-a-provider"><a class="header" href="#choosing-a-provider">Choosing a Provider</a></h2>
<h3 id="use-vsphere-when"><a class="header" href="#use-vsphere-when">Use vSphere When:</a></h3>
<ul>
<li>You have existing VMware infrastructure</li>
<li>You need enterprise features (HA, DRS, vMotion)</li>
<li>You require advanced networking (NSX, distributed switches)</li>
<li>You need centralized management (vCenter)</li>
</ul>
<h3 id="use-libvirtkvm-when"><a class="header" href="#use-libvirtkvm-when">Use Libvirt/KVM When:</a></h3>
<ul>
<li>You want open-source virtualization</li>
<li>You‚Äôre running on Linux hosts</li>
<li>You need cost-effective virtualization</li>
<li>You want direct host integration</li>
</ul>
<h3 id="use-proxmox-ve-when"><a class="header" href="#use-proxmox-ve-when">Use Proxmox VE When:</a></h3>
<ul>
<li>You need both VMs and containers</li>
<li>You want integrated backup solutions</li>
<li>You need cluster management</li>
<li>You want web-based management</li>
</ul>
<h3 id="use-mock-provider-when"><a class="header" href="#use-mock-provider-when">Use Mock Provider When:</a></h3>
<ul>
<li>You‚Äôre developing or testing VirtRigaud</li>
<li>You need to simulate VM operations</li>
<li>You‚Äôre creating demos or training materials</li>
<li>You‚Äôre testing VirtRigaud without hypervisors</li>
</ul>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="vsphere"><a class="header" href="#vsphere">vSphere</a></h3>
<ul>
<li><strong>Best for</strong>: Large-scale enterprise deployments</li>
<li><strong>Scalability</strong>: Hundreds to thousands of VMs</li>
<li><strong>Overhead</strong>: Higher due to feature richness</li>
<li><strong>Resource Efficiency</strong>: Excellent with DRS</li>
</ul>
<h3 id="libvirtkvm"><a class="header" href="#libvirtkvm">Libvirt/KVM</a></h3>
<ul>
<li><strong>Best for</strong>: Linux-based deployments</li>
<li><strong>Scalability</strong>: Moderate to large deployments</li>
<li><strong>Overhead</strong>: Low, near-native performance</li>
<li><strong>Resource Efficiency</strong>: Good with proper tuning</li>
</ul>
<h3 id="proxmox-ve"><a class="header" href="#proxmox-ve">Proxmox VE</a></h3>
<ul>
<li><strong>Best for</strong>: SMB and mixed workloads</li>
<li><strong>Scalability</strong>: Small to medium deployments</li>
<li><strong>Overhead</strong>: Moderate</li>
<li><strong>Resource Efficiency</strong>: Good with clustering</li>
</ul>
<h2 id="future-roadmap"><a class="header" href="#future-roadmap">Future Roadmap</a></h2>
<h3 id="planned-enhancements"><a class="header" href="#planned-enhancements">Planned Enhancements</a></h3>
<h4 id="vsphere-1"><a class="header" href="#vsphere-1">vSphere</a></h4>
<ul>
<li>vSphere 8.0 support</li>
<li>Enhanced NSX integration</li>
<li>GPU passthrough support</li>
<li>vSAN policy automation</li>
</ul>
<h4 id="libvirt"><a class="header" href="#libvirt">Libvirt</a></h4>
<ul>
<li>Live migration support</li>
<li>SR-IOV networking</li>
<li>NUMA topology optimization</li>
<li>Enhanced performance monitoring</li>
</ul>
<h4 id="proxmox"><a class="header" href="#proxmox">Proxmox</a></h4>
<ul>
<li>HA configuration</li>
<li>Storage replication</li>
<li>Advanced networking</li>
<li>Performance optimizations</li>
</ul>
<h2 id="support-matrix"><a class="header" href="#support-matrix">Support Matrix</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Feature Category</th><th>vSphere</th><th>Libvirt</th><th>Proxmox</th><th>Mock</th></tr></thead><tbody>
<tr><td><strong>Production Ready</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>‚úÖ Beta</td><td>‚úÖ Testing</td></tr>
<tr><td><strong>Documentation</strong></td><td>Complete</td><td>Complete</td><td>Complete</td><td>Complete</td></tr>
<tr><td><strong>Community Support</strong></td><td>Active</td><td>Active</td><td>Growing</td><td>N/A</td></tr>
<tr><td><strong>Enterprise Support</strong></td><td>Available</td><td>Available</td><td>Available</td><td>N/A</td></tr>
</tbody></table>
</div>
<h2 id="version-history"><a class="header" href="#version-history">Version History</a></h2>
<ul>
<li><strong>v0.2.3</strong>: Provider feature parity - Reconfigure, Clone, TaskStatus, ConsoleURL</li>
<li><strong>v0.2.2</strong>: Nested virtualization, TPM support, comprehensive snapshot management</li>
<li><strong>v0.2.1</strong>: Critical fixes, documentation updates, VMClass disk settings</li>
<li><strong>v0.2.0</strong>: Production-ready vSphere and Libvirt providers</li>
<li><strong>v0.1.0</strong>: Initial provider framework and mock implementation</li>
</ul>
<hr />
<p><em>This document reflects VirtRigaud v0.2.3 capabilities. For the latest updates, see the <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud documentation</a>.</em></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vsphere-provider"><a class="header" href="#vsphere-provider">vSphere Provider</a></h1>
<p>The vSphere provider enables VirtRigaud to manage virtual machines on VMware vSphere environments, including vCenter Server and standalone ESXi hosts. This provider is designed for enterprise production environments with comprehensive support for vSphere features.</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>This provider implements the VirtRigaud provider interface to manage VM lifecycle operations on VMware vSphere:</p>
<ul>
<li><strong>Create</strong>: Create VMs from templates, content libraries, or OVF/OVA files</li>
<li><strong>Delete</strong>: Remove VMs and associated storage (with configurable retention)</li>
<li><strong>Power</strong>: Start, stop, restart, and suspend virtual machines</li>
<li><strong>Describe</strong>: Query VM state, resource usage, guest info, and vSphere properties</li>
<li><strong>Reconfigure</strong>: Hot-add CPU/memory, resize disks, modify network adapters (v0.2.3+)</li>
<li><strong>Clone</strong>: Create full or linked clones from existing VMs or templates (v0.2.3+)</li>
<li><strong>Snapshot</strong>: Create, delete, and revert VM snapshots with memory state</li>
<li><strong>TaskStatus</strong>: Track asynchronous operations with progress monitoring (v0.2.3+)</li>
<li><strong>ConsoleURL</strong>: Generate vSphere web client console URLs (v0.2.3+)</li>
<li><strong>ImagePrepare</strong>: Import OVF/OVA, deploy from content library, or ensure template existence</li>
</ul>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<p><strong>‚ö†Ô∏è IMPORTANT: Active vSphere Environment Required</strong></p>
<p>The vSphere provider connects to VMware vSphere infrastructure and requires active vCenter Server or ESXi hosts.</p>
<h3 id="requirements"><a class="header" href="#requirements">Requirements:</a></h3>
<ul>
<li><strong>vCenter Server 7.0+</strong> or <strong>ESXi 7.0+</strong> (running and accessible)</li>
<li><strong>User account</strong> with appropriate privileges for VM management</li>
<li><strong>Network connectivity</strong> from VirtRigaud to vCenter/ESXi (HTTPS/443)</li>
<li><strong>vSphere infrastructure</strong>:
<ul>
<li>Configured datacenters, clusters, and hosts</li>
<li>Storage (datastores) for VM files</li>
<li>Networks (port groups) for VM connectivity</li>
<li>Resource pools for VM placement (optional)</li>
</ul>
</li>
</ul>
<h3 id="testingdevelopment"><a class="header" href="#testingdevelopment">Testing/Development:</a></h3>
<p>For development environments:</p>
<ul>
<li>Use <strong>VMware vSphere Hypervisor (ESXi)</strong> free version</li>
<li><strong>vCenter Server Appliance</strong> evaluation license</li>
<li><strong>VMware Workstation/Fusion</strong> with nested ESXi</li>
<li><strong>EVE-NG</strong> or <strong>GNS3</strong> with vSphere emulation</li>
</ul>
<h2 id="authentication"><a class="header" href="#authentication">Authentication</a></h2>
<p>The vSphere provider supports multiple authentication methods:</p>
<h3 id="usernamepassword-authentication-common"><a class="header" href="#usernamepassword-authentication-common">Username/Password Authentication (Common)</a></h3>
<p>Standard vSphere user authentication:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-prod
  namespace: default
spec:
  type: vsphere
  endpoint: https://vcenter.example.com/sdk
  credentialSecretRef:
    name: vsphere-credentials
  # Optional: Skip TLS verification (development only)
  insecureSkipVerify: false
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.3"
    service:
      port: 9090
</code></pre>
<p>Create credentials secret:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: vsphere-credentials
  namespace: default
type: Opaque
stringData:
  username: "virtrigaud@vsphere.local"
  password: "SecurePassword123!"
</code></pre>
<h3 id="session-token-authentication-advanced"><a class="header" href="#session-token-authentication-advanced">Session Token Authentication (Advanced)</a></h3>
<p>For environments using external authentication:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: vsphere-token
  namespace: default
type: Opaque
stringData:
  token: "vmware-api-session-id:abcd1234..."
</code></pre>
<h3 id="service-account-authentication-recommended"><a class="header" href="#service-account-authentication-recommended">Service Account Authentication (Recommended)</a></h3>
<p>Create a dedicated service account with minimal required privileges:</p>
<pre><code class="language-yaml"># vSphere privileges for VirtRigaud service account:
# - Datastore: Allocate space, Browse datastore, Low level file operations
# - Network: Assign network  
# - Resource: Assign virtual machine to resource pool
# - Virtual machine: All privileges (or subset based on requirements)
# - Global: Enable methods, Disable methods, Licenses
</code></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="connection-endpoints"><a class="header" href="#connection-endpoints">Connection Endpoints</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Endpoint Type</th><th>Format</th><th>Use Case</th></tr></thead><tbody>
<tr><td>vCenter Server</td><td><code>https://vcenter.example.com/sdk</code></td><td>Multi-host management (recommended)</td></tr>
<tr><td>vCenter FQDN</td><td><code>https://vcenter.corp.local/sdk</code></td><td>Internal domain environments</td></tr>
<tr><td>vCenter IP</td><td><code>https://192.168.1.10/sdk</code></td><td>Direct IP access</td></tr>
<tr><td>ESXi Host</td><td><code>https://esxi-host.example.com</code></td><td>Single host environments</td></tr>
</tbody></table>
</div>
<h3 id="deployment-configuration"><a class="header" href="#deployment-configuration">Deployment Configuration</a></h3>
<h4 id="using-helm-values"><a class="header" href="#using-helm-values">Using Helm Values</a></h4>
<pre><code class="language-yaml"># values.yaml
providers:
  vsphere:
    enabled: true
    endpoint: "https://vcenter.example.com/sdk"
    insecureSkipVerify: false  # Set to true for self-signed certificates
    credentialSecretRef:
      name: vsphere-credentials
      namespace: virtrigaud-system
</code></pre>
<h4 id="production-configuration-with-tls"><a class="header" href="#production-configuration-with-tls">Production Configuration with TLS</a></h4>
<pre><code class="language-yaml"># Create secret with credentials and TLS certificates
apiVersion: v1
kind: Secret
metadata:
  name: vsphere-secure-credentials
  namespace: virtrigaud-system
type: Opaque
stringData:
  username: "svc-virtrigaud@vsphere.local"
  password: "SecurePassword123!"
  # Optional: Custom CA certificate for vCenter
  ca.crt: |
    -----BEGIN CERTIFICATE-----
    # Your vCenter CA certificate here
    -----END CERTIFICATE-----

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-production
  namespace: virtrigaud-system
spec:
  type: vsphere
  endpoint: https://vcenter.prod.example.com/sdk
  credentialSecretRef:
    name: vsphere-secure-credentials
  insecureSkipVerify: false
</code></pre>
<h4 id="development-configuration"><a class="header" href="#development-configuration">Development Configuration</a></h4>
<pre><code class="language-yaml"># For development with self-signed certificates
providers:
  vsphere:
    enabled: true
    endpoint: "https://esxi-dev.local"
    insecureSkipVerify: true  # Only for development!
    credentialSecretRef:
      name: vsphere-dev-credentials
</code></pre>
<h4 id="multi-vcenter-configuration"><a class="header" href="#multi-vcenter-configuration">Multi-vCenter Configuration</a></h4>
<pre><code class="language-yaml"># Deploy multiple providers for different vCenters
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-datacenter-a
spec:
  type: vsphere
  endpoint: https://vcenter-a.example.com/sdk
  credentialSecretRef:
    name: vsphere-credentials-a

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-datacenter-b
spec:
  type: vsphere
  endpoint: https://vcenter-b.example.com/sdk
  credentialSecretRef:
    name: vsphere-credentials-b
</code></pre>
<h2 id="vsphere-infrastructure-setup"><a class="header" href="#vsphere-infrastructure-setup">vSphere Infrastructure Setup</a></h2>
<h3 id="required-vsphere-objects"><a class="header" href="#required-vsphere-objects">Required vSphere Objects</a></h3>
<p>The provider expects the following vSphere infrastructure to be configured:</p>
<h4 id="datacenters-and-clusters"><a class="header" href="#datacenters-and-clusters">Datacenters and Clusters</a></h4>
<pre><code class="language-bash"># Example vSphere hierarchy:
Datacenter: "Production"
‚îú‚îÄ‚îÄ Cluster: "Compute-Cluster"
‚îÇ   ‚îú‚îÄ‚îÄ ESXi Host: esxi-01.example.com
‚îÇ   ‚îú‚îÄ‚îÄ ESXi Host: esxi-02.example.com
‚îÇ   ‚îî‚îÄ‚îÄ ESXi Host: esxi-03.example.com
‚îú‚îÄ‚îÄ Datastores:
‚îÇ   ‚îú‚îÄ‚îÄ "datastore-ssd"     # High-performance storage
‚îÇ   ‚îú‚îÄ‚îÄ "datastore-hdd"     # Standard storage
‚îÇ   ‚îî‚îÄ‚îÄ "datastore-backup"  # Backup storage
‚îî‚îÄ‚îÄ Networks:
    ‚îú‚îÄ‚îÄ "VM Network"        # Default VM network
    ‚îú‚îÄ‚îÄ "DMZ-Network"       # DMZ port group
    ‚îî‚îÄ‚îÄ "Management"        # Management network
</code></pre>
<h4 id="resource-pools-optional"><a class="header" href="#resource-pools-optional">Resource Pools (Optional)</a></h4>
<pre><code class="language-bash"># Create resource pools for workload isolation
Datacenter: "Production"
‚îî‚îÄ‚îÄ Cluster: "Compute-Cluster"
    ‚îî‚îÄ‚îÄ Resource Pools:
        ‚îú‚îÄ‚îÄ "Development"    # Dev workloads (lower priority)
        ‚îú‚îÄ‚îÄ "Production"     # Prod workloads (high priority)
        ‚îî‚îÄ‚îÄ "Testing"        # Test workloads (medium priority)
</code></pre>
<h2 id="vm-configuration"><a class="header" href="#vm-configuration">VM Configuration</a></h2>
<h3 id="vmclass-specification"><a class="header" href="#vmclass-specification">VMClass Specification</a></h3>
<p>Define CPU, memory, and vSphere-specific settings:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: standard-vm
spec:
  cpus: 4
  memory: "8Gi"
  # vSphere-specific configuration
  spec:
    # VM hardware settings
    hardware:
      version: "vmx-19"              # Hardware version
      firmware: "efi"                # BIOS or EFI
      secureBoot: true               # Secure boot (EFI only)
      enableCpuHotAdd: true          # Hot-add CPU
      enableMemoryHotAdd: true       # Hot-add memory
    
    # CPU configuration
    cpu:
      coresPerSocket: 2              # CPU topology
      enableVirtualization: false    # Nested virtualization
      reservationMHz: 1000           # CPU reservation
      limitMHz: 4000                 # CPU limit
    
    # Memory configuration  
    memory:
      reservationMB: 2048            # Memory reservation
      limitMB: 8192                  # Memory limit
      shareLevel: "normal"           # Memory shares (low/normal/high)
    
    # Storage configuration
    storage:
      diskFormat: "thin"             # thick/thin/eagerZeroedThick
      storagePolicy: "VM Storage Policy - SSD"  # vSAN storage policy
    
    # vSphere placement
    placement:
      datacenter: "Production"       # Target datacenter
      cluster: "Compute-Cluster"     # Target cluster  
      resourcePool: "Production"     # Target resource pool
      datastore: "datastore-ssd"     # Preferred datastore
      folder: "/vm/virtrigaud"       # VM folder
</code></pre>
<h3 id="vmimage-specification"><a class="header" href="#vmimage-specification">VMImage Specification</a></h3>
<p>Reference vSphere templates, content library items, or OVF files:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-22-04-template
spec:
  # Template from vSphere inventory
  source:
    template: "ubuntu-22.04-template"
    datacenter: "Production"
    folder: "/vm/templates"
  
  # Or from content library
  # source:
  #   contentLibrary: "OS Templates"
  #   item: "ubuntu-22.04-cloud"
  
  # Or from OVF/OVA URL
  # source:
  #   ovf: "https://releases.ubuntu.com/22.04/ubuntu-22.04-server-cloudimg-amd64.ova"
  
  # Guest OS identification
  guestOS: "ubuntu64Guest"
  
  # Customization specification
  customization:
    type: "cloudInit"              # cloudInit, sysprep, or linux
    spec: "ubuntu-cloud-init"      # Reference to customization spec
</code></pre>
<h3 id="complete-vm-example"><a class="header" href="#complete-vm-example">Complete VM Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-application
spec:
  providerRef:
    name: vsphere-prod
  classRef:
    name: standard-vm
  imageRef:
    name: ubuntu-22-04-template
  powerState: On
  
  # Disk configuration
  disks:
    - name: root
      size: "100Gi"
      storageClass: "ssd-storage"
      # vSphere-specific disk options
      spec:
        diskMode: "persistent"       # persistent, independent_persistent, independent_nonpersistent
        diskFormat: "thin"           # thick, thin, eagerZeroedThick
        controllerType: "scsi"       # scsi, ide, nvme
        unitNumber: 0                # SCSI unit number
    
    - name: data
      size: "500Gi" 
      storageClass: "hdd-storage"
      spec:
        diskFormat: "thick"
        controllerType: "scsi"
        unitNumber: 1
  
  # Network configuration
  networks:
    # Primary application network
    - name: app-network
      portGroup: "VM Network"
      # Optional: Static IP assignment
      staticIP:
        address: "192.168.100.50/24"
        gateway: "192.168.100.1"
        dns: ["192.168.1.10", "8.8.8.8"]
    
    # Management network
    - name: mgmt-network
      portGroup: "Management"
      # DHCP assignment (default)
  
  # vSphere-specific placement
  placement:
    datacenter: "Production"
    cluster: "Compute-Cluster"
    resourcePool: "Production"
    folder: "/vm/applications"
    datastore: "datastore-ssd"      # Override class default
    host: "esxi-01.example.com"      # Pin to specific host (optional)
  
  # Guest customization
  userData:
    cloudInit:
      inline: |
        #cloud-config
        hostname: web-application
        users:
          - name: ubuntu
            sudo: ALL=(ALL) NOPASSWD:ALL
            ssh_authorized_keys:
              - "ssh-ed25519 AAAA..."
        packages:
          - nginx
          - docker.io
          - open-vm-tools          # VMware tools for guest integration
        runcmd:
          - systemctl enable nginx
          - systemctl enable docker
          - systemctl enable open-vm-tools
</code></pre>
<h2 id="advanced-features-2"><a class="header" href="#advanced-features-2">Advanced Features</a></h2>
<h3 id="vm-reconfiguration-v023"><a class="header" href="#vm-reconfiguration-v023">VM Reconfiguration (v0.2.3+)</a></h3>
<p>The vSphere provider supports online VM reconfiguration for CPU, memory, and disk resources:</p>
<pre><code class="language-yaml"># Reconfigure VM resources
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  vmClassRef: medium  # Change from small to medium
  powerState: "On"
</code></pre>
<p><strong>Capabilities</strong>:</p>
<ul>
<li><strong>Online CPU Changes</strong>: Hot-add CPUs to running VMs (requires guest OS support)</li>
<li><strong>Online Memory Changes</strong>: Hot-add memory to running VMs (requires guest OS support)</li>
<li><strong>Disk Resizing</strong>: Expand disks online (shrinking not supported for safety)</li>
<li><strong>Automatic Fallback</strong>: Falls back to offline changes if hot-add not supported</li>
<li><strong>Intelligent Detection</strong>: Only applies changes when needed</li>
</ul>
<p><strong>Memory Format Support</strong>:</p>
<ul>
<li>Standard units: <code>2Gi</code>, <code>4096Mi</code>, <code>2048MiB</code>, <code>2GiB</code></li>
<li>Parser handles multiple memory unit formats</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Disk shrinking prevented to avoid data loss</li>
<li>Some guest operating systems require special configuration for hot-add</li>
<li>BIOS firmware VMs have limited hot-add support (use EFI firmware)</li>
</ul>
<h3 id="vm-cloning-v023"><a class="header" href="#vm-cloning-v023">VM Cloning (v0.2.3+)</a></h3>
<p>Create full or linked clones of existing VMs and templates:</p>
<pre><code class="language-yaml"># Clone from existing VM
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server-02
spec:
  vmClassRef: small
  vmImageRef: web-server-01  # Source VM
  cloneType: linked  # or "full"
</code></pre>
<p><strong>Clone Types</strong>:</p>
<ul>
<li><strong>Full Clone</strong>: Independent copy with separate storage</li>
<li><strong>Linked Clone</strong>: Space-efficient copy using snapshots
<ul>
<li>Automatically creates snapshot if none exists</li>
<li>Requires less storage and faster creation</li>
<li>Parent VM must remain available</li>
</ul>
</li>
</ul>
<p><strong>Use Cases</strong>:</p>
<ul>
<li>Rapid test environment provisioning</li>
<li>Development environment duplication</li>
<li>Template-based deployments</li>
<li>Disaster recovery scenarios</li>
</ul>
<h3 id="task-status-tracking-v023"><a class="header" href="#task-status-tracking-v023">Task Status Tracking (v0.2.3+)</a></h3>
<p>Monitor asynchronous vSphere operations in real-time:</p>
<pre><code class="language-yaml"># VirtRigaud automatically tracks long-running operations
# No manual configuration needed

# Task tracking provides:
# - Real-time task state (queued, running, success, error)
# - Progress percentage
# - Error messages for failed tasks
# - Integration with vSphere task manager
</code></pre>
<p><strong>Features</strong>:</p>
<ul>
<li>Automatic tracking of all async operations</li>
<li>Progress monitoring via govmomi task manager</li>
<li>Detailed error reporting</li>
<li>Task history visibility in vCenter</li>
</ul>
<h3 id="console-access-v023"><a class="header" href="#console-access-v023">Console Access (v0.2.3+)</a></h3>
<p>Generate direct vSphere web client console URLs:</p>
<pre><code class="language-yaml"># Access provided in VM status
kubectl get vm web-server -o yaml

status:
  consolURL: "https://vcenter.example.com/ui/app/vm;nav=h/urn:vmomi:VirtualMachine:vm-123:xxxxx/summary"
  phase: Running
</code></pre>
<p><strong>Features</strong>:</p>
<ul>
<li>Direct browser-based VM console access</li>
<li>No additional tools required</li>
<li>Works with vSphere web client</li>
<li>Includes VM instance UUID for reliable identification</li>
<li>Generated automatically in Describe operations</li>
</ul>
<h3 id="template-management"><a class="header" href="#template-management">Template Management</a></h3>
<h4 id="creating-templates"><a class="header" href="#creating-templates">Creating Templates</a></h4>
<pre><code class="language-yaml"># Convert existing VM to template
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMTemplate
metadata:
  name: create-ubuntu-template
spec:
  sourceVM: "ubuntu-base-vm"
  datacenter: "Production"
  targetFolder: "/vm/templates"
  templateName: "ubuntu-22.04-template"
  
  # Template metadata
  annotation: |
    Ubuntu 22.04 LTS Template
    Created: 2024-01-15
    Includes: cloud-init, open-vm-tools
  
  # Template customization
  powerOff: true                   # Power off before conversion
  removeSnapshots: true           # Clean up snapshots
  updateTools: true               # Update VMware tools
</code></pre>
<h4 id="content-library-integration"><a class="header" href="#content-library-integration">Content Library Integration</a></h4>
<pre><code class="language-yaml"># Deploy from content library
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage  
metadata:
  name: centos-stream-9
spec:
  source:
    contentLibrary: "OS Templates"
    item: "CentOS-Stream-9"
    datacenter: "Production"
  
  # Content library item properties
  properties:
    version: "9.0"
    provider: "CentOS"
    osType: "linux"
</code></pre>
<h3 id="storage-policies"><a class="header" href="#storage-policies">Storage Policies</a></h3>
<pre><code class="language-yaml"># VMClass with vSAN storage policy
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: high-performance
spec:
  cpus: 8
  memory: "32Gi"
  spec:
    storage:
      # vSAN storage policies
      homePolicy: "VM Storage Policy - Performance"    # VM home/config files
      diskPolicy: "VM Storage Policy - SSD Only"       # Virtual disks
      swapPolicy: "VM Storage Policy - Standard"        # Swap files
      
      # Traditional storage
      datastoreCluster: "DatastoreCluster-SSD"         # Datastore cluster
      antiAffinityRules: true                          # VM anti-affinity
</code></pre>
<h3 id="network-advanced-configuration"><a class="header" href="#network-advanced-configuration">Network Advanced Configuration</a></h3>
<pre><code class="language-yaml"># Advanced networking with distributed switches
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMNetworkAttachment
metadata:
  name: advanced-networking
spec:
  networks:
    # Distributed port group
    - name: frontend
      portGroup: "DPG-Frontend-VLAN100"
      distributedSwitch: "DSwitch-Production"
      vlan: 100
      
    # NSX-T logical switch
    - name: backend  
      portGroup: "LS-Backend-App"
      nsx: true
      securityPolicy: "Backend-Security-Policy"
      
    # SR-IOV for high performance
    - name: storage
      portGroup: "DPG-Storage-VLAN200"
      sriov: true
      bandwidth:
        reservation: 1000  # Mbps
        limit: 10000      # Mbps
        shares: 100       # Priority
</code></pre>
<h3 id="high-availability"><a class="header" href="#high-availability">High Availability</a></h3>
<pre><code class="language-yaml"># VM with HA/DRS settings
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: critical-application
spec:
  providerRef:
    name: vsphere-prod
  # ... other config ...
  
  # High availability configuration
  availability:
    # HA restart priority
    restartPriority: "high"          # disabled, low, medium, high
    isolationResponse: "powerOff"    # none, powerOff, shutdown
    vmMonitoring: "vmMonitoringOnly" # vmMonitoringDisabled, vmMonitoringOnly, vmAndAppMonitoring
    
    # DRS configuration
    drsAutomationLevel: "fullyAutomated"  # manual, partiallyAutomated, fullyAutomated
    drsVmBehavior: "fullyAutomated"       # manual, partiallyAutomated, fullyAutomated
    
    # Anti-affinity rules
    antiAffinityGroups: ["web-tier", "database-tier"]
    
    # Host affinity (pin to specific hosts)
    hostAffinityGroups: ["production-hosts"]
</code></pre>
<h3 id="snapshot-management"><a class="header" href="#snapshot-management">Snapshot Management</a></h3>
<pre><code class="language-yaml"># Advanced snapshot configuration
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSnapshot
metadata:
  name: pre-upgrade-snapshot
spec:
  vmRef:
    name: web-application
  
  # Snapshot settings
  name: "Pre-upgrade snapshot"
  description: "Snapshot before application upgrade"
  memory: true                    # Include memory state
  quiesce: true                   # Quiesce guest filesystem
  
  # Retention policy
  retention:
    maxSnapshots: 3               # Keep max 3 snapshots
    maxAge: "7d"                  # Delete after 7 days
    
  # Schedule (optional)
  schedule: "0 2 * * 0"          # Weekly at 2 AM Sunday
</code></pre>
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="common-issues-1"><a class="header" href="#common-issues-1">Common Issues</a></h3>
<h4 id="-connection-failed"><a class="header" href="#-connection-failed">‚ùå Connection Failed</a></h4>
<p><strong>Symptom</strong>: <code>failed to connect to vSphere: connection refused</code></p>
<p><strong>Causes &amp; Solutions</strong>:</p>
<ol>
<li>
<p><strong>Network connectivity</strong>:</p>
<pre><code class="language-bash"># Test connectivity to vCenter
telnet vcenter.example.com 443

# Test from Kubernetes pod
kubectl run debug --rm -i --tty --image=curlimages/curl -- \
  curl -k https://vcenter.example.com
</code></pre>
</li>
<li>
<p><strong>DNS resolution</strong>:</p>
<pre><code class="language-bash"># Test DNS resolution
nslookup vcenter.example.com

# Use IP address if DNS fails
</code></pre>
</li>
<li>
<p><strong>Firewall rules</strong>: Ensure port 443 is accessible from Kubernetes cluster</p>
</li>
</ol>
<h4 id="-authentication-failed"><a class="header" href="#-authentication-failed">‚ùå Authentication Failed</a></h4>
<p><strong>Symptom</strong>: <code>Login failed: incorrect user name or password</code></p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p><strong>Verify credentials</strong>:</p>
<pre><code class="language-bash"># Test credentials manually
kubectl get secret vsphere-credentials -o yaml

# Decode and verify
echo "base64-password" | base64 -d
</code></pre>
</li>
<li>
<p><strong>Check user permissions</strong>:</p>
<ul>
<li>Verify user exists in vCenter</li>
<li>Check assigned roles and privileges</li>
<li>Ensure user is not locked out</li>
</ul>
</li>
<li>
<p><strong>Test login via vSphere Client</strong>: Verify credentials work in the GUI</p>
</li>
</ol>
<h4 id="-insufficient-privileges"><a class="header" href="#-insufficient-privileges">‚ùå Insufficient Privileges</a></h4>
<p><strong>Symptom</strong>: <code>operation requires privilege 'VirtualMachine.Interact.PowerOn'</code></p>
<p><strong>Solution</strong>: Grant required privileges to the service account:</p>
<pre><code class="language-bash"># Required privileges for VirtRigaud:
# - Datastore privileges:
#   * Datastore.AllocateSpace
#   * Datastore.Browse  
#   * Datastore.FileManagement
# - Network privileges:
#   * Network.Assign
# - Resource privileges:
#   * Resource.AssignVMToPool
# - Virtual machine privileges:
#   * VirtualMachine.* (all) or specific subset
# - Global privileges:
#   * Global.EnableMethods
#   * Global.DisableMethods
</code></pre>
<h4 id="-template-not-found"><a class="header" href="#-template-not-found">‚ùå Template Not Found</a></h4>
<p><strong>Symptom</strong>: <code>template 'ubuntu-template' not found</code></p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># List available templates
govc ls /datacenter/vm/templates/

# Check template path and permissions
govc object.collect -s vm/templates/ubuntu-template summary.config.name

# Verify template is properly marked as template
govc object.collect -s vm/templates/ubuntu-template config.template
</code></pre>
<h4 id="-datastore-issues"><a class="header" href="#-datastore-issues">‚ùå Datastore Issues</a></h4>
<p><strong>Symptom</strong>: <code>insufficient disk space</code> or <code>datastore not accessible</code></p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Check datastore capacity
govc datastore.info datastore-name

# List accessible datastores
govc datastore.ls

# Check datastore cluster configuration
govc cluster.ls
</code></pre>
<h4 id="-network-configuration"><a class="header" href="#-network-configuration">‚ùå Network Configuration</a></h4>
<p><strong>Symptom</strong>: <code>network 'VM Network' not found</code></p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># List available networks
govc ls /datacenter/network/

# Check distributed port groups
govc dvs.portgroup.info

# Verify network accessibility from cluster
govc cluster.network.info
</code></pre>
<h3 id="validation-commands"><a class="header" href="#validation-commands">Validation Commands</a></h3>
<p>Test your vSphere setup before deploying:</p>
<pre><code class="language-bash"># 1. Install and configure govc CLI tool
export GOVC_URL='https://vcenter.example.com'
export GOVC_USERNAME='administrator@vsphere.local'
export GOVC_PASSWORD='password'
export GOVC_INSECURE=1  # for self-signed certificates

# 2. Test connectivity
govc about

# 3. List datacenters
govc ls

# 4. List clusters and hosts
govc ls /datacenter/host/

# 5. List datastores
govc ls /datacenter/datastore/

# 6. List networks
govc ls /datacenter/network/

# 7. List templates
govc ls /datacenter/vm/templates/

# 8. Test VM creation (dry run)
govc vm.create -c 1 -m 1024 -g ubuntu64Guest -net "VM Network" test-vm
govc vm.destroy test-vm
</code></pre>
<h3 id="debug-logging"><a class="header" href="#debug-logging">Debug Logging</a></h3>
<p>Enable verbose logging for the vSphere provider:</p>
<pre><code class="language-yaml">providers:
  vsphere:
    env:
      - name: LOG_LEVEL
        value: "debug"
      - name: GOVMOMI_DEBUG
        value: "true"
    endpoint: "https://vcenter.example.com"
</code></pre>
<p>Monitor vSphere tasks:</p>
<pre><code class="language-bash"># Monitor recent tasks in vCenter
govc task.ls

# Get details of specific task
govc task.info task-123
</code></pre>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="resource-allocation"><a class="header" href="#resource-allocation">Resource Allocation</a></h3>
<pre><code class="language-yaml"># High-performance VMClass
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: performance-optimized
spec:
  cpus: 16
  memory: "64Gi"
  spec:
    cpu:
      coresPerSocket: 8            # Match physical CPU topology
      reservationMHz: 8000         # Guarantee CPU resources
      shares: 2000                 # High priority (normal=1000)
      enableVirtualization: false  # Disable if not needed for performance
    
    memory:
      reservationMB: 65536         # Guarantee memory
      shares: 2000                 # High priority
      shareLevel: "high"           # Alternative to shares value
    
    hardware:
      enableCpuHotAdd: false       # Better performance when disabled
      enableMemoryHotAdd: false    # Better performance when disabled
      
    # NUMA configuration for large VMs
    numa:
      enabled: true
      coresPerSocket: 8            # Align with NUMA topology
</code></pre>
<h3 id="storage-optimization"><a class="header" href="#storage-optimization">Storage Optimization</a></h3>
<pre><code class="language-yaml"># Storage-optimized configuration
spec:
  storage:
    diskFormat: "eagerZeroedThick"  # Best performance, more space usage
    controllerType: "pvscsi"        # Paravirtual SCSI for better performance
    multiwriter: false              # Disable unless needed
    
    # vSAN optimization
    storagePolicy: "Performance-Tier"
    cachingPolicy: "writethrough"   # or "writeback" for better performance
    
    # Multiple controllers for high IOPS
    scsiControllers:
      - type: "pvscsi"
        busNumber: 0
        maxDevices: 15
      - type: "pvscsi" 
        busNumber: 1
        maxDevices: 15
</code></pre>
<h3 id="network-optimization"><a class="header" href="#network-optimization">Network Optimization</a></h3>
<pre><code class="language-yaml"># High-performance networking
networks:
  - name: high-performance
    portGroup: "DPG-HighPerf-SR-IOV"
    adapter: "vmxnet3"             # Best performance adapter
    sriov: true                    # SR-IOV for near-native performance
    bandwidth:
      reservation: 1000            # Guaranteed bandwidth (Mbps)
      limit: 10000                 # Maximum bandwidth (Mbps)
      shares: 100                  # Priority level
</code></pre>
<h2 id="api-reference-1"><a class="header" href="#api-reference-1">API Reference</a></h2>
<p>For complete API reference, see the <a href="providers/../api-reference/">Provider API Documentation</a>.</p>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<p>To contribute to the vSphere provider:</p>
<ol>
<li>See the <a href="providers/../tutorial.html">Provider Development Guide</a></li>
<li>Check the <a href="https://github.com/projectbeskar/virtrigaud">GitHub repository</a></li>
<li>Review <a href="https://github.com/projectbeskar/virtrigaud/labels/provider%2Fvsphere">open issues</a></li>
</ol>
<h2 id="support-1"><a class="header" href="#support-1">Support</a></h2>
<ul>
<li><strong>Documentation</strong>: <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud Docs</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub Issues</a></li>
<li><strong>Community</strong>: <a href="https://discord.gg/projectbeskar">Discord</a></li>
<li><strong>VMware</strong>: <a href="https://developer.vmware.com/apis/vsphere-automation/">vSphere API Documentation</a></li>
<li><strong>govc</strong>: <a href="https://github.com/vmware/govmomi/tree/master/govc">govc CLI Tool</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="libvirtkvm-provider"><a class="header" href="#libvirtkvm-provider">LibVirt/KVM Provider</a></h1>
<p>The LibVirt provider enables VirtRigaud to manage virtual machines on KVM/QEMU hypervisors using the LibVirt API. This provider runs as a dedicated pod that communicates with LibVirt daemons locally or remotely, making it ideal for development, on-premises deployments, and cloud environments.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>This provider implements the VirtRigaud provider interface to manage VM lifecycle operations on LibVirt/KVM:</p>
<ul>
<li><strong>Create</strong>: Create VMs from cloud images with comprehensive cloud-init support</li>
<li><strong>Delete</strong>: Remove VMs and associated storage volumes (with cleanup)</li>
<li><strong>Power</strong>: Start, stop, and reboot virtual machines</li>
<li><strong>Describe</strong>: Query VM state, resource usage, guest agent information, and network details</li>
<li><strong>Reconfigure</strong>: Modify VM resources (v0.2.3+ - requires VM restart)</li>
<li><strong>Clone</strong>: Create new VMs based on existing VM configurations</li>
<li><strong>Snapshot</strong>: Create, delete, and revert VM snapshots (storage-dependent)</li>
<li><strong>ConsoleURL</strong>: Generate VNC console URLs for remote access (v0.2.3+)</li>
<li><strong>ImagePrepare</strong>: Download and prepare cloud images from URLs</li>
<li><strong>Storage Management</strong>: Advanced storage pool and volume operations</li>
<li><strong>Cloud-Init</strong>: Full NoCloud datasource support with ISO generation</li>
<li><strong>QEMU Guest Agent</strong>: Integration for enhanced guest OS monitoring</li>
<li><strong>Network Configuration</strong>: Support for various network types and bridges</li>
</ul>
<h2 id="prerequisites-3"><a class="header" href="#prerequisites-3">Prerequisites</a></h2>
<p>The LibVirt provider connects to a LibVirt daemon (libvirtd) which can run locally or remotely. This makes it flexible for both development and production environments.</p>
<h3 id="connection-options"><a class="header" href="#connection-options">Connection Options:</a></h3>
<ul>
<li><strong>Local LibVirt</strong>: Connects to local libvirtd via <code>qemu:///system</code> (ideal for development)</li>
<li><strong>Remote LibVirt</strong>: Connects to remote libvirtd over SSH/TLS (production)</li>
<li><strong>Container LibVirt</strong>: Works with containerized libvirt or KubeVirt</li>
</ul>
<h3 id="requirements-1"><a class="header" href="#requirements-1">Requirements:</a></h3>
<ul>
<li><strong>LibVirt daemon</strong> (libvirtd) running locally or accessible remotely</li>
<li><strong>KVM/QEMU</strong> hypervisor support (hardware virtualization recommended)</li>
<li><strong>Storage pools</strong> configured for VM disk storage</li>
<li><strong>Network bridges</strong> or interfaces for VM networking</li>
<li><strong>Appropriate permissions</strong> for VM management operations</li>
</ul>
<h3 id="development-setup"><a class="header" href="#development-setup">Development Setup:</a></h3>
<p>For local development, you can:</p>
<ul>
<li><strong>Linux</strong>: Install <code>libvirt-daemon-system</code> and <code>qemu-kvm</code> packages</li>
<li><strong>macOS/Windows</strong>: Use remote LibVirt or nested virtualization</li>
<li><strong>Testing</strong>: The provider can connect to local libvirtd without complex infrastructure</li>
</ul>
<h2 id="authentication--connection"><a class="header" href="#authentication--connection">Authentication &amp; Connection</a></h2>
<p>The LibVirt provider supports multiple connection methods:</p>
<h3 id="local-libvirt-connection"><a class="header" href="#local-libvirt-connection">Local LibVirt Connection</a></h3>
<p>For connecting to a LibVirt daemon on the same host as the provider pod:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-local
  namespace: default
spec:
  type: libvirt
  endpoint: "qemu:///system"  # Local system connection
  credentialSecretRef:
    name: libvirt-local-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.3"
    service:
      port: 9090
</code></pre>
<p><strong>Note</strong>: When using local connections, ensure the provider pod has appropriate permissions to access the LibVirt socket.</p>
<h3 id="remote-connection-with-ssh"><a class="header" href="#remote-connection-with-ssh">Remote Connection with SSH</a></h3>
<p>For remote LibVirt over SSH:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-remote
  namespace: default
spec:
  type: libvirt
  endpoint: "qemu+ssh://user@libvirt-host/system"
  credentialSecretRef:
    name: libvirt-ssh-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.3"
    service:
      port: 9090
</code></pre>
<p>Create SSH credentials secret:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: libvirt-ssh-credentials
  namespace: default
type: Opaque
stringData:
  username: "libvirt-user"
  # For key-based auth (recommended):
  tls.key: |
    -----BEGIN PRIVATE KEY-----
    # Your SSH private key here
    -----END PRIVATE KEY-----
  # For password auth (less secure):
  password: "your-password"
</code></pre>
<h3 id="remote-connection-with-tls"><a class="header" href="#remote-connection-with-tls">Remote Connection with TLS</a></h3>
<p>For remote LibVirt over TLS:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-tls
  namespace: default
spec:
  type: libvirt
  endpoint: "qemu+tls://libvirt-host:16514/system"
  credentialSecretRef:
    name: libvirt-tls-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.3"
    service:
      port: 9090
</code></pre>
<p>Create TLS credentials secret:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: libvirt-tls-credentials
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: # Base64 encoded client certificate
  tls.key: # Base64 encoded client private key
  ca.crt:  # Base64 encoded CA certificate
</code></pre>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<h3 id="connection-uris"><a class="header" href="#connection-uris">Connection URIs</a></h3>
<p>The LibVirt provider supports standard LibVirt connection URIs:</p>
<div class="table-wrapper"><table><thead><tr><th>URI Format</th><th>Description</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>qemu:///system</code></td><td>Local system connection</td><td>Development, single-host</td></tr>
<tr><td><code>qemu+ssh://user@host/system</code></td><td>SSH connection</td><td>Remote access with SSH</td></tr>
<tr><td><code>qemu+tls://host:16514/system</code></td><td>TLS connection</td><td>Secure remote access</td></tr>
<tr><td><code>qemu+tcp://host:16509/system</code></td><td>TCP connection</td><td>Insecure remote (testing only)</td></tr>
</tbody></table>
</div>
<p><strong>‚ö†Ô∏è Note</strong>: All LibVirt URI schemes are now supported in the CRD validation pattern.</p>
<h3 id="deployment-configuration-1"><a class="header" href="#deployment-configuration-1">Deployment Configuration</a></h3>
<h4 id="using-helm-values-1"><a class="header" href="#using-helm-values-1">Using Helm Values</a></h4>
<pre><code class="language-yaml"># values.yaml
providers:
  libvirt:
    enabled: true
    endpoint: "qemu:///system"  # Adjust for your environment
    # For remote connections:
    # endpoint: "qemu+ssh://user@libvirt-host/system"
    credentialSecretRef:
      name: libvirt-credentials  # Optional for local connections
</code></pre>
<h4 id="development-configuration-1"><a class="header" href="#development-configuration-1">Development Configuration</a></h4>
<pre><code class="language-yaml"># For local development with LibVirt
providers:
  libvirt:
    enabled: true
    endpoint: "qemu:///system"
    runtime:
      # Mount host libvirt socket (for local access)
      volumes:
      - name: libvirt-sock
        hostPath:
          path: /var/run/libvirt/libvirt-sock
      volumeMounts:
      - name: libvirt-sock
        mountPath: /var/run/libvirt/libvirt-sock
</code></pre>
<h4 id="production-configuration"><a class="header" href="#production-configuration">Production Configuration</a></h4>
<pre><code class="language-yaml"># For production with remote LibVirt
apiVersion: v1
kind: Secret
metadata:
  name: libvirt-credentials
  namespace: virtrigaud-system
type: Opaque
stringData:
  username: "virtrigaud-service"
  tls.crt: |
    -----BEGIN CERTIFICATE-----
    # Client certificate for TLS authentication
    -----END CERTIFICATE-----
  tls.key: |
    -----BEGIN PRIVATE KEY-----
    # Client private key
    -----END PRIVATE KEY-----
  ca.crt: |
    -----BEGIN CERTIFICATE-----
    # CA certificate
    -----END CERTIFICATE-----

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-production
  namespace: virtrigaud-system
spec:
  type: libvirt
  endpoint: "qemu+tls://libvirt.example.com:16514/system"
  credentialSecretRef:
    name: libvirt-credentials
</code></pre>
<h2 id="storage-configuration"><a class="header" href="#storage-configuration">Storage Configuration</a></h2>
<h3 id="storage-pools"><a class="header" href="#storage-pools">Storage Pools</a></h3>
<p>LibVirt requires storage pools for VM disks. Common configurations:</p>
<pre><code class="language-bash"># Create directory-based storage pool
virsh pool-define-as default dir --target /var/lib/libvirt/images
virsh pool-build default
virsh pool-start default
virsh pool-autostart default

# Create LVM-based storage pool (performance)
virsh pool-define-as lvm-pool logical --source-name vg-libvirt --target /dev/vg-libvirt
virsh pool-start lvm-pool
virsh pool-autostart lvm-pool
</code></pre>
<h3 id="vmclass-storage-specification"><a class="header" href="#vmclass-storage-specification">VMClass Storage Specification</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: standard
spec:
  cpus: 2
  memory: "4Gi"
  # LibVirt-specific storage settings
  spec:
    storage:
      pool: "default"        # Storage pool name
      format: "qcow2"        # Disk format (qcow2, raw)
      cache: "writethrough"  # Cache mode
      io: "threads"          # I/O mode
</code></pre>
<h2 id="network-configuration-1"><a class="header" href="#network-configuration-1">Network Configuration</a></h2>
<h3 id="network-setup"><a class="header" href="#network-setup">Network Setup</a></h3>
<p>Configure LibVirt networks for VM connectivity:</p>
<pre><code class="language-bash"># Create NAT network (default)
virsh net-define /usr/share/libvirt/networks/default.xml
virsh net-start default
virsh net-autostart default

# Create bridge network (for external access)
cat &gt; /tmp/bridge-network.xml &lt;&lt; EOF
&lt;network&gt;
  &lt;name&gt;br0&lt;/name&gt;
  &lt;forward mode='bridge'/&gt;
  &lt;bridge name='br0'/&gt;
&lt;/network&gt;
EOF
virsh net-define /tmp/bridge-network.xml
virsh net-start br0
</code></pre>
<h3 id="network-bridge-mapping"><a class="header" href="#network-bridge-mapping">Network Bridge Mapping</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Network Name</th><th>LibVirt Network</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>default</code>, <code>nat</code></td><td>default</td><td>NAT networking</td></tr>
<tr><td><code>bridge</code>, <code>br0</code></td><td>br0</td><td>Bridged networking</td></tr>
<tr><td><code>isolated</code></td><td>isolated</td><td>Host-only networking</td></tr>
</tbody></table>
</div>
<h3 id="vm-network-configuration"><a class="header" href="#vm-network-configuration">VM Network Configuration</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  providerRef:
    name: libvirt-local
  networks:
    # Use default NAT network
    - name: default
    # Use bridged network for external access
    - name: bridge
      bridge: br0
      mac: "52:54:00:12:34:56"  # Optional MAC address
</code></pre>
<h2 id="vm-configuration-1"><a class="header" href="#vm-configuration-1">VM Configuration</a></h2>
<h3 id="vmclass-specification-1"><a class="header" href="#vmclass-specification-1">VMClass Specification</a></h3>
<p>Define hardware resources and LibVirt-specific settings:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: development
spec:
  cpus: 2
  memory: "4Gi"
  # LibVirt-specific configuration
  spec:
    machine: "pc-i440fx-2.12"  # Machine type
    cpu:
      mode: "host-model"       # CPU mode (host-model, host-passthrough)
      topology:
        sockets: 1
        cores: 2
        threads: 1
    features:
      acpi: true
      apic: true
      pae: true
    clock:
      offset: "utc"
      timers:
        rtc: "catchup"
        pit: "delay"
        hpet: false
</code></pre>
<h3 id="vmimage-specification-1"><a class="header" href="#vmimage-specification-1">VMImage Specification</a></h3>
<p>Reference existing disk images or templates:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-22-04
spec:
  source:
    # Path to existing image in storage pool
    disk: "/var/lib/libvirt/images/ubuntu-22.04-base.qcow2"
    # Or reference by pool and volume
    # pool: "default"
    # volume: "ubuntu-22.04-base"
  format: "qcow2"
  
  # Cloud-init preparation
  cloudInit:
    enabled: true
    userDataTemplate: |
      #cloud-config
      hostname: {{"{{ .Name }}"}}
      users:
        - name: ubuntu
          sudo: ALL=(ALL) NOPASSWD:ALL
          ssh_authorized_keys:
            - {{"{{ .SSHPublicKey }}"}}
</code></pre>
<h3 id="complete-vm-example-1"><a class="header" href="#complete-vm-example-1">Complete VM Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: dev-workstation
spec:
  providerRef:
    name: libvirt-local
  classRef:
    name: development
  imageRef:
    name: ubuntu-22-04
  powerState: On
  
  # Disk configuration
  disks:
    - name: root
      size: "50Gi"
      storageClass: "fast-ssd"  # Maps to LibVirt storage pool
  
  # Network configuration  
  networks:
    - name: default  # NAT network for internet
    - name: bridge   # Bridge for LAN access
      staticIP:
        address: "192.168.1.100/24"
        gateway: "192.168.1.1"
        dns: ["8.8.8.8", "1.1.1.1"]
  
  # Cloud-init user data
  userData:
    cloudInit:
      inline: |
        #cloud-config
        hostname: dev-workstation
        users:
          - name: developer
            sudo: ALL=(ALL) NOPASSWD:ALL
            shell: /bin/bash
            ssh_authorized_keys:
              - "ssh-ed25519 AAAA..."
        packages:
          - build-essential
          - docker.io
          - code
        runcmd:
          - systemctl enable docker
          - usermod -aG docker developer
</code></pre>
<h2 id="cloud-init-integration"><a class="header" href="#cloud-init-integration">Cloud-Init Integration</a></h2>
<h3 id="automatic-configuration"><a class="header" href="#automatic-configuration">Automatic Configuration</a></h3>
<p>The LibVirt provider automatically handles cloud-init setup:</p>
<ul>
<li><strong>ISO Generation</strong>: Creates cloud-init ISO with user-data and meta-data</li>
<li><strong>Attachment</strong>: Attaches ISO as CD-ROM device to VM</li>
<li><strong>Network Config</strong>: Generates network configuration from VM spec</li>
<li><strong>User Data</strong>: Renders templates with VM-specific values</li>
</ul>
<h3 id="advanced-cloud-init"><a class="header" href="#advanced-cloud-init">Advanced Cloud-Init</a></h3>
<pre><code class="language-yaml">userData:
  cloudInit:
    inline: |
      #cloud-config
      hostname: {{"{{ .Name }}"}}
      
      # Network configuration (if not using DHCP)
      network:
        version: 2
        ethernets:
          ens3:
            addresses: [192.168.1.100/24]
            gateway4: 192.168.1.1
            nameservers:
              addresses: [8.8.8.8, 1.1.1.1]
      
      # Storage configuration
      disk_setup:
        /dev/vdb:
          table_type: gpt
          layout: true
      
      fs_setup:
        - device: /dev/vdb1
          filesystem: ext4
          label: data
      
      mounts:
        - [/dev/vdb1, /data, ext4, defaults]
      
      # Package installation
      packages:
        - qemu-guest-agent  # Enable guest agent
        - cloud-init
        - curl
      
      # Enable services
      runcmd:
        - systemctl enable qemu-guest-agent
        - systemctl start qemu-guest-agent
</code></pre>
<h2 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h2>
<h3 id="kvm-optimization"><a class="header" href="#kvm-optimization">KVM Optimization</a></h3>
<pre><code class="language-yaml"># VMClass with performance optimizations
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: high-performance
spec:
  cpus: 8
  memory: "16Gi"
  spec:
    cpu:
      mode: "host-passthrough"  # Best performance
      topology:
        sockets: 1
        cores: 8
        threads: 1
    # NUMA topology for large VMs
    numa:
      cells:
        - id: 0
          cpus: "0-7"
          memory: "16"
    
    # Virtio devices for performance
    devices:
      disk:
        bus: "virtio"
        cache: "none"
        io: "native"
      network:
        model: "virtio"
      video:
        model: "virtio"
</code></pre>
<h3 id="storage-performance"><a class="header" href="#storage-performance">Storage Performance</a></h3>
<pre><code class="language-bash"># Create high-performance storage pool
virsh pool-define-as ssd-pool logical --source-name vg-ssd --target /dev/vg-ssd
virsh pool-start ssd-pool

# Use raw format for better performance (larger disk usage)
virsh vol-create-as ssd-pool vm-disk 100G --format raw

# Enable native AIO and disable cache for direct I/O
# (configured automatically by provider based on VMClass)
</code></pre>
<h2 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h2>
<h3 id="common-issues-2"><a class="header" href="#common-issues-2">Common Issues</a></h3>
<h4 id="-connection-failed-1"><a class="header" href="#-connection-failed-1">‚ùå Connection Failed</a></h4>
<p><strong>Symptom</strong>: <code>failed to connect to Libvirt: &lt;error&gt;</code></p>
<p><strong>Causes &amp; Solutions</strong>:</p>
<ol>
<li>
<p><strong>Local connection issues</strong>:</p>
<pre><code class="language-bash"># Check libvirtd status
sudo systemctl status libvirtd

# Start if not running
sudo systemctl start libvirtd
sudo systemctl enable libvirtd

# Test connection
virsh -c qemu:///system list
</code></pre>
</li>
<li>
<p><strong>Remote SSH connection</strong>:</p>
<pre><code class="language-bash"># Test SSH connectivity
ssh user@libvirt-host virsh list

# Check SSH key permissions
chmod 600 ~/.ssh/id_rsa
</code></pre>
</li>
<li>
<p><strong>Remote TLS connection</strong>:</p>
<pre><code class="language-bash"># Verify certificates
openssl x509 -in client-cert.pem -text -noout

# Test TLS connection
virsh -c qemu+tls://host:16514/system list
</code></pre>
</li>
</ol>
<h4 id="-permission-denied"><a class="header" href="#-permission-denied">‚ùå Permission Denied</a></h4>
<p><strong>Symptom</strong>: <code>authentication failed</code> or <code>permission denied</code></p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Add user to libvirt group
sudo usermod -a -G libvirt $USER

# Check libvirt group membership
groups $USER

# Verify permissions on libvirt socket
ls -la /var/run/libvirt/libvirt-sock

# For containerized providers, ensure socket is mounted
</code></pre>
<h4 id="-storage-pool-not-found"><a class="header" href="#-storage-pool-not-found">‚ùå Storage Pool Not Found</a></h4>
<p><strong>Symptom</strong>: <code>storage pool 'default' not found</code></p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># List available pools
virsh pool-list --all

# Create default pool if missing
virsh pool-define-as default dir --target /var/lib/libvirt/images
virsh pool-build default
virsh pool-start default
virsh pool-autostart default

# Verify pool is active
virsh pool-info default
</code></pre>
<h4 id="-network-not-available"><a class="header" href="#-network-not-available">‚ùå Network Not Available</a></h4>
<p><strong>Symptom</strong>: <code>network 'default' not found</code></p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># List networks
virsh net-list --all

# Start default network
virsh net-start default
virsh net-autostart default

# Create bridge network if needed
virsh net-define /usr/share/libvirt/networks/default.xml
</code></pre>
<h4 id="-kvm-not-available"><a class="header" href="#-kvm-not-available">‚ùå KVM Not Available</a></h4>
<p><strong>Symptom</strong>: <code>KVM is not available</code> or <code>hardware acceleration not available</code></p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p><strong>Check virtualization support</strong>:</p>
<pre><code class="language-bash"># Check CPU virtualization features
egrep -c '(vmx|svm)' /proc/cpuinfo

# Check KVM modules
lsmod | grep kvm

# Load KVM modules if missing
sudo modprobe kvm
sudo modprobe kvm_intel  # or kvm_amd
</code></pre>
</li>
<li>
<p><strong>BIOS/UEFI settings</strong>: Enable Intel VT-x or AMD-V</p>
</li>
<li>
<p><strong>Nested virtualization</strong>: If running in a VM, enable nested virtualization</p>
</li>
</ol>
<h3 id="validation-commands-1"><a class="header" href="#validation-commands-1">Validation Commands</a></h3>
<p>Test your LibVirt setup before deploying:</p>
<pre><code class="language-bash"># 1. Test LibVirt connection
virsh -c qemu:///system list

# 2. Check storage pools
virsh pool-list --all

# 3. Check networks
virsh net-list --all

# 4. Test VM creation (simple test)
virt-install --name test-vm --memory 512 --vcpus 1 \
  --disk size=1 --network network=default \
  --boot cdrom --noautoconsole --dry-run

# 5. From within Kubernetes pod
kubectl run debug --rm -i --tty --image=ubuntu:22.04 -- bash
# Then test virsh commands if socket is mounted
</code></pre>
<h3 id="debug-logging-1"><a class="header" href="#debug-logging-1">Debug Logging</a></h3>
<p>Enable verbose logging for the LibVirt provider:</p>
<pre><code class="language-yaml">providers:
  libvirt:
    env:
      - name: LOG_LEVEL
        value: "debug"
      - name: LIBVIRT_DEBUG
        value: "1"
    endpoint: "qemu:///system"
</code></pre>
<h2 id="advanced-features-3"><a class="header" href="#advanced-features-3">Advanced Features</a></h2>
<h3 id="vm-reconfiguration-v023-1"><a class="header" href="#vm-reconfiguration-v023-1">VM Reconfiguration (v0.2.3+)</a></h3>
<p>The Libvirt provider supports VM reconfiguration for CPU, memory, and disk resources:</p>
<pre><code class="language-yaml"># Reconfigure VM resources
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  vmClassRef: medium  # Change from small to medium
  powerState: "On"
</code></pre>
<p><strong>Capabilities</strong>:</p>
<ul>
<li><strong>Online CPU Changes</strong>: Modify CPU count using <code>virsh setvcpus --live</code> for running VMs</li>
<li><strong>Online Memory Changes</strong>: Modify memory using <code>virsh setmem --live</code> for running VMs</li>
<li><strong>Disk Resizing</strong>: Expand disk volumes via storage provider integration</li>
<li><strong>Offline Configuration</strong>: Updates persistent config for stopped VMs via <code>--config</code> flag</li>
</ul>
<p><strong>Important Notes</strong>:</p>
<ul>
<li>Most changes require VM restart for full effect</li>
<li>Online changes apply to running VM but may need restart for persistence</li>
<li>Disk shrinking not supported for safety</li>
<li>Memory format parsing supports bytes, KiB, MiB, GiB</li>
</ul>
<p><strong>Implementation Details</strong>:</p>
<ul>
<li>Uses <code>virsh setvcpus --live --config</code> for CPU changes</li>
<li>Uses <code>virsh setmem --live --config</code> for memory changes</li>
<li>Parses current VM configuration with <code>virsh dominfo</code></li>
<li>Integrates with storage provider for volume resizing</li>
</ul>
<h3 id="vnc-console-access-v023"><a class="header" href="#vnc-console-access-v023">VNC Console Access (v0.2.3+)</a></h3>
<p>Generate VNC console URLs for direct VM access:</p>
<pre><code class="language-yaml"># Access provided in VM status
kubectl get vm web-server -o yaml

status:
  consoleURL: "vnc://libvirt-host.example.com:5900"
  phase: Running
</code></pre>
<p><strong>Features</strong>:</p>
<ul>
<li>Automatic VNC port extraction from domain XML</li>
<li>Direct connection URLs for VNC clients</li>
<li>Support for standard VNC viewers (TigerVNC, RealVNC, etc.)</li>
<li>Web-based VNC viewers compatible (noVNC)</li>
</ul>
<p><strong>VNC Client Usage</strong>:</p>
<pre><code class="language-bash"># Using vncviewer
vncviewer libvirt-host.example.com:5900

# Using TigerVNC
tigervnc libvirt-host.example.com:5900

# Web browser (with noVNC)
# Access through web-based VNC proxy
</code></pre>
<p><strong>Configuration</strong>:
VNC is automatically configured during VM creation. The provider:</p>
<ol>
<li>Extracts VNC configuration from domain XML using <code>virsh dumpxml</code></li>
<li>Parses the graphics port number</li>
<li>Constructs the VNC URL with host and port</li>
<li>Returns URL in Describe operations</li>
</ol>
<h2 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h2>
<h3 id="high-availability-setup"><a class="header" href="#high-availability-setup">High Availability Setup</a></h3>
<pre><code class="language-yaml"># Multiple LibVirt hosts for HA
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-cluster
spec:
  type: libvirt
  # Use load balancer or failover endpoint
  endpoint: "qemu+tls://libvirt-cluster.example.com:16514/system"
  runtime:
    replicas: 2  # Multiple provider instances
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app: libvirt-provider
            topologyKey: kubernetes.io/hostname
</code></pre>
<h3 id="gpu-passthrough"><a class="header" href="#gpu-passthrough">GPU Passthrough</a></h3>
<pre><code class="language-yaml"># VMClass with GPU passthrough
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: gpu-workstation
spec:
  cpus: 8
  memory: "32Gi"
  spec:
    devices:
      hostdev:
        - type: "pci"
          source:
            address:
              domain: "0x0000"
              bus: "0x01"
              slot: "0x00"
              function: "0x0"
          managed: true
</code></pre>
<h2 id="api-reference-2"><a class="header" href="#api-reference-2">API Reference</a></h2>
<p>For complete API reference, see the <a href="providers/../api-reference/">Provider API Documentation</a>.</p>
<h2 id="contributing-1"><a class="header" href="#contributing-1">Contributing</a></h2>
<p>To contribute to the LibVirt provider:</p>
<ol>
<li>See the <a href="providers/../tutorial.html">Provider Development Guide</a></li>
<li>Check the <a href="https://github.com/projectbeskar/virtrigaud">GitHub repository</a></li>
<li>Review <a href="https://github.com/projectbeskar/virtrigaud/labels/provider%2Flibvirt">open issues</a></li>
</ol>
<h2 id="support-2"><a class="header" href="#support-2">Support</a></h2>
<ul>
<li><strong>Documentation</strong>: <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud Docs</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub Issues</a></li>
<li><strong>Community</strong>: <a href="https://discord.gg/projectbeskar">Discord</a></li>
<li><strong>LibVirt</strong>: <a href="https://libvirt.org/docs.html">libvirt.org</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="proxmox-ve-provider"><a class="header" href="#proxmox-ve-provider">Proxmox VE Provider</a></h1>
<p>The Proxmox VE provider enables VirtRigaud to manage virtual machines on Proxmox Virtual Environment (PVE) clusters using the native Proxmox API.</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>This provider implements the VirtRigaud provider interface to manage VM lifecycle operations on Proxmox VE:</p>
<ul>
<li><strong>Create</strong>: Create VMs from templates or ISO images with cloud-init support</li>
<li><strong>Delete</strong>: Remove VMs and associated resources</li>
<li><strong>Power</strong>: Start, stop, and reboot virtual machines</li>
<li><strong>Describe</strong>: Query VM state, IPs, and console access</li>
<li><strong>Guest Agent Integration</strong>: Enhanced IP detection via QEMU guest agent (v0.2.3+)</li>
<li><strong>Reconfigure</strong>: Hot-plug CPU/memory changes, disk expansion</li>
<li><strong>Clone</strong>: Create linked or full clones of existing VMs</li>
<li><strong>Snapshot</strong>: Create, delete, and revert VM snapshots with memory state</li>
<li><strong>ImagePrepare</strong>: Import and prepare VM templates from URLs or ensure existence</li>
</ul>
<h2 id="prerequisites-4"><a class="header" href="#prerequisites-4">Prerequisites</a></h2>
<p><strong>‚ö†Ô∏è IMPORTANT: Active Proxmox VE Server Required</strong></p>
<p>The Proxmox provider requires a running Proxmox VE server to function. Unlike some providers that can operate in simulation mode, this provider performs actual API calls to Proxmox VE during startup and operation.</p>
<h3 id="requirements-2"><a class="header" href="#requirements-2">Requirements:</a></h3>
<ul>
<li><strong>Proxmox VE 7.0 or later</strong> (running and accessible)</li>
<li><strong>API token or user account</strong> with appropriate privileges</li>
<li><strong>Network connectivity</strong> from VirtRigaud to Proxmox API (port 8006/HTTPS)</li>
<li><strong>Valid TLS configuration</strong> (production) or skip verification (development)</li>
</ul>
<h3 id="testingdevelopment-1"><a class="header" href="#testingdevelopment-1">Testing/Development:</a></h3>
<p>If you don‚Äôt have a Proxmox VE server available:</p>
<ul>
<li>Use <a href="https://pve.proxmox.com/wiki/Installation">Proxmox VE in a VM</a> for testing</li>
<li>Consider alternative providers (libvirt, vSphere) for local development</li>
<li>The provider will fail startup validation without a reachable Proxmox endpoint</li>
</ul>
<h2 id="authentication-1"><a class="header" href="#authentication-1">Authentication</a></h2>
<p>The Proxmox provider supports two authentication methods:</p>
<h3 id="api-token-authentication-recommended"><a class="header" href="#api-token-authentication-recommended">API Token Authentication (Recommended)</a></h3>
<p>API tokens provide secure, scope-limited access without exposing user passwords.</p>
<ol>
<li>
<p><strong>Create API Token in Proxmox</strong>:</p>
<pre><code class="language-bash"># In Proxmox web UI: Datacenter -&gt; Permissions -&gt; API Tokens
# Or via CLI:
pveum user token add &lt;USER@REALM&gt; &lt;TOKENID&gt; --privsep 0
</code></pre>
</li>
<li>
<p><strong>Configure Provider</strong>:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: proxmox-prod
  namespace: default
spec:
  type: proxmox
  endpoint: https://pve.example.com:8006
  credentialSecretRef:
    name: pve-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-proxmox:v0.2.3"
    service:
      port: 9090
</code></pre>
</li>
<li>
<p><strong>Create Credentials Secret</strong>:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: pve-credentials
  namespace: default
type: Opaque
stringData:
  token_id: "virtrigaud@pve!vrtg-token"
  token_secret: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
</code></pre>
</li>
</ol>
<h3 id="session-cookie-authentication-optional"><a class="header" href="#session-cookie-authentication-optional">Session Cookie Authentication (Optional)</a></h3>
<p>For environments that cannot use API tokens:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: pve-credentials
  namespace: default
type: Opaque
stringData:
  username: "virtrigaud@pve"
  password: "secure-password"
</code></pre>
<h2 id="deployment-configuration-2"><a class="header" href="#deployment-configuration-2">Deployment Configuration</a></h2>
<h3 id="required-environment-variables"><a class="header" href="#required-environment-variables">Required Environment Variables</a></h3>
<p>The Proxmox provider <strong>requires</strong> environment variables to connect to your Proxmox VE server. Configure these variables in your Helm values file:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Required</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>PVE_ENDPOINT</code></td><td>‚úÖ <strong>Yes</strong></td><td>Proxmox VE API endpoint URL</td><td><code>https://pve.example.com:8006</code></td></tr>
<tr><td><code>PVE_USERNAME</code></td><td>‚úÖ <strong>Yes</strong>*</td><td>Username for password auth</td><td><code>root@pam</code> or <code>user@realm</code></td></tr>
<tr><td><code>PVE_PASSWORD</code></td><td>‚úÖ <strong>Yes</strong>*</td><td>Password for username</td><td><code>secure-password</code></td></tr>
<tr><td><code>PVE_TOKEN_ID</code></td><td>‚úÖ <strong>Yes</strong>**</td><td>API token ID (alternative)</td><td><code>user@realm!tokenid</code></td></tr>
<tr><td><code>PVE_TOKEN_SECRET</code></td><td>‚úÖ <strong>Yes</strong>**</td><td>API token secret (alternative)</td><td><code>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code></td></tr>
<tr><td><code>PVE_INSECURE_SKIP_VERIFY</code></td><td>üîµ Optional</td><td>Skip TLS verification</td><td><code>true</code> (dev only)</td></tr>
</tbody></table>
</div>
<blockquote>
<p><strong>*</strong> Either username/password OR token authentication is required<br />
<strong>**</strong> API token authentication is recommended for production</p>
</blockquote>
<h3 id="helm-configuration-examples"><a class="header" href="#helm-configuration-examples">Helm Configuration Examples</a></h3>
<h4 id="usernamepassword-authentication"><a class="header" href="#usernamepassword-authentication">Username/Password Authentication</a></h4>
<pre><code class="language-yaml"># values.yaml
providers:
  proxmox:
    enabled: true
    env:
      - name: PVE_ENDPOINT
        value: "https://your-proxmox-server.example.com:8006"
      - name: PVE_USERNAME
        value: "root@pam"
      - name: PVE_PASSWORD
        value: "your-secure-password"
</code></pre>
<h4 id="api-token-authentication-recommended-1"><a class="header" href="#api-token-authentication-recommended-1">API Token Authentication (Recommended)</a></h4>
<pre><code class="language-yaml"># values.yaml  
providers:
  proxmox:
    enabled: true
    env:
      - name: PVE_ENDPOINT
        value: "https://your-proxmox-server.example.com:8006"
      - name: PVE_TOKEN_ID
        value: "virtrigaud@pve!automation"
      - name: PVE_TOKEN_SECRET
        value: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
</code></pre>
<h4 id="using-kubernetes-secrets-production"><a class="header" href="#using-kubernetes-secrets-production">Using Kubernetes Secrets (Production)</a></h4>
<p>For production environments, use Kubernetes secrets:</p>
<pre><code class="language-yaml"># Create secret first
apiVersion: v1
kind: Secret
metadata:
  name: proxmox-credentials
type: Opaque
stringData:
  PVE_ENDPOINT: "https://your-proxmox-server.example.com:8006"
  PVE_TOKEN_ID: "virtrigaud@pve!automation"  
  PVE_TOKEN_SECRET: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

---
# values.yaml - Reference the secret
providers:
  proxmox:
    enabled: true
    env:
      - name: PVE_ENDPOINT
        valueFrom:
          secretKeyRef:
            name: proxmox-credentials
            key: PVE_ENDPOINT
      - name: PVE_TOKEN_ID
        valueFrom:
          secretKeyRef:
            name: proxmox-credentials
            key: PVE_TOKEN_ID
      - name: PVE_TOKEN_SECRET
        valueFrom:
          secretKeyRef:
            name: proxmox-credentials
            key: PVE_TOKEN_SECRET
</code></pre>
<h3 id="configuration-validation"><a class="header" href="#configuration-validation">Configuration Validation</a></h3>
<p>The provider validates configuration at startup and will <strong>fail to start</strong> if:</p>
<ul>
<li>‚úÖ <code>PVE_ENDPOINT</code> is missing or invalid</li>
<li>‚úÖ Neither username/password nor token credentials are provided</li>
<li>‚úÖ Proxmox server is unreachable</li>
<li>‚úÖ Authentication fails</li>
</ul>
<h4 id="error-examples"><a class="header" href="#error-examples">Error Examples</a></h4>
<pre><code class="language-bash"># Missing endpoint
ERROR Failed to create PVE client error="endpoint is required"

# Invalid endpoint format  
ERROR Failed to create PVE client error="invalid endpoint URL"

# Authentication failure
ERROR Failed to authenticate error="authentication failed: invalid credentials"

# Connection failure
ERROR Failed to connect error="dial tcp: no route to host"
</code></pre>
<h3 id="development-vs-production"><a class="header" href="#development-vs-production">Development vs Production</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Environment</th><th>Endpoint</th><th>Authentication</th><th>TLS</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Development</strong></td><td><code>https://pve-test.local:8006</code></td><td>Username/Password</td><td>Skip verify</td><td>Use <code>PVE_INSECURE_SKIP_VERIFY=true</code></td></tr>
<tr><td><strong>Staging</strong></td><td><code>https://pve-staging.company.com:8006</code></td><td>API Token</td><td>Custom CA</td><td>Configure CA bundle</td></tr>
<tr><td><strong>Production</strong></td><td><code>https://pve.company.com:8006</code></td><td>API Token</td><td>Valid cert</td><td>Use Kubernetes secrets</td></tr>
</tbody></table>
</div>
<h2 id="tls-configuration"><a class="header" href="#tls-configuration">TLS Configuration</a></h2>
<h3 id="self-signed-certificates-development"><a class="header" href="#self-signed-certificates-development">Self-Signed Certificates (Development)</a></h3>
<p>For test environments with self-signed certificates:</p>
<pre><code class="language-yaml">spec:
  runtime:
    env:
      - name: PVE_INSECURE_SKIP_VERIFY
        value: "true"
</code></pre>
<h3 id="custom-ca-certificate-production"><a class="header" href="#custom-ca-certificate-production">Custom CA Certificate (Production)</a></h3>
<p>For production with custom CA:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: pve-credentials
type: Opaque
stringData:
  ca.crt: |
    -----BEGIN CERTIFICATE-----
    MIIDXTCCAkWgAwIBAgIJAL...
    -----END CERTIFICATE-----
</code></pre>
<h2 id="reconfiguration-support"><a class="header" href="#reconfiguration-support">Reconfiguration Support</a></h2>
<h3 id="online-reconfiguration"><a class="header" href="#online-reconfiguration">Online Reconfiguration</a></h3>
<p>The Proxmox provider supports online (hot-plug) reconfiguration for:</p>
<ul>
<li><strong>CPU</strong>: Add/remove vCPUs while VM is running (guest OS support required)</li>
<li><strong>Memory</strong>: Increase memory using balloon driver (guest tools required)</li>
<li><strong>Disk Expansion</strong>: Expand disks online (disk shrinking not supported)</li>
</ul>
<h3 id="reconfigure-matrix"><a class="header" href="#reconfigure-matrix">Reconfigure Matrix</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Online Support</th><th>Requirements</th><th>Notes</th></tr></thead><tbody>
<tr><td>CPU increase</td><td>‚úÖ Yes</td><td>Guest OS support</td><td>Most modern Linux/Windows</td></tr>
<tr><td>CPU decrease</td><td>‚úÖ Yes</td><td>Guest OS support</td><td>May require guest cooperation</td></tr>
<tr><td>Memory increase</td><td>‚úÖ Yes</td><td>Balloon driver</td><td>Install qemu-guest-agent</td></tr>
<tr><td>Memory decrease</td><td>‚ö†Ô∏è Limited</td><td>Balloon driver + guest</td><td>May require power cycle</td></tr>
<tr><td>Disk expand</td><td>‚úÖ Yes</td><td>Online resize support</td><td>Filesystem resize separate</td></tr>
<tr><td>Disk shrink</td><td>‚ùå No</td><td>Not supported</td><td>Security/data protection</td></tr>
</tbody></table>
</div>
<h3 id="example-reconfiguration"><a class="header" href="#example-reconfiguration">Example Reconfiguration</a></h3>
<pre><code class="language-yaml"># Scale up VM resources
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  # ... existing spec ...
  classRef:
    name: large  # Changed from 'small'
---
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: large
spec:
  cpus: 8        # Increased from 2
  memory: "16Gi" # Increased from 4Gi
</code></pre>
<h2 id="snapshot-management-1"><a class="header" href="#snapshot-management-1">Snapshot Management</a></h2>
<h3 id="snapshot-features"><a class="header" href="#snapshot-features">Snapshot Features</a></h3>
<ul>
<li><strong>Memory Snapshots</strong>: Include VM memory state for consistent restore</li>
<li><strong>Crash-Consistent</strong>: Without memory for faster snapshots</li>
<li><strong>Snapshot Trees</strong>: Nested snapshots with parent-child relationships</li>
<li><strong>Metadata</strong>: Description and timestamp tracking</li>
</ul>
<h3 id="snapshot-operations-1"><a class="header" href="#snapshot-operations-1">Snapshot Operations</a></h3>
<pre><code class="language-yaml"># Create snapshot with memory
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSnapshot
metadata:
  name: before-upgrade
spec:
  vmRef:
    name: web-server
  description: "Pre-maintenance snapshot"
  includeMemory: true  # Include running memory state
</code></pre>
<pre><code class="language-bash"># Create snapshot via kubectl
kubectl create vmsnapshot before-upgrade \
  --vm=web-server \
  --description="Before major upgrade" \
  --include-memory=true
</code></pre>
<h2 id="multi-nic-networking"><a class="header" href="#multi-nic-networking">Multi-NIC Networking</a></h2>
<h3 id="network-configuration-2"><a class="header" href="#network-configuration-2">Network Configuration</a></h3>
<p>The provider supports multiple network interfaces with:</p>
<ul>
<li><strong>Bridge Assignment</strong>: Map to Proxmox bridges (vmbr0, vmbr1, etc.)</li>
<li><strong>VLAN Tagging</strong>: 802.1Q VLAN support</li>
<li><strong>Static IPs</strong>: Cloud-init integration for network configuration</li>
<li><strong>MAC Addresses</strong>: Custom MAC assignment</li>
</ul>
<h3 id="example-multi-nic-vm"><a class="header" href="#example-multi-nic-vm">Example Multi-NIC VM</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: multi-nic-vm
spec:
  providerRef:
    name: proxmox-prod
  classRef:
    name: medium
  imageRef:
    name: ubuntu-22
  networks:
    # Primary LAN interface
    - name: lan
      bridge: vmbr0
      staticIP:
        address: "192.168.1.100/24"
        gateway: "192.168.1.1"
        dns: ["8.8.8.8", "1.1.1.1"]
    
    # DMZ interface with VLAN
    - name: dmz
      bridge: vmbr1
      vlan: 100
      staticIP:
        address: "10.0.100.50/24"
    
    # Management interface
    - name: mgmt
      bridge: vmbr2
      mac: "02:00:00:aa:bb:cc"
</code></pre>
<h3 id="network-bridge-mapping-1"><a class="header" href="#network-bridge-mapping-1">Network Bridge Mapping</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Network Name</th><th>Default Bridge</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>lan</code>, <code>default</code></td><td>vmbr0</td><td>General LAN connectivity</td></tr>
<tr><td><code>dmz</code></td><td>vmbr1</td><td>DMZ/public services</td></tr>
<tr><td><code>mgmt</code>, <code>management</code></td><td>vmbr2</td><td>Management network</td></tr>
<tr><td><code>vmbr*</code></td><td>Same name</td><td>Direct bridge reference</td></tr>
</tbody></table>
</div>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<h3 id="required-environment-variables-1"><a class="header" href="#required-environment-variables-1">Required Environment Variables</a></h3>
<p><strong>‚ö†Ô∏è The provider requires environment variables to connect to Proxmox VE:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Required</th><th>Default</th><th>Example</th></tr></thead><tbody>
<tr><td><code>PVE_ENDPOINT</code></td><td>Proxmox API endpoint URL</td><td><strong>Yes</strong></td><td>-</td><td><code>https://pve.example.com:8006/api2</code></td></tr>
<tr><td><code>PVE_TOKEN_ID</code></td><td>API token identifier</td><td>Yes*</td><td>-</td><td><code>virtrigaud@pve!vrtg-token</code></td></tr>
<tr><td><code>PVE_TOKEN_SECRET</code></td><td>API token secret</td><td>Yes*</td><td>-</td><td><code>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code></td></tr>
<tr><td><code>PVE_USERNAME</code></td><td>Username for session auth</td><td>Yes*</td><td>-</td><td><code>virtrigaud@pve</code></td></tr>
<tr><td><code>PVE_PASSWORD</code></td><td>Password for session auth</td><td>Yes*</td><td>-</td><td><code>secure-password</code></td></tr>
<tr><td><code>PVE_NODE_SELECTOR</code></td><td>Preferred nodes (comma-separated)</td><td>No</td><td>Auto-detect</td><td><code>pve-node-1,pve-node-2</code></td></tr>
<tr><td><code>PVE_INSECURE_SKIP_VERIFY</code></td><td>Skip TLS verification</td><td>No</td><td><code>false</code></td><td><code>true</code></td></tr>
<tr><td><code>PVE_CA_BUNDLE</code></td><td>Custom CA certificate</td><td>No</td><td>-</td><td><code>-----BEGIN CERTIFICATE-----...</code></td></tr>
</tbody></table>
</div>
<p>* Either token (<code>PVE_TOKEN_ID</code> + <code>PVE_TOKEN_SECRET</code>) or username/password (<code>PVE_USERNAME</code> + <code>PVE_PASSWORD</code>) is required</p>
<h3 id="deployment-configuration-3"><a class="header" href="#deployment-configuration-3">Deployment Configuration</a></h3>
<p>The provider needs environment variables to connect to Proxmox. Here are complete deployment examples:</p>
<h4 id="using-helm-values-2"><a class="header" href="#using-helm-values-2">Using Helm Values</a></h4>
<pre><code class="language-yaml"># values.yaml
providers:
  proxmox:
    enabled: true
    env:
      - name: PVE_ENDPOINT
        value: "https://pve.example.com:8006/api2"
      - name: PVE_TOKEN_ID
        value: "virtrigaud@pve!vrtg-token"
      - name: PVE_TOKEN_SECRET
        value: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      - name: PVE_INSECURE_SKIP_VERIFY
        value: "true"  # Only for development!
      - name: PVE_NODE_SELECTOR
        value: "pve-node-1,pve-node-2"  # Optional
</code></pre>
<h4 id="using-kubernetes-secrets-recommended"><a class="header" href="#using-kubernetes-secrets-recommended">Using Kubernetes Secrets (Recommended)</a></h4>
<pre><code class="language-yaml"># Create secret with credentials
apiVersion: v1
kind: Secret
metadata:
  name: proxmox-credentials
  namespace: virtrigaud-system
type: Opaque
stringData:
  PVE_ENDPOINT: "https://pve.example.com:8006/api2"
  PVE_TOKEN_ID: "virtrigaud@pve!vrtg-token"
  PVE_TOKEN_SECRET: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
  PVE_INSECURE_SKIP_VERIFY: "false"

---
# Reference secret in deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: virtrigaud-provider-proxmox
spec:
  template:
    spec:
      containers:
      - name: provider-proxmox
        image: ghcr.io/projectbeskar/virtrigaud/provider-proxmox:v0.2.3
        envFrom:
        - secretRef:
            name: proxmox-credentials
</code></pre>
<h4 id="developmenttesting-configuration"><a class="header" href="#developmenttesting-configuration">Development/Testing Configuration</a></h4>
<pre><code class="language-yaml"># For development with a local Proxmox VE instance
providers:
  proxmox:
    enabled: true
    env:
      - name: PVE_ENDPOINT
        value: "https://192.168.1.100:8006/api2"
      - name: PVE_USERNAME
        value: "root@pam"
      - name: PVE_PASSWORD
        value: "your-password"
      - name: PVE_INSECURE_SKIP_VERIFY
        value: "true"
</code></pre>
<h3 id="node-selection"><a class="header" href="#node-selection">Node Selection</a></h3>
<p>The provider can be configured to prefer specific nodes:</p>
<pre><code class="language-yaml">env:
  - name: PVE_NODE_SELECTOR
    value: "pve-node-1,pve-node-2"
</code></pre>
<p>If not specified, the provider will automatically select nodes based on availability.</p>
<h2 id="vm-configuration-2"><a class="header" href="#vm-configuration-2">VM Configuration</a></h2>
<h3 id="vmclass-specification-2"><a class="header" href="#vmclass-specification-2">VMClass Specification</a></h3>
<p>Define CPU and memory resources:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: small
spec:
  cpus: 2
  memory: "4Gi"
  # Proxmox-specific settings
  spec:
    machine: "q35"
    bios: "uefi"
</code></pre>
<h3 id="vmimage-specification-2"><a class="header" href="#vmimage-specification-2">VMImage Specification</a></h3>
<p>Reference Proxmox templates:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-22
spec:
  source: "ubuntu-22-template"  # Template name in Proxmox
  # Or clone from existing VM:
  # source: "9000"  # VMID to clone from
</code></pre>
<h3 id="virtualmachine-example"><a class="header" href="#virtualmachine-example">VirtualMachine Example</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  providerRef:
    name: proxmox-prod
  classRef:
    name: small
  imageRef:
    name: ubuntu-22
  powerState: On
  networks:
    - name: lan
      # Maps to Proxmox bridge or VLAN configuration
  disks:
    - name: root
      size: "40Gi"
  userData:
    cloudInit:
      inline: |
        #cloud-config
        hostname: web-server
        users:
          - name: ubuntu
            ssh_authorized_keys:
              - "ssh-ed25519 AAAA..."
        packages:
          - nginx
</code></pre>
<h2 id="cloud-init-integration-1"><a class="header" href="#cloud-init-integration-1">Cloud-Init Integration</a></h2>
<p>The provider automatically configures cloud-init for supported VMs:</p>
<h3 id="automatic-configuration-1"><a class="header" href="#automatic-configuration-1">Automatic Configuration</a></h3>
<ul>
<li><strong>IDE2 Device</strong>: Attached as cloudinit drive</li>
<li><strong>User Data</strong>: Rendered from VirtualMachine spec</li>
<li><strong>Network Config</strong>: Generated from network specifications</li>
<li><strong>SSH Keys</strong>: Extracted from userData or secrets</li>
</ul>
<h3 id="static-ip-configuration"><a class="header" href="#static-ip-configuration">Static IP Configuration</a></h3>
<p>Configure static IPs using cloud-init:</p>
<pre><code class="language-yaml">userData:
  cloudInit:
    inline: |
      #cloud-config
      write_files:
        - path: /etc/netplan/01-static.yaml
          content: |
            network:
              version: 2
              ethernets:
                ens18:
                  addresses: [192.168.1.100/24]
                  gateway4: 192.168.1.1
                  nameservers:
                    addresses: [8.8.8.8, 1.1.1.1]
</code></pre>
<p>Or use Proxmox IP configuration:</p>
<pre><code class="language-yaml"># This would be handled by the provider internally
# when processing network specifications
</code></pre>
<h2 id="guest-agent-integration-v023"><a class="header" href="#guest-agent-integration-v023">Guest Agent Integration (v0.2.3+)</a></h2>
<p>The Proxmox provider now integrates with the QEMU Guest Agent for enhanced VM monitoring:</p>
<h3 id="ip-address-detection"><a class="header" href="#ip-address-detection">IP Address Detection</a></h3>
<p>When a VM is running, the provider automatically queries the QEMU guest agent to retrieve accurate IP addresses:</p>
<pre><code class="language-yaml"># IP addresses are automatically populated in VM status
kubectl get vm my-vm -o yaml

status:
  phase: Running
  ipAddresses:
    - 192.168.1.100
    - fd00::1234:5678:9abc:def0
</code></pre>
<h3 id="features-1"><a class="header" href="#features-1">Features</a></h3>
<ul>
<li><strong>Automatic IP Detection</strong>: Retrieves all network interface IPs from running VMs</li>
<li><strong>IPv4 and IPv6 Support</strong>: Reports both address families</li>
<li><strong>Smart Filtering</strong>: Excludes loopback (127.0.0.1, ::1) and link-local (169.254.x.x, fe80::) addresses</li>
<li><strong>Real-time Updates</strong>: Information updated during Describe operations</li>
<li><strong>Graceful Degradation</strong>: Falls back gracefully when guest agent is not available</li>
</ul>
<h3 id="requirements-3"><a class="header" href="#requirements-3">Requirements</a></h3>
<p>For guest agent integration to work, the VM must have:</p>
<ol>
<li>
<p><strong>QEMU Guest Agent Installed</strong>:</p>
<pre><code class="language-bash"># Ubuntu/Debian
apt-get install qemu-guest-agent

# CentOS/RHEL
yum install qemu-guest-agent

# Enable and start the service
systemctl enable --now qemu-guest-agent
</code></pre>
</li>
<li>
<p><strong>VM Configuration</strong>: Guest agent is automatically enabled during VM creation</p>
</li>
</ol>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<p>The provider:</p>
<ol>
<li>Checks if VM is in running state</li>
<li>Makes API call to <code>/api2/json/nodes/{node}/qemu/{vmid}/agent/network-get-interfaces</code></li>
<li>Parses network interface details from guest agent response</li>
<li>Filters out irrelevant addresses (loopback, link-local)</li>
<li>Populates <code>status.ipAddresses</code> field</li>
</ol>
<h3 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h3>
<p>If IP addresses are not appearing:</p>
<ul>
<li>Verify guest agent is installed: <code>systemctl status qemu-guest-agent</code></li>
<li>Check Proxmox VM options: <code>qm config &lt;vmid&gt; | grep agent</code></li>
<li>Ensure VM has network connectivity</li>
<li>Check provider logs for guest agent errors</li>
</ul>
<h2 id="cloning-behavior"><a class="header" href="#cloning-behavior">Cloning Behavior</a></h2>
<h3 id="linked-clones-default"><a class="header" href="#linked-clones-default">Linked Clones (Default)</a></h3>
<p>Efficient space usage, faster creation:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClone
metadata:
  name: web-clone
spec:
  sourceVMRef:
    name: template-vm
  linkedClone: true  # Default
</code></pre>
<h3 id="full-clones"><a class="header" href="#full-clones">Full Clones</a></h3>
<p>Independent copies, slower creation:</p>
<pre><code class="language-yaml">spec:
  linkedClone: false
</code></pre>
<h2 id="snapshots"><a class="header" href="#snapshots">Snapshots</a></h2>
<p>Create and manage VM snapshots:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSnapshot
metadata:
  name: before-upgrade
spec:
  vmRef:
    name: web-server
  description: "Snapshot before system upgrade"
</code></pre>
<h2 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h2>
<h3 id="common-issues-3"><a class="header" href="#common-issues-3">Common Issues</a></h3>
<h4 id="authentication-failures"><a class="header" href="#authentication-failures">Authentication Failures</a></h4>
<pre><code>Error: failed to connect to Proxmox VE: authentication failed
</code></pre>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify API token permissions</li>
<li>Check token expiration</li>
<li>Ensure user has VM.* privileges</li>
</ul>
<h4 id="tls-certificate-errors"><a class="header" href="#tls-certificate-errors">TLS Certificate Errors</a></h4>
<pre><code>Error: x509: certificate signed by unknown authority
</code></pre>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Add custom CA certificate to credentials secret</li>
<li>Use <code>PVE_INSECURE_SKIP_VERIFY=true</code> for testing</li>
<li>Verify certificate chain</li>
</ul>
<h4 id="vm-creation-failures"><a class="header" href="#vm-creation-failures">VM Creation Failures</a></h4>
<pre><code>Error: create VM failed with status 400: storage 'local-lvm' does not exist
</code></pre>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify storage configuration in Proxmox</li>
<li>Check node availability</li>
<li>Ensure sufficient resources</li>
</ul>
<h3 id="debug-logging-2"><a class="header" href="#debug-logging-2">Debug Logging</a></h3>
<p>Enable debug logging for troubleshooting:</p>
<pre><code class="language-yaml">env:
  - name: LOG_LEVEL
    value: "debug"
</code></pre>
<h3 id="health-checks"><a class="header" href="#health-checks">Health Checks</a></h3>
<p>Monitor provider health:</p>
<pre><code class="language-bash"># Check provider pod logs
kubectl logs -n virtrigaud-system deployment/provider-proxmox

# Test connectivity
kubectl exec -n virtrigaud-system deployment/provider-proxmox -- \
  curl -k https://pve.example.com:8006/api2/json/version
</code></pre>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<h3 id="resource-allocation-1"><a class="header" href="#resource-allocation-1">Resource Allocation</a></h3>
<p>For production environments:</p>
<pre><code class="language-yaml">resources:
  requests:
    cpu: 100m
    memory: 256Mi
  limits:
    cpu: 500m
    memory: 512Mi
</code></pre>
<h3 id="concurrent-operations"><a class="header" href="#concurrent-operations">Concurrent Operations</a></h3>
<p>The provider handles concurrent VM operations efficiently but consider:</p>
<ul>
<li>Node capacity limits</li>
<li>Storage I/O constraints</li>
<li>Network bandwidth</li>
</ul>
<h3 id="task-polling"><a class="header" href="#task-polling">Task Polling</a></h3>
<p>Task completion is polled every 2 seconds with a 5-minute timeout. These can be tuned via environment variables if needed.</p>
<h2 id="minimal-proxmox-ve-permissions"><a class="header" href="#minimal-proxmox-ve-permissions">Minimal Proxmox VE Permissions</a></h2>
<h3 id="required-api-token-permissions"><a class="header" href="#required-api-token-permissions">Required API Token Permissions</a></h3>
<p>Create an API token with these minimal privileges:</p>
<pre><code class="language-bash"># Create user for VirtRigaud
pveum user add virtrigaud@pve --comment "VirtRigaud Provider"

# Create API token
pveum user token add virtrigaud@pve vrtg-token --privsep 1

# Grant minimal required permissions
pveum acl modify / --users virtrigaud@pve --roles PVEVMAdmin,PVEDatastoreUser

# Custom role with minimal permissions (alternative)
pveum role add VirtRigaud --privs "VM.Allocate,VM.Audit,VM.Config.CPU,VM.Config.Memory,VM.Config.Disk,VM.Config.Network,VM.Config.Options,VM.Monitor,VM.PowerMgmt,VM.Snapshot,VM.Clone,Datastore.Allocate,Datastore.AllocateSpace,Pool.Allocate"
pveum acl modify / --users virtrigaud@pve --roles VirtRigaud
</code></pre>
<h3 id="permission-details"><a class="header" href="#permission-details">Permission Details</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Permission</th><th>Usage</th><th>Required</th></tr></thead><tbody>
<tr><td><code>VM.Allocate</code></td><td>Create new VMs</td><td>‚úÖ Core</td></tr>
<tr><td><code>VM.Audit</code></td><td>Read VM configuration</td><td>‚úÖ Core</td></tr>
<tr><td><code>VM.Config.*</code></td><td>Modify VM settings</td><td>‚úÖ Reconfigure</td></tr>
<tr><td><code>VM.Monitor</code></td><td>VM status monitoring</td><td>‚úÖ Core</td></tr>
<tr><td><code>VM.PowerMgmt</code></td><td>Power operations</td><td>‚úÖ Core</td></tr>
<tr><td><code>VM.Snapshot</code></td><td>Snapshot operations</td><td>‚ö†Ô∏è Optional</td></tr>
<tr><td><code>VM.Clone</code></td><td>VM cloning</td><td>‚ö†Ô∏è Optional</td></tr>
<tr><td><code>Datastore.Allocate</code></td><td>Create VM disks</td><td>‚úÖ Core</td></tr>
<tr><td><code>Pool.Allocate</code></td><td>Resource pool usage</td><td>‚ö†Ô∏è Optional</td></tr>
</tbody></table>
</div>
<h3 id="token-rotation-procedure"><a class="header" href="#token-rotation-procedure">Token Rotation Procedure</a></h3>
<pre><code class="language-bash"># 1. Create new token
NEW_TOKEN=$(pveum user token add virtrigaud@pve vrtg-token-2 --privsep 1 --output-format json | jq -r '.value')

# 2. Update Kubernetes secret
kubectl patch secret pve-credentials -n virtrigaud-system --type='merge' -p='{"stringData":{"token_id":"virtrigaud@pve!vrtg-token-2","token_secret":"'$NEW_TOKEN'"}}'

# 3. Restart provider to use new token
kubectl rollout restart deployment provider-proxmox -n virtrigaud-system

# 4. Verify new token works
kubectl logs deployment/provider-proxmox -n virtrigaud-system

# 5. Remove old token
pveum user token remove virtrigaud@pve vrtg-token
</code></pre>
<h2 id="networkpolicy-examples"><a class="header" href="#networkpolicy-examples">NetworkPolicy Examples</a></h2>
<h3 id="production-networkpolicy"><a class="header" href="#production-networkpolicy">Production NetworkPolicy</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: provider-proxmox-netpol
  namespace: virtrigaud-system
spec:
  podSelector:
    matchLabels:
      app: provider-proxmox
  policyTypes: [Ingress, Egress]
  
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: virtrigaud-manager
    ports: [9443, 8080]
  
  egress:
  # DNS resolution
  - to: []
    ports: [53]
  
  # Proxmox VE API
  - to:
    - ipBlock:
        cidr: 192.168.1.0/24  # Your PVE network
    ports: [8006]
</code></pre>
<h3 id="development-networkpolicy"><a class="header" href="#development-networkpolicy">Development NetworkPolicy</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: provider-proxmox-dev-netpol
  namespace: virtrigaud-system
spec:
  podSelector:
    matchLabels:
      app: provider-proxmox
      environment: development
  egress:
  - to: []  # Allow all egress for development
</code></pre>
<h2 id="storage-and-placement"><a class="header" href="#storage-and-placement">Storage and Placement</a></h2>
<h3 id="storage-class-mapping"><a class="header" href="#storage-class-mapping">Storage Class Mapping</a></h3>
<p>Configure storage placement for different workloads:</p>
<pre><code class="language-yaml"># High-performance storage
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: high-performance
spec:
  cpus: 8
  memory: "32Gi"
  storage:
    class: "nvme-storage"  # Maps to PVE storage
    type: "thin"           # Thin provisioning
    
# Standard storage
apiVersion: infra.virtrigaud.io/v1beta1  
kind: VMClass
metadata:
  name: standard
spec:
  cpus: 4
  memory: "8Gi"
  storage:
    class: "ssd-storage"
    type: "thick"          # Thick provisioning
</code></pre>
<h3 id="placement-policies"><a class="header" href="#placement-policies">Placement Policies</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMPlacementPolicy
metadata:
  name: production-placement
spec:
  nodeSelector:
    - "pve-node-1"
    - "pve-node-2"
  antiAffinity:
    - key: "vm.type"
      operator: "In"
      values: ["database"]
  constraints:
    maxVMsPerNode: 10
    minFreeMemory: "4Gi"
</code></pre>
<h2 id="performance-testing"><a class="header" href="#performance-testing">Performance Testing</a></h2>
<h3 id="load-test-results"><a class="header" href="#load-test-results">Load Test Results</a></h3>
<p>Performance benchmarks using virtrigaud-loadgen against fake PVE server:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>P50 Latency</th><th>P95 Latency</th><th>Throughput</th><th>Notes</th></tr></thead><tbody>
<tr><td>Create VM</td><td>2.3s</td><td>4.1s</td><td>12 ops/min</td><td>Including cloud-init</td></tr>
<tr><td>Power On</td><td>800ms</td><td>1.2s</td><td>45 ops/min</td><td>Async operation</td></tr>
<tr><td>Power Off</td><td>650ms</td><td>1.1s</td><td>50 ops/min</td><td>Graceful shutdown</td></tr>
<tr><td>Describe</td><td>120ms</td><td>200ms</td><td>200 ops/min</td><td>Status query</td></tr>
<tr><td>Reconfigure CPU</td><td>1.8s</td><td>3.2s</td><td>15 ops/min</td><td>Online hot-plug</td></tr>
<tr><td>Snapshot Create</td><td>3.5s</td><td>6.8s</td><td>8 ops/min</td><td>With memory</td></tr>
<tr><td>Clone (Linked)</td><td>1.9s</td><td>3.4s</td><td>12 ops/min</td><td>Fast COW clone</td></tr>
</tbody></table>
</div>
<h3 id="running-performance-tests"><a class="header" href="#running-performance-tests">Running Performance Tests</a></h3>
<pre><code class="language-bash"># Deploy fake PVE server for testing
kubectl apply -f test/performance/proxmox-loadtest.yaml

# Run performance test
kubectl create job proxmox-perf-test --from=cronjob/proxmox-performance-test

# View results
kubectl logs job/proxmox-perf-test -f
</code></pre>
<h2 id="security-best-practices"><a class="header" href="#security-best-practices">Security Best Practices</a></h2>
<ol>
<li><strong>Use API Tokens</strong>: Prefer API tokens over username/password</li>
<li><strong>Least Privilege</strong>: Grant minimal required permissions (see above)</li>
<li><strong>TLS Verification</strong>: Always verify certificates in production</li>
<li><strong>Secret Management</strong>: Use Kubernetes secrets with proper RBAC</li>
<li><strong>Network Policies</strong>: Restrict provider network access (see examples)</li>
<li><strong>Regular Rotation</strong>: Rotate API tokens quarterly</li>
<li><strong>Audit Logging</strong>: Enable PVE audit logs for provider actions</li>
<li><strong>Resource Quotas</strong>: Limit provider resource consumption</li>
</ol>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<h3 id="multi-node-setup"><a class="header" href="#multi-node-setup">Multi-Node Setup</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: proxmox-cluster
spec:
  type: proxmox
  endpoint: https://pve-cluster.example.com:8006
  runtime:
    env:
      - name: PVE_NODE_SELECTOR
        value: "pve-1,pve-2,pve-3"
</code></pre>
<h3 id="high-availability-configuration"><a class="header" href="#high-availability-configuration">High-Availability Configuration</a></h3>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: provider-proxmox
spec:
  replicas: 2
  template:
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: provider-proxmox
              topologyKey: kubernetes.io/hostname
</code></pre>
<h2 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h2>
<h3 id="common-issues-4"><a class="header" href="#common-issues-4">Common Issues</a></h3>
<h4 id="-endpoint-is-required-error"><a class="header" href="#-endpoint-is-required-error">‚ùå ‚Äúendpoint is required‚Äù Error</a></h4>
<p><strong>Symptom</strong>: Provider pod crashes with <code>ERROR Failed to create PVE client error="endpoint is required"</code></p>
<p><strong>Cause</strong>: Missing or empty <code>PVE_ENDPOINT</code> environment variable</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-yaml"># Ensure PVE_ENDPOINT is set in deployment
env:
  - name: PVE_ENDPOINT
    value: "https://your-proxmox.example.com:8006/api2"
</code></pre>
<h4 id="-connection-timeoutrefused"><a class="header" href="#-connection-timeoutrefused">‚ùå Connection Timeout/Refused</a></h4>
<p><strong>Symptom</strong>: Provider fails with connection timeouts or ‚Äúconnection refused‚Äù</p>
<p><strong>Cause</strong>: Network connectivity issues or wrong endpoint URL</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p><strong>Verify endpoint</strong>: Test from a pod in the cluster:</p>
<pre><code class="language-bash">kubectl run test-curl --rm -i --tty --image=curlimages/curl -- \
  curl -k https://your-proxmox.example.com:8006/api2/json/version
</code></pre>
</li>
<li>
<p><strong>Check firewall</strong>: Ensure port 8006 is accessible from Kubernetes cluster</p>
</li>
<li>
<p><strong>Verify URL format</strong>: Should be <code>https://hostname:8006/api2</code> (note the <code>/api2</code> path)</p>
</li>
</ol>
<h4 id="-tls-certificate-errors"><a class="header" href="#-tls-certificate-errors">‚ùå TLS Certificate Errors</a></h4>
<p><strong>Symptom</strong>: <code>x509: certificate signed by unknown authority</code></p>
<p><strong>Solutions</strong>:</p>
<ul>
<li><strong>Development</strong>: Set <code>PVE_INSECURE_SKIP_VERIFY=true</code> (not for production!)</li>
<li><strong>Production</strong>: Provide valid TLS certificates or CA bundle</li>
</ul>
<h4 id="-authentication-failures"><a class="header" href="#-authentication-failures">‚ùå Authentication Failures</a></h4>
<p><strong>Symptom</strong>: <code>401 Unauthorized</code> or <code>authentication failure</code></p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>
<p><strong>Verify token permissions</strong>:</p>
<pre><code class="language-bash"># Test API token manually
curl -k "https://pve.example.com:8006/api2/json/version" \
  -H "Authorization: PVEAPIToken=USER@REALM!TOKENID=SECRET"
</code></pre>
</li>
<li>
<p><strong>Check user privileges</strong>: Ensure user has VM management permissions</p>
</li>
<li>
<p><strong>Verify token format</strong>: Should be <code>user@realm!tokenid</code> (note the <code>!</code>)</p>
</li>
</ol>
<h4 id="-provider-not-starting"><a class="header" href="#-provider-not-starting">‚ùå Provider Not Starting</a></h4>
<p><strong>Symptom</strong>: Pod in <code>CrashLoopBackOff</code> or <code>0/1 Ready</code></p>
<p><strong>Diagnostic Steps</strong>:</p>
<pre><code class="language-bash"># Check pod logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-provider-proxmox

# Check environment variables
kubectl describe pod -n virtrigaud-system -l app.kubernetes.io/component=provider-proxmox

# Verify configuration
kubectl get secret proxmox-credentials -o yaml
</code></pre>
<h3 id="validation-commands-2"><a class="header" href="#validation-commands-2">Validation Commands</a></h3>
<p>Test your Proxmox connection before deploying:</p>
<pre><code class="language-bash"># 1. Test network connectivity
telnet your-proxmox.example.com 8006

# 2. Test API endpoint
curl -k https://your-proxmox.example.com:8006/api2/json/version

# 3. Test authentication
curl -k "https://your-proxmox.example.com:8006/api2/json/nodes" \
  -H "Authorization: PVEAPIToken=USER@REALM!TOKENID=SECRET"

# 4. Test from within cluster
kubectl run debug --rm -i --tty --image=curlimages/curl -- sh
# Then run curl commands from inside the pod
</code></pre>
<h3 id="debug-logging-3"><a class="header" href="#debug-logging-3">Debug Logging</a></h3>
<p>Enable verbose logging for the provider:</p>
<pre><code class="language-yaml">providers:
  proxmox:
    env:
      - name: LOG_LEVEL
        value: "debug"
      - name: PVE_ENDPOINT
        value: "https://pve.example.com:8006/api2"
</code></pre>
<h2 id="api-reference-3"><a class="header" href="#api-reference-3">API Reference</a></h2>
<p>For complete API reference, see the <a href="providers/../api-reference/">Provider API Documentation</a>.</p>
<h2 id="contributing-2"><a class="header" href="#contributing-2">Contributing</a></h2>
<p>To contribute to the Proxmox provider:</p>
<ol>
<li>See the <a href="providers/../tutorial.html">Provider Development Guide</a></li>
<li>Check the <a href="https://github.com/projectbeskar/virtrigaud">GitHub repository</a></li>
<li>Review <a href="https://github.com/projectbeskar/virtrigaud/labels/provider%2Fproxmox">open issues</a></li>
</ol>
<h2 id="support-3"><a class="header" href="#support-3">Support</a></h2>
<ul>
<li><strong>Documentation</strong>: <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud Docs</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub Issues</a></li>
<li><strong>Community</strong>: <a href="https://discord.gg/projectbeskar">Discord</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provider-developer-tutorial"><a class="header" href="#provider-developer-tutorial">Provider Developer Tutorial</a></h1>
<p>This comprehensive tutorial walks you through creating a complete VirtRigaud provider from scratch. By the end, you‚Äôll have a fully functional provider that can create, manage, and delete virtual machines.</p>
<h2 id="prerequisites-5"><a class="header" href="#prerequisites-5">Prerequisites</a></h2>
<p>Before starting this tutorial, ensure you have:</p>
<ul>
<li>Go 1.23 or later installed</li>
<li>Docker installed for containerization</li>
<li>kubectl and a Kubernetes cluster (Kind/minikube for local development)</li>
<li>Helm 3.x installed</li>
<li>Basic understanding of gRPC and protobuf</li>
</ul>
<h2 id="tutorial-overview"><a class="header" href="#tutorial-overview">Tutorial Overview</a></h2>
<p>We‚Äôll build a <strong>File Provider</strong> that manages ‚Äúvirtual machines‚Äù as JSON files on disk. While not practical for production, this provider demonstrates all the core concepts without requiring actual hypervisor access.</p>
<p><strong>What we‚Äôll build:</strong></p>
<ul>
<li>A complete provider implementation using the VirtRigaud SDK</li>
<li>Conformance tests that pass VCTS core profile</li>
<li>A Helm chart for deployment</li>
<li>CI/CD integration</li>
<li>Publication to the provider catalog</li>
</ul>
<h2 id="step-1-initialize-your-provider-project"><a class="header" href="#step-1-initialize-your-provider-project">Step 1: Initialize Your Provider Project</a></h2>
<h3 id="11-create-project-structure"><a class="header" href="#11-create-project-structure">1.1 Create Project Structure</a></h3>
<pre><code class="language-bash"># Create project directory
mkdir virtrigaud-provider-file
cd virtrigaud-provider-file

# Initialize the provider project
vrtg-provider init file
</code></pre>
<p>The <code>vrtg-provider init</code> command creates the following structure:</p>
<pre><code>virtrigaud-provider-file/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ provider-file/
‚îÇ       ‚îú‚îÄ‚îÄ main.go
‚îÇ       ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îî‚îÄ‚îÄ provider/
‚îÇ       ‚îú‚îÄ‚îÄ provider.go
‚îÇ       ‚îú‚îÄ‚îÄ capabilities.go
‚îÇ       ‚îî‚îÄ‚îÄ provider_test.go
‚îú‚îÄ‚îÄ charts/
‚îÇ   ‚îî‚îÄ‚îÄ provider-file/
‚îÇ       ‚îú‚îÄ‚îÄ Chart.yaml
‚îÇ       ‚îú‚îÄ‚îÄ values.yaml
‚îÇ       ‚îî‚îÄ‚îÄ templates/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
</code></pre>
<h3 id="12-examine-generated-files"><a class="header" href="#12-examine-generated-files">1.2 Examine Generated Files</a></h3>
<p><strong>main.go</strong> - Entry point that sets up the gRPC server:</p>
<pre><code class="language-go">package main

import (
    "log"
    
    "github.com/projectbeskar/virtrigaud/sdk/provider/server"
    "github.com/projectbeskar/virtrigaud/proto/rpc/provider/v1"
    "virtrigaud-provider-file/internal/provider"
)

func main() {
    // Create provider instance
    p, err := provider.New()
    if err != nil {
        log.Fatalf("Failed to create provider: %v", err)
    }
    
    // Configure server
    config := &amp;server.Config{
        Port:        9443,
        HealthPort:  8080,
        EnableTLS:   false,
    }
    
    srv, err := server.New(config)
    if err != nil {
        log.Fatalf("Failed to create server: %v", err)
    }
    
    // Register provider service
    providerv1.RegisterProviderServiceServer(srv.GRPCServer(), p)
    
    // Start server
    log.Println("Starting file provider on port 9443...")
    if err := srv.Serve(); err != nil {
        log.Fatalf("Server failed: %v", err)
    }
}
</code></pre>
<p><strong>go.mod</strong> - Module definition with SDK dependency:</p>
<pre><code class="language-go">module virtrigaud-provider-file

go 1.23

require (
    github.com/projectbeskar/virtrigaud/sdk v0.1.0
    github.com/projectbeskar/virtrigaud/proto v0.1.0
)
</code></pre>
<h2 id="step-2-implement-the-core-provider"><a class="header" href="#step-2-implement-the-core-provider">Step 2: Implement the Core Provider</a></h2>
<h3 id="21-design-the-file-provider"><a class="header" href="#21-design-the-file-provider">2.1 Design the File Provider</a></h3>
<p>Our file provider will:</p>
<ul>
<li>Store VM metadata as JSON files in <code>/var/lib/virtrigaud/vms/</code></li>
<li>Use filename as VM ID</li>
<li>Simulate power operations with state files</li>
<li>Support basic CRUD operations</li>
</ul>
<h3 id="22-define-the-vm-model"><a class="header" href="#22-define-the-vm-model">2.2 Define the VM Model</a></h3>
<p>Create <code>internal/provider/vm.go</code>:</p>
<pre><code class="language-go">package provider

import (
    "encoding/json"
    "fmt"
    "io/ioutil"
    "os"
    "path/filepath"
    "time"
    
    "github.com/projectbeskar/virtrigaud/proto/rpc/provider/v1"
)

type VirtualMachine struct {
    ID          string                 `json:"id"`
    Name        string                 `json:"name"`
    Spec        *providerv1.VMSpec     `json:"spec"`
    Status      *providerv1.VMStatus   `json:"status"`
    CreatedAt   time.Time              `json:"created_at"`
    UpdatedAt   time.Time              `json:"updated_at"`
}

type FileStore struct {
    baseDir string
}

func NewFileStore(baseDir string) *FileStore {
    return &amp;FileStore{baseDir: baseDir}
}

func (fs *FileStore) Save(vm *VirtualMachine) error {
    if err := os.MkdirAll(fs.baseDir, 0755); err != nil {
        return fmt.Errorf("failed to create directory: %w", err)
    }
    
    vm.UpdatedAt = time.Now()
    data, err := json.MarshalIndent(vm, "", "  ")
    if err != nil {
        return fmt.Errorf("failed to marshal VM: %w", err)
    }
    
    filename := filepath.Join(fs.baseDir, vm.ID+".json")
    return ioutil.WriteFile(filename, data, 0644)
}

func (fs *FileStore) Load(id string) (*VirtualMachine, error) {
    filename := filepath.Join(fs.baseDir, id+".json")
    data, err := ioutil.ReadFile(filename)
    if err != nil {
        if os.IsNotExist(err) {
            return nil, fmt.Errorf("VM not found: %s", id)
        }
        return nil, fmt.Errorf("failed to read VM file: %w", err)
    }
    
    var vm VirtualMachine
    if err := json.Unmarshal(data, &amp;vm); err != nil {
        return nil, fmt.Errorf("failed to unmarshal VM: %w", err)
    }
    
    return &amp;vm, nil
}

func (fs *FileStore) Delete(id string) error {
    filename := filepath.Join(fs.baseDir, id+".json")
    if err := os.Remove(filename); err != nil &amp;&amp; !os.IsNotExist(err) {
        return fmt.Errorf("failed to delete VM file: %w", err)
    }
    return nil
}

func (fs *FileStore) List() ([]*VirtualMachine, error) {
    files, err := ioutil.ReadDir(fs.baseDir)
    if err != nil {
        if os.IsNotExist(err) {
            return []*VirtualMachine{}, nil
        }
        return nil, fmt.Errorf("failed to read directory: %w", err)
    }
    
    var vms []*VirtualMachine
    for _, file := range files {
        if !file.IsDir() &amp;&amp; filepath.Ext(file.Name()) == ".json" {
            id := file.Name()[:len(file.Name())-5] // Remove .json extension
            vm, err := fs.Load(id)
            if err != nil {
                continue // Skip invalid files
            }
            vms = append(vms, vm)
        }
    }
    
    return vms, nil
}
</code></pre>
<h3 id="23-implement-the-provider-interface"><a class="header" href="#23-implement-the-provider-interface">2.3 Implement the Provider Interface</a></h3>
<p>Update <code>internal/provider/provider.go</code>:</p>
<pre><code class="language-go">package provider

import (
    "context"
    "fmt"
    "os"
    "path/filepath"
    "time"
    
    "github.com/google/uuid"
    "google.golang.org/grpc/codes"
    "google.golang.org/grpc/status"
    
    "github.com/projectbeskar/virtrigaud/proto/rpc/provider/v1"
    "github.com/projectbeskar/virtrigaud/sdk/provider/capabilities"
    "github.com/projectbeskar/virtrigaud/sdk/provider/errors"
)

type Provider struct {
    store *FileStore
    caps  *capabilities.ProviderCapabilities
}

func New() (*Provider, error) {
    // Get storage directory from environment or use default
    baseDir := os.Getenv("PROVIDER_STORAGE_DIR")
    if baseDir == "" {
        baseDir = "/var/lib/virtrigaud/vms"
    }
    
    // Create capabilities
    caps := &amp;capabilities.ProviderCapabilities{
        ProviderInfo: &amp;providerv1.ProviderInfo{
            Name:        "file",
            Version:     "0.1.0",
            Description: "File-based virtual machine provider for development and testing",
        },
        SupportedCapabilities: []capabilities.Capability{
            capabilities.CapabilityCore,
            capabilities.CapabilitySnapshot,
            capabilities.CapabilityClone,
        },
    }
    
    return &amp;Provider{
        store: NewFileStore(baseDir),
        caps:  caps,
    }, nil
}

// GetCapabilities returns provider capabilities
func (p *Provider) GetCapabilities(ctx context.Context, req *providerv1.GetCapabilitiesRequest) (*providerv1.GetCapabilitiesResponse, error) {
    return &amp;providerv1.GetCapabilitiesResponse{
        ProviderId: "file-provider",
        Capabilities: []*providerv1.Capability{
            {
                Name:        "vm.create",
                Supported:   true,
                Description: "Create virtual machines",
            },
            {
                Name:        "vm.read",
                Supported:   true,
                Description: "Read virtual machine information",
            },
            {
                Name:        "vm.update",
                Supported:   true,
                Description: "Update virtual machine configuration",
            },
            {
                Name:        "vm.delete",
                Supported:   true,
                Description: "Delete virtual machines",
            },
            {
                Name:        "vm.power",
                Supported:   true,
                Description: "Control virtual machine power state",
            },
            {
                Name:        "vm.snapshot",
                Supported:   true,
                Description: "Create and manage VM snapshots",
            },
            {
                Name:        "vm.clone",
                Supported:   true,
                Description: "Clone virtual machines",
            },
        },
    }, nil
}

// CreateVM creates a new virtual machine
func (p *Provider) CreateVM(ctx context.Context, req *providerv1.CreateVMRequest) (*providerv1.CreateVMResponse, error) {
    // Validate request
    if req.Name == "" {
        return nil, errors.NewInvalidSpec("VM name is required")
    }
    
    if req.Spec == nil {
        return nil, errors.NewInvalidSpec("VM spec is required")
    }
    
    // Generate unique ID
    vmID := uuid.New().String()
    
    // Create VM object
    vm := &amp;VirtualMachine{
        ID:   vmID,
        Name: req.Name,
        Spec: req.Spec,
        Status: &amp;providerv1.VMStatus{
            State:   "Creating",
            Message: "VM is being created",
        },
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }
    
    // Save to store
    if err := p.store.Save(vm); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to save VM: %v", err)
    }
    
    // Simulate creation time
    go func() {
        time.Sleep(2 * time.Second)
        vm.Status.State = "Running"
        vm.Status.Message = "VM is running"
        p.store.Save(vm)
    }()
    
    return &amp;providerv1.CreateVMResponse{
        VmId:   vmID,
        Status: vm.Status,
    }, nil
}

// GetVM retrieves virtual machine information
func (p *Provider) GetVM(ctx context.Context, req *providerv1.GetVMRequest) (*providerv1.GetVMResponse, error) {
    if req.VmId == "" {
        return nil, errors.NewInvalidSpec("VM ID is required")
    }
    
    vm, err := p.store.Load(req.VmId)
    if err != nil {
        return nil, errors.NewNotFound("VM not found: %s", req.VmId)
    }
    
    return &amp;providerv1.GetVMResponse{
        VmId:   vm.ID,
        Name:   vm.Name,
        Spec:   vm.Spec,
        Status: vm.Status,
    }, nil
}

// UpdateVM updates virtual machine configuration
func (p *Provider) UpdateVM(ctx context.Context, req *providerv1.UpdateVMRequest) (*providerv1.UpdateVMResponse, error) {
    if req.VmId == "" {
        return nil, errors.NewInvalidSpec("VM ID is required")
    }
    
    vm, err := p.store.Load(req.VmId)
    if err != nil {
        return nil, errors.NewNotFound("VM not found: %s", req.VmId)
    }
    
    // Update spec if provided
    if req.Spec != nil {
        vm.Spec = req.Spec
        vm.Status.Message = "VM configuration updated"
        
        if err := p.store.Save(vm); err != nil {
            return nil, status.Errorf(codes.Internal, "failed to save VM: %v", err)
        }
    }
    
    return &amp;providerv1.UpdateVMResponse{
        Status: vm.Status,
    }, nil
}

// DeleteVM deletes a virtual machine
func (p *Provider) DeleteVM(ctx context.Context, req *providerv1.DeleteVMRequest) (*providerv1.DeleteVMResponse, error) {
    if req.VmId == "" {
        return nil, errors.NewInvalidSpec("VM ID is required")
    }
    
    // Check if VM exists
    _, err := p.store.Load(req.VmId)
    if err != nil {
        return nil, errors.NewNotFound("VM not found: %s", req.VmId)
    }
    
    // Delete VM
    if err := p.store.Delete(req.VmId); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to delete VM: %v", err)
    }
    
    return &amp;providerv1.DeleteVMResponse{
        Success: true,
        Message: "VM deleted successfully",
    }, nil
}

// PowerVM controls virtual machine power state
func (p *Provider) PowerVM(ctx context.Context, req *providerv1.PowerVMRequest) (*providerv1.PowerVMResponse, error) {
    if req.VmId == "" {
        return nil, errors.NewInvalidSpec("VM ID is required")
    }
    
    vm, err := p.store.Load(req.VmId)
    if err != nil {
        return nil, errors.NewNotFound("VM not found: %s", req.VmId)
    }
    
    // Update power state based on operation
    switch req.PowerOp {
    case providerv1.PowerOp_POWER_OP_ON:
        vm.Status.State = "Running"
        vm.Status.Message = "VM is running"
    case providerv1.PowerOp_POWER_OP_OFF:
        vm.Status.State = "Stopped"
        vm.Status.Message = "VM is stopped"
    case providerv1.PowerOp_POWER_OP_REBOOT:
        vm.Status.State = "Rebooting"
        vm.Status.Message = "VM is rebooting"
        // Simulate reboot
        go func() {
            time.Sleep(3 * time.Second)
            vm.Status.State = "Running"
            vm.Status.Message = "VM is running"
            p.store.Save(vm)
        }()
    default:
        return nil, errors.NewInvalidSpec("unsupported power operation: %v", req.PowerOp)
    }
    
    if err := p.store.Save(vm); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to save VM: %v", err)
    }
    
    return &amp;providerv1.PowerVMResponse{
        Status: vm.Status,
    }, nil
}

// ListVMs lists all virtual machines
func (p *Provider) ListVMs(ctx context.Context, req *providerv1.ListVMsRequest) (*providerv1.ListVMsResponse, error) {
    vms, err := p.store.List()
    if err != nil {
        return nil, status.Errorf(codes.Internal, "failed to list VMs: %v", err)
    }
    
    var vmInfos []*providerv1.VMInfo
    for _, vm := range vms {
        vmInfos = append(vmInfos, &amp;providerv1.VMInfo{
            VmId:   vm.ID,
            Name:   vm.Name,
            Status: vm.Status,
        })
    }
    
    return &amp;providerv1.ListVMsResponse{
        Vms: vmInfos,
    }, nil
}

// CreateSnapshot creates a VM snapshot
func (p *Provider) CreateSnapshot(ctx context.Context, req *providerv1.CreateSnapshotRequest) (*providerv1.CreateSnapshotResponse, error) {
    if req.VmId == "" {
        return nil, errors.NewInvalidSpec("VM ID is required")
    }
    
    vm, err := p.store.Load(req.VmId)
    if err != nil {
        return nil, errors.NewNotFound("VM not found: %s", req.VmId)
    }
    
    // Create snapshot (simulate by copying VM file)
    snapshotID := uuid.New().String()
    snapshotPath := filepath.Join(filepath.Dir(p.store.baseDir), "snapshots")
    
    if err := os.MkdirAll(snapshotPath, 0755); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to create snapshot directory: %v", err)
    }
    
    // Copy VM data to snapshot
    snapshotVM := *vm
    snapshotVM.ID = snapshotID
    snapshotStore := NewFileStore(snapshotPath)
    
    if err := snapshotStore.Save(&amp;snapshotVM); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to save snapshot: %v", err)
    }
    
    return &amp;providerv1.CreateSnapshotResponse{
        SnapshotId: snapshotID,
        Status: &amp;providerv1.TaskStatus{
            State:   "Completed",
            Message: "Snapshot created successfully",
        },
    }, nil
}

// CloneVM clones a virtual machine
func (p *Provider) CloneVM(ctx context.Context, req *providerv1.CloneVMRequest) (*providerv1.CloneVMResponse, error) {
    if req.SourceVmId == "" {
        return nil, errors.NewInvalidSpec("Source VM ID is required")
    }
    
    if req.CloneName == "" {
        return nil, errors.NewInvalidSpec("Clone name is required")
    }
    
    // Load source VM
    sourceVM, err := p.store.Load(req.SourceVmId)
    if err != nil {
        return nil, errors.NewNotFound("Source VM not found: %s", req.SourceVmId)
    }
    
    // Create clone
    cloneID := uuid.New().String()
    cloneVM := &amp;VirtualMachine{
        ID:   cloneID,
        Name: req.CloneName,
        Spec: sourceVM.Spec, // Copy spec from source
        Status: &amp;providerv1.VMStatus{
            State:   "Stopped",
            Message: "Clone created successfully",
        },
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }
    
    if err := p.store.Save(cloneVM); err != nil {
        return nil, status.Errorf(codes.Internal, "failed to save clone: %v", err)
    }
    
    return &amp;providerv1.CloneVMResponse{
        CloneVmId: cloneID,
        Status: &amp;providerv1.TaskStatus{
            State:   "Completed",
            Message: "VM cloned successfully",
        },
    }, nil
}
</code></pre>
<h2 id="step-3-add-tests-and-validation"><a class="header" href="#step-3-add-tests-and-validation">Step 3: Add Tests and Validation</a></h2>
<h3 id="31-create-unit-tests"><a class="header" href="#31-create-unit-tests">3.1 Create Unit Tests</a></h3>
<p>Create <code>internal/provider/provider_test.go</code>:</p>
<pre><code class="language-go">package provider

import (
    "context"
    "os"
    "path/filepath"
    "testing"
    "time"
    
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
    
    "github.com/projectbeskar/virtrigaud/proto/rpc/provider/v1"
)

func TestProvider_CreateVM(t *testing.T) {
    // Create temporary directory for testing
    tmpDir, err := os.MkdirTemp("", "file-provider-test")
    require.NoError(t, err)
    defer os.RemoveAll(tmpDir)
    
    // Set storage directory
    os.Setenv("PROVIDER_STORAGE_DIR", tmpDir)
    defer os.Unsetenv("PROVIDER_STORAGE_DIR")
    
    // Create provider
    p, err := New()
    require.NoError(t, err)
    
    // Test VM creation
    req := &amp;providerv1.CreateVMRequest{
        Name: "test-vm",
        Spec: &amp;providerv1.VMSpec{
            Cpu:    2,
            Memory: 4096,
            Image:  "ubuntu:20.04",
        },
    }
    
    resp, err := p.CreateVM(context.Background(), req)
    require.NoError(t, err)
    assert.NotEmpty(t, resp.VmId)
    assert.Equal(t, "Creating", resp.Status.State)
    
    // Verify VM file was created
    vmFile := filepath.Join(tmpDir, resp.VmId+".json")
    assert.FileExists(t, vmFile)
}

func TestProvider_GetVM(t *testing.T) {
    tmpDir, err := os.MkdirTemp("", "file-provider-test")
    require.NoError(t, err)
    defer os.RemoveAll(tmpDir)
    
    os.Setenv("PROVIDER_STORAGE_DIR", tmpDir)
    defer os.Unsetenv("PROVIDER_STORAGE_DIR")
    
    p, err := New()
    require.NoError(t, err)
    
    // Create VM first
    createReq := &amp;providerv1.CreateVMRequest{
        Name: "test-vm",
        Spec: &amp;providerv1.VMSpec{
            Cpu:    2,
            Memory: 4096,
        },
    }
    
    createResp, err := p.CreateVM(context.Background(), createReq)
    require.NoError(t, err)
    
    // Get VM
    getReq := &amp;providerv1.GetVMRequest{
        VmId: createResp.VmId,
    }
    
    getResp, err := p.GetVM(context.Background(), getReq)
    require.NoError(t, err)
    assert.Equal(t, createResp.VmId, getResp.VmId)
    assert.Equal(t, "test-vm", getResp.Name)
    assert.Equal(t, int32(2), getResp.Spec.Cpu)
}

func TestProvider_PowerVM(t *testing.T) {
    tmpDir, err := os.MkdirTemp("", "file-provider-test")
    require.NoError(t, err)
    defer os.RemoveAll(tmpDir)
    
    os.Setenv("PROVIDER_STORAGE_DIR", tmpDir)
    defer os.Unsetenv("PROVIDER_STORAGE_DIR")
    
    p, err := New()
    require.NoError(t, err)
    
    // Create VM
    createReq := &amp;providerv1.CreateVMRequest{
        Name: "test-vm",
        Spec: &amp;providerv1.VMSpec{Cpu: 1, Memory: 1024},
    }
    
    createResp, err := p.CreateVM(context.Background(), createReq)
    require.NoError(t, err)
    
    // Power off VM
    powerReq := &amp;providerv1.PowerVMRequest{
        VmId:    createResp.VmId,
        PowerOp: providerv1.PowerOp_POWER_OP_OFF,
    }
    
    powerResp, err := p.PowerVM(context.Background(), powerReq)
    require.NoError(t, err)
    assert.Equal(t, "Stopped", powerResp.Status.State)
    
    // Power on VM
    powerReq.PowerOp = providerv1.PowerOp_POWER_OP_ON
    powerResp, err = p.PowerVM(context.Background(), powerReq)
    require.NoError(t, err)
    assert.Equal(t, "Running", powerResp.Status.State)
}

func TestProvider_GetCapabilities(t *testing.T) {
    p, err := New()
    require.NoError(t, err)
    
    req := &amp;providerv1.GetCapabilitiesRequest{}
    resp, err := p.GetCapabilities(context.Background(), req)
    require.NoError(t, err)
    
    assert.Equal(t, "file-provider", resp.ProviderId)
    assert.NotEmpty(t, resp.Capabilities)
    
    // Check for core capabilities
    capNames := make(map[string]bool)
    for _, cap := range resp.Capabilities {
        capNames[cap.Name] = cap.Supported
    }
    
    assert.True(t, capNames["vm.create"])
    assert.True(t, capNames["vm.read"])
    assert.True(t, capNames["vm.delete"])
    assert.True(t, capNames["vm.power"])
}

func TestProvider_CloneVM(t *testing.T) {
    tmpDir, err := os.MkdirTemp("", "file-provider-test")
    require.NoError(t, err)
    defer os.RemoveAll(tmpDir)
    
    os.Setenv("PROVIDER_STORAGE_DIR", tmpDir)
    defer os.Unsetenv("PROVIDER_STORAGE_DIR")
    
    p, err := New()
    require.NoError(t, err)
    
    // Create source VM
    createReq := &amp;providerv1.CreateVMRequest{
        Name: "source-vm",
        Spec: &amp;providerv1.VMSpec{
            Cpu:    4,
            Memory: 8192,
            Image:  "centos:8",
        },
    }
    
    createResp, err := p.CreateVM(context.Background(), createReq)
    require.NoError(t, err)
    
    // Clone VM
    cloneReq := &amp;providerv1.CloneVMRequest{
        SourceVmId: createResp.VmId,
        CloneName:  "cloned-vm",
    }
    
    cloneResp, err := p.CloneVM(context.Background(), cloneReq)
    require.NoError(t, err)
    assert.NotEmpty(t, cloneResp.CloneVmId)
    assert.NotEqual(t, createResp.VmId, cloneResp.CloneVmId)
    
    // Verify clone has same specs as source
    getReq := &amp;providerv1.GetVMRequest{
        VmId: cloneResp.CloneVmId,
    }
    
    getResp, err := p.GetVM(context.Background(), getReq)
    require.NoError(t, err)
    assert.Equal(t, "cloned-vm", getResp.Name)
    assert.Equal(t, int32(4), getResp.Spec.Cpu)
    assert.Equal(t, int32(8192), getResp.Spec.Memory)
    assert.Equal(t, "centos:8", getResp.Spec.Image)
}
</code></pre>
<h3 id="32-add-build-and-test-targets"><a class="header" href="#32-add-build-and-test-targets">3.2 Add Build and Test Targets</a></h3>
<p>Update the <code>Makefile</code>:</p>
<pre><code class="language-makefile"># File Provider Makefile

.PHONY: help build test lint clean run docker-build docker-push

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

build: ## Build the provider binary
	go build -o bin/provider-file ./cmd/provider-file

test: ## Run tests
	go test -v ./...

test-coverage: ## Run tests with coverage
	go test -v -coverprofile=coverage.out ./...
	go tool cover -html=coverage.out -o coverage.html

lint: ## Run linters
	golangci-lint run ./...

clean: ## Clean build artifacts
	rm -rf bin/
	rm -f coverage.out coverage.html

run: build ## Run the provider locally
	PROVIDER_STORAGE_DIR=/tmp/virtrigaud-file ./bin/provider-file

docker-build: ## Build Docker image
	docker build -f cmd/provider-file/Dockerfile -t provider-file:latest .

docker-push: docker-build ## Build and push Docker image
	docker tag provider-file:latest ghcr.io/yourorg/provider-file:latest
	docker push ghcr.io/yourorg/provider-file:latest

# Development targets
dev-setup: ## Set up development environment
	go mod download
	go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest

integration-test: build ## Run integration tests
	./scripts/integration-test.sh
</code></pre>
<h2 id="step-4-test-with-vcts-virtrigaud-conformance-test-suite"><a class="header" href="#step-4-test-with-vcts-virtrigaud-conformance-test-suite">Step 4: Test with VCTS (VirtRigaud Conformance Test Suite)</a></h2>
<h3 id="41-install-vcts"><a class="header" href="#41-install-vcts">4.1 Install VCTS</a></h3>
<pre><code class="language-bash"># Build VCTS from the main repository
go install github.com/projectbeskar/virtrigaud/cmd/vcts@latest
</code></pre>
<h3 id="42-create-vcts-configuration"><a class="header" href="#42-create-vcts-configuration">4.2 Create VCTS Configuration</a></h3>
<p>Create <code>vcts-config.yaml</code>:</p>
<pre><code class="language-yaml">provider:
  name: "file"
  endpoint: "localhost:9443"
  tls: false
  
profiles:
  core:
    enabled: true
    vm_specs:
      - name: "basic"
        cpu: 1
        memory: 1024
        image: "test:latest"
      - name: "medium"
        cpu: 2
        memory: 4096
        image: "ubuntu:20.04"
        
  snapshot:
    enabled: true
    
  clone:
    enabled: true

tests:
  timeout: "30s"
  parallel: false
  cleanup: true
</code></pre>
<h3 id="43-run-conformance-tests"><a class="header" href="#43-run-conformance-tests">4.3 Run Conformance Tests</a></h3>
<pre><code class="language-bash"># Start the provider
make run &amp;
PROVIDER_PID=$!

# Wait for provider to start
sleep 3

# Run VCTS core profile
vcts run --config vcts-config.yaml --profile core

# Run all enabled profiles
vcts run --config vcts-config.yaml --profile all

# Stop the provider
kill $PROVIDER_PID
</code></pre>
<p>Expected output:</p>
<pre><code>‚úÖ Core Profile Tests
  ‚úÖ Provider.GetCapabilities
  ‚úÖ Provider.CreateVM
  ‚úÖ Provider.GetVM
  ‚úÖ Provider.UpdateVM
  ‚úÖ Provider.DeleteVM
  ‚úÖ Provider.PowerVM
  ‚úÖ Provider.ListVMs

‚úÖ Snapshot Profile Tests
  ‚úÖ Provider.CreateSnapshot

‚úÖ Clone Profile Tests
  ‚úÖ Provider.CloneVM

üéâ All tests passed! Provider is conformant.
</code></pre>
<h2 id="step-5-create-helm-chart-for-deployment"><a class="header" href="#step-5-create-helm-chart-for-deployment">Step 5: Create Helm Chart for Deployment</a></h2>
<h3 id="51-chart-structure"><a class="header" href="#51-chart-structure">5.1 Chart Structure</a></h3>
<p>The generated chart in <code>charts/provider-file/</code> includes:</p>
<pre><code>charts/provider-file/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ serviceaccount.yaml
‚îÇ   ‚îú‚îÄ‚îÄ rbac.yaml
‚îÇ   ‚îî‚îÄ‚îÄ _helpers.tpl
‚îî‚îÄ‚îÄ examples/
    ‚îî‚îÄ‚îÄ values-development.yaml
</code></pre>
<h3 id="52-customize-chart-values"><a class="header" href="#52-customize-chart-values">5.2 Customize Chart Values</a></h3>
<p>Update <code>charts/provider-file/values.yaml</code>:</p>
<pre><code class="language-yaml"># Default values for provider-file

replicaCount: 1

image:
  repository: ghcr.io/yourorg/provider-file
  pullPolicy: IfNotPresent
  tag: "0.1.0"

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext:
  fsGroup: 2000
  runAsNonRoot: true
  runAsUser: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 9443
  healthPort: 8080

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

# Provider-specific configuration
provider:
  storageDir: "/var/lib/virtrigaud/vms"
  logLevel: "info"

# Persistent storage for VM data
persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: 10Gi
  storageClass: ""
</code></pre>
<h3 id="53-test-helm-chart"><a class="header" href="#53-test-helm-chart">5.3 Test Helm Chart</a></h3>
<pre><code class="language-bash"># Lint the chart
helm lint charts/provider-file/

# Template the chart
helm template provider-file charts/provider-file/ \
  --values charts/provider-file/values.yaml

# Install to local cluster
helm install provider-file charts/provider-file/ \
  --namespace provider-file \
  --create-namespace \
  --values charts/provider-file/examples/values-development.yaml
</code></pre>
<h2 id="step-6-set-up-cicd"><a class="header" href="#step-6-set-up-cicd">Step 6: Set Up CI/CD</a></h2>
<h3 id="61-github-actions-workflow"><a class="header" href="#61-github-actions-workflow">6.1 GitHub Actions Workflow</a></h3>
<p>The generated <code>.github/workflows/ci.yml</code> includes:</p>
<pre><code class="language-yaml">name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  GO_VERSION: '1.23'

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}
    
    - name: Run tests
      run: make test

    - name: Run linting
      run: make lint

  build:
    name: Build
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}
    
    - name: Build binary
      run: make build

    - name: Build Docker image
      run: make docker-build

  conformance:
    name: Conformance Tests
    runs-on: ubuntu-latest
    needs: build
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}
    
    - name: Build provider
      run: make build

    - name: Install VCTS
      run: go install github.com/projectbeskar/virtrigaud/cmd/vcts@latest

    - name: Run conformance tests
      run: |
        # Start provider in background
        PROVIDER_STORAGE_DIR=/tmp/vcts-test ./bin/provider-file &amp;
        PROVIDER_PID=$!
        
        # Wait for startup
        sleep 5
        
        # Run VCTS
        vcts run --config vcts-config.yaml --profile core
        
        # Clean up
        kill $PROVIDER_PID

  release:
    name: Release
    runs-on: ubuntu-latest
    needs: [test, build, conformance]
    if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    
    - name: Build and push Docker image
      run: |
        echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
        make docker-push

    - name: Package Helm chart
      run: |
        helm package charts/provider-file/ -d dist/
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: release-artifacts
        path: |
          bin/
          dist/
</code></pre>
<h2 id="step-7-publish-to-provider-catalog"><a class="header" href="#step-7-publish-to-provider-catalog">Step 7: Publish to Provider Catalog</a></h2>
<h3 id="71-run-provider-verification"><a class="header" href="#71-run-provider-verification">7.1 Run Provider Verification</a></h3>
<pre><code class="language-bash"># Verify the provider meets all requirements
vrtg-provider verify --profile all
</code></pre>
<h3 id="72-publish-to-catalog"><a class="header" href="#72-publish-to-catalog">7.2 Publish to Catalog</a></h3>
<pre><code class="language-bash"># Publish to the VirtRigaud provider catalog
vrtg-provider publish \
  --name file \
  --image ghcr.io/yourorg/provider-file \
  --tag 0.1.0 \
  --repo https://github.com/yourorg/virtrigaud-provider-file \
  --maintainer your-email@example.com \
  --license Apache-2.0
</code></pre>
<p>This command will:</p>
<ol>
<li>Run VCTS conformance tests</li>
<li>Generate a provider badge</li>
<li>Create a catalog entry</li>
<li>Open a pull request to the main VirtRigaud repository</li>
</ol>
<h3 id="73-example-catalog-entry"><a class="header" href="#73-example-catalog-entry">7.3 Example Catalog Entry</a></h3>
<p>The generated catalog entry will look like:</p>
<pre><code class="language-yaml">- name: file
  displayName: "File Provider"
  description: "File-based virtual machine provider for development and testing"
  repo: "https://github.com/yourorg/virtrigaud-provider-file"
  image: "ghcr.io/yourorg/provider-file"
  tag: "0.1.0"
  capabilities:
    - core
    - snapshot
    - clone
  conformance:
    profiles:
      core: pass
      snapshot: pass
      clone: pass
      image-prepare: skip
      advanced: skip
    report_url: "https://github.com/yourorg/virtrigaud-provider-file/actions"
    badge_url: "https://img.shields.io/badge/conformance-pass-green"
    last_tested: "2025-08-26T15:00:00Z"
  maintainer: "your-email@example.com"
  license: "Apache-2.0"
  maturity: "beta"
  tags:
    - file
    - development
    - testing
  documentation: "https://github.com/yourorg/virtrigaud-provider-file/blob/main/README.md"
</code></pre>
<h2 id="step-8-production-considerations"><a class="header" href="#step-8-production-considerations">Step 8: Production Considerations</a></h2>
<h3 id="81-security-hardening"><a class="header" href="#81-security-hardening">8.1 Security Hardening</a></h3>
<pre><code class="language-yaml"># Production values.yaml
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65534

podSecurityContext:
  fsGroup: 65534
  runAsNonRoot: true
  runAsUser: 65534
  seccompProfile:
    type: RuntimeDefault

networkPolicy:
  enabled: true
  ingress:
    fromNamespaces:
      - virtrigaud-system
  egress:
    - to: []
      ports:
        - protocol: UDP
          port: 53
</code></pre>
<h3 id="82-observability"><a class="header" href="#82-observability">8.2 Observability</a></h3>
<p>Add monitoring and logging:</p>
<pre><code class="language-go">// Add to provider.go
import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    vmOperations = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "file_provider_vm_operations_total",
            Help: "Total number of VM operations",
        },
        []string{"operation", "status"},
    )
    
    vmOperationDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "file_provider_vm_operation_duration_seconds",
            Help: "Duration of VM operations",
        },
        []string{"operation"},
    )
)

func (p *Provider) CreateVM(ctx context.Context, req *providerv1.CreateVMRequest) (*providerv1.CreateVMResponse, error) {
    start := time.Now()
    defer func() {
        vmOperationDuration.WithLabelValues("create").Observe(time.Since(start).Seconds())
    }()
    
    // ... existing implementation ...
    
    vmOperations.WithLabelValues("create", "success").Inc()
    return resp, nil
}
</code></pre>
<h3 id="83-performance-optimization"><a class="header" href="#83-performance-optimization">8.3 Performance Optimization</a></h3>
<ul>
<li>Add connection pooling for gRPC clients</li>
<li>Implement caching for frequently accessed VMs</li>
<li>Use background workers for long-running operations</li>
<li>Add rate limiting and request validation</li>
</ul>
<h3 id="84-error-handling-and-resilience"><a class="header" href="#84-error-handling-and-resilience">8.4 Error Handling and Resilience</a></h3>
<ul>
<li>Implement circuit breakers for external dependencies</li>
<li>Add retry logic with exponential backoff</li>
<li>Use structured logging with correlation IDs</li>
<li>Implement graceful shutdown handling</li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>You‚Äôve successfully created a complete VirtRigaud provider! This tutorial covered:</p>
<p>‚úÖ <strong>Provider Implementation</strong> - Full gRPC service with all core operations<br />
‚úÖ <strong>SDK Integration</strong> - Using VirtRigaud SDK for server setup and utilities<br />
‚úÖ <strong>Testing</strong> - Unit tests and VCTS conformance validation<br />
‚úÖ <strong>Containerization</strong> - Docker images and Helm charts<br />
‚úÖ <strong>CI/CD</strong> - Automated testing and publishing<br />
‚úÖ <strong>Catalog Integration</strong> - Publishing to the provider ecosystem</p>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<ol>
<li>
<p><strong>Explore Advanced Features</strong>:</p>
<ul>
<li>Add image management capabilities</li>
<li>Implement networking configuration</li>
<li>Add storage volume management</li>
</ul>
</li>
<li>
<p><strong>Integration Examples</strong>:</p>
<ul>
<li>Connect to real hypervisors (libvirt, vSphere, etc.)</li>
<li>Add authentication and authorization</li>
<li>Implement backup and disaster recovery</li>
</ul>
</li>
<li>
<p><strong>Community Contribution</strong>:</p>
<ul>
<li>Submit your provider to the catalog</li>
<li>Contribute improvements to the SDK</li>
<li>Help other developers with provider development</li>
</ul>
</li>
<li>
<p><strong>Production Deployment</strong>:</p>
<ul>
<li>Set up monitoring and alerting</li>
<li>Implement proper security measures</li>
<li>Plan for scaling and high availability</li>
</ul>
</li>
</ol>
<p>For more information, visit the <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud documentation</a> or join our community discussions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="versioning--breaking-changes"><a class="header" href="#versioning--breaking-changes">Versioning &amp; Breaking Changes</a></h1>
<p>This document outlines VirtRigaud‚Äôs approach to versioning, compatibility, and managing breaking changes across the provider ecosystem.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>VirtRigaud follows semantic versioning (SemVer) principles and maintains backward compatibility through careful API design and migration strategies. The system has multiple versioning dimensions:</p>
<ul>
<li><strong>VirtRigaud Core</strong> - The main platform (API server, manager, CRDs)</li>
<li><strong>Provider SDK</strong> - Go SDK for building providers</li>
<li><strong>Proto Contracts</strong> - gRPC/protobuf API definitions</li>
<li><strong>Individual Providers</strong> - Each provider has independent versioning</li>
</ul>
<h2 id="semantic-versioning"><a class="header" href="#semantic-versioning">Semantic Versioning</a></h2>
<p>All VirtRigaud components follow <a href="https://semver.org/">Semantic Versioning 2.0.0</a>:</p>
<h3 id="version-format-majorminorpatch"><a class="header" href="#version-format-majorminorpatch">Version Format: MAJOR.MINOR.PATCH</a></h3>
<ul>
<li><strong>MAJOR</strong> (X.0.0): Breaking changes that require user action</li>
<li><strong>MINOR</strong> (0.X.0): New features that are backward compatible</li>
<li><strong>PATCH</strong> (0.0.X): Bug fixes and security updates</li>
</ul>
<h3 id="examples-3"><a class="header" href="#examples-3">Examples</a></h3>
<pre><code>1.0.0 ‚Üí 1.0.1  # Patch: Bug fixes only
1.0.1 ‚Üí 1.1.0  # Minor: New features, backward compatible
1.1.0 ‚Üí 2.0.0  # Major: Breaking changes
</code></pre>
<h2 id="component-versioning-strategy"><a class="header" href="#component-versioning-strategy">Component Versioning Strategy</a></h2>
<h3 id="virtrigaud-core-apis"><a class="header" href="#virtrigaud-core-apis">VirtRigaud Core APIs</a></h3>
<p>Kubernetes-style API versioning with multiple supported versions:</p>
<pre><code class="language-yaml"># Supported API versions
apiVersion: infra.virtrigaud.io/v1beta1  # Development/preview
apiVersion: infra.virtrigaud.io/v1beta1   # Pre-release/testing
apiVersion: infra.virtrigaud.io/v1        # Stable/production
</code></pre>
<p><strong>Stability Levels:</strong></p>
<ul>
<li><strong>Alpha (v1beta1)</strong>: Experimental, may change or be removed</li>
<li><strong>Beta (v1beta1)</strong>: Well-tested, minimal changes expected</li>
<li><strong>Stable (v1)</strong>: Production-ready, strong backward compatibility</li>
</ul>
<p><strong>Support Windows:</strong></p>
<ul>
<li>Alpha: Best effort, no guarantees</li>
<li>Beta: Supported for 2 minor releases after stable equivalent</li>
<li>Stable: Supported for 12 months after deprecation</li>
</ul>
<h3 id="provider-sdk-versioning"><a class="header" href="#provider-sdk-versioning">Provider SDK Versioning</a></h3>
<p>SDK versions are independent of core VirtRigaud versions:</p>
<pre><code class="language-go">// Go module versioning
module github.com/projectbeskar/virtrigaud/sdk

// Version tags
sdk/v0.1.0    # Initial release
sdk/v0.2.0    # New features
sdk/v1.0.0    # First stable release
sdk/v2.0.0    # Breaking changes (new module path: sdk/v2)
</code></pre>
<p><strong>SDK Compatibility Matrix:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>SDK Version</th><th>VirtRigaud Core</th><th>Go Version</th><th>Status</th></tr></thead><tbody>
<tr><td>v0.1.x</td><td>0.1.0 - 0.2.x</td><td>1.23+</td><td>Beta</td></tr>
<tr><td>v1.0.x</td><td>0.2.0 - 1.0.x</td><td>1.23+</td><td>Stable</td></tr>
<tr><td>v1.1.x</td><td>0.3.0 - 1.1.x</td><td>1.23+</td><td>Stable</td></tr>
<tr><td>v2.0.x</td><td>1.0.0+</td><td>1.24+</td><td>Future</td></tr>
</tbody></table>
</div>
<h3 id="proto-contract-versioning"><a class="header" href="#proto-contract-versioning">Proto Contract Versioning</a></h3>
<p>Protobuf APIs use both module versions and service versions:</p>
<pre><code class="language-protobuf">// Service versioning in proto files
package provider.v1;
service ProviderService {
  // API methods
}

// Module versioning
module github.com/projectbeskar/virtrigaud/proto
</code></pre>
<p><strong>Proto Evolution Rules:</strong></p>
<ul>
<li>‚úÖ Add new fields (with proper defaults)</li>
<li>‚úÖ Add new RPC methods</li>
<li>‚úÖ Add new enum values</li>
<li>‚ùå Remove fields or methods</li>
<li>‚ùå Change field types or semantics</li>
<li>‚ùå Remove enum values</li>
</ul>
<h3 id="provider-versioning"><a class="header" href="#provider-versioning">Provider Versioning</a></h3>
<p>Each provider maintains independent versioning:</p>
<pre><code class="language-yaml"># Provider catalog entry
name: vsphere
tag: "1.2.3"      # Provider version
sdk_version: "v1.0.0"  # SDK dependency
proto_version: "v0.1.0"  # Proto dependency
</code></pre>
<h2 id="breaking-change-policy"><a class="header" href="#breaking-change-policy">Breaking Change Policy</a></h2>
<h3 id="what-constitutes-a-breaking-change"><a class="header" href="#what-constitutes-a-breaking-change">What Constitutes a Breaking Change</a></h3>
<p><strong>API Breaking Changes:</strong></p>
<ul>
<li>Removing or renaming API fields</li>
<li>Changing field types or semantics</li>
<li>Removing API endpoints or methods</li>
<li>Changing required vs optional fields</li>
<li>Modifying default behaviors</li>
<li>Changing error codes or messages that clients depend on</li>
</ul>
<p><strong>SDK Breaking Changes:</strong></p>
<ul>
<li>Removing public functions, types, or methods</li>
<li>Changing function signatures</li>
<li>Modifying struct fields (without proper backward compatibility)</li>
<li>Changing package import paths</li>
<li>Removing or renaming configuration options</li>
</ul>
<p><strong>Proto Breaking Changes:</strong></p>
<ul>
<li>Removing fields or RPC methods</li>
<li>Changing field numbers or types</li>
<li>Removing enum values</li>
<li>Modifying service or method names</li>
</ul>
<h3 id="breaking-change-process"><a class="header" href="#breaking-change-process">Breaking Change Process</a></h3>
<h4 id="1-proposal-phase"><a class="header" href="#1-proposal-phase">1. Proposal Phase</a></h4>
<pre><code class="language-markdown"># Breaking Change Proposal: [Title]

## Summary
Brief description of the change and motivation.

## Motivation  
Why is this change necessary? What problems does it solve?

## Proposed Changes
Detailed description of the changes.

## Migration Path
How will users migrate from old to new behavior?

## Timeline
- Deprecation announcement: v1.1.0
- Breaking change implementation: v2.0.0
- Legacy support removal: v3.0.0

## Alternatives Considered
What other approaches were considered?
</code></pre>
<h4 id="2-deprecation-phase"><a class="header" href="#2-deprecation-phase">2. Deprecation Phase</a></h4>
<pre><code class="language-go">// Deprecated functions include clear migration guidance
// Deprecated: Use NewCreateVMRequest instead. Will be removed in v2.0.0.
func CreateVM(name string) *VMRequest {
    return &amp;VMRequest{Name: name}
}

// New recommended approach
func NewCreateVMRequest(spec *VMSpec) *CreateVMRequest {
    return &amp;CreateVMRequest{Spec: spec}
}
</code></pre>
<h4 id="3-migration-tools"><a class="header" href="#3-migration-tools">3. Migration Tools</a></h4>
<pre><code class="language-bash"># Migration command examples
vrtg-provider migrate --from v1 --to v2
vrtg-provider check-compatibility --target-version v2.0.0
</code></pre>
<h4 id="4-communication"><a class="header" href="#4-communication">4. Communication</a></h4>
<ul>
<li>Release notes with migration guide</li>
<li>Blog posts for major changes</li>
<li>Community discussions and Q&amp;A</li>
<li>Updated documentation</li>
</ul>
<h2 id="compatibility-testing"><a class="header" href="#compatibility-testing">Compatibility Testing</a></h2>
<h3 id="automated-compatibility-checks"><a class="header" href="#automated-compatibility-checks">Automated Compatibility Checks</a></h3>
<pre><code class="language-yaml"># .github/workflows/compatibility.yml
name: Compatibility Check

jobs:
  compatibility-matrix:
    strategy:
      matrix:
        sdk_version: [v1.0.0, v1.1.0, current]
        provider_version: [v1.0.0, v1.1.0, current]
    
    steps:
    - name: Test SDK ${{ matrix.sdk_version }} with Provider ${{ matrix.provider_version }}
      run: |
        # Build provider with specific SDK version
        # Run conformance tests
        # Report compatibility results
</code></pre>
<h3 id="buf-proto-compatibility"><a class="header" href="#buf-proto-compatibility">Buf Proto Compatibility</a></h3>
<pre><code class="language-yaml"># proto/buf.yaml
version: v1
breaking:
  use:
    # Prevent breaking changes
    - FILE_NO_DELETE
    - FIELD_NO_DELETE
    - FIELD_SAME_TYPE
    - ENUM_VALUE_NO_DELETE
    - RPC_NO_DELETE
    - SERVICE_NO_DELETE
  ignore:
    # Allowed changes during alpha/beta
    - "provider/v1beta1"
</code></pre>
<pre><code class="language-bash"># Check for breaking changes
buf breaking --against 'https://github.com/projectbeskar/virtrigaud.git#branch=main'
</code></pre>
<h3 id="provider-compatibility-testing"><a class="header" href="#provider-compatibility-testing">Provider Compatibility Testing</a></h3>
<pre><code class="language-bash"># Test provider against multiple VirtRigaud versions
vcts run --provider ./provider --virtrigaud-version 0.1.0
vcts run --provider ./provider --virtrigaud-version 0.2.0
vcts run --provider ./provider --virtrigaud-version 1.0.0
</code></pre>
<h2 id="migration-strategies"><a class="header" href="#migration-strategies">Migration Strategies</a></h2>
<h3 id="api-version-migration"><a class="header" href="#api-version-migration">API Version Migration</a></h3>
<h4 id="example-virtualmachine-v1beta1--v1beta1"><a class="header" href="#example-virtualmachine-v1beta1--v1beta1">Example: VirtualMachine v1beta1 ‚Üí v1beta1</a></h4>
<pre><code class="language-go">// Conversion webhook approach
func (src *v1beta1.VirtualMachine) ConvertTo(dst *v1beta1.VirtualMachine) error {
    // Convert common fields
    dst.ObjectMeta = src.ObjectMeta
    
    // Handle field migrations
    if src.Spec.PowerState == "On" {
        dst.Spec.PowerState = v1beta1.PowerStateOn
    }
    
    // Set new fields with appropriate defaults
    if dst.Spec.Phase == "" {
        dst.Spec.Phase = v1beta1.PhaseUnknown
    }
    
    return nil
}
</code></pre>
<h4 id="gradual-migration-process"><a class="header" href="#gradual-migration-process">Gradual Migration Process</a></h4>
<pre><code class="language-bash"># Phase 1: Dual support (both versions work)
kubectl apply -f vm-v1beta1.yaml  # Still works
kubectl apply -f vm-v1beta1.yaml   # Also works

# Phase 2: Deprecation warning
kubectl apply -f vm-v1beta1.yaml
# Warning: v1beta1 is deprecated, use v1beta1

# Phase 3: Conversion only (internal storage uses v1beta1)
kubectl apply -f vm-v1beta1.yaml  # Automatically converted

# Phase 4: Removal (after support window)
kubectl apply -f vm-v1beta1.yaml  # Error: version not supported
</code></pre>
<h3 id="provider-sdk-migration"><a class="header" href="#provider-sdk-migration">Provider SDK Migration</a></h3>
<h4 id="example-sdk-v1--v2"><a class="header" href="#example-sdk-v1--v2">Example: SDK v1 ‚Üí v2</a></h4>
<p><strong>SDK v1 (deprecated):</strong></p>
<pre><code class="language-go">// Old SDK pattern
func NewProvider(config Config) *Provider {
    return &amp;Provider{config: config}
}

func (p *Provider) CreateVM(name string, cpu int, memory int) error {
    // Implementation
}
</code></pre>
<p><strong>SDK v2 (new):</strong></p>
<pre><code class="language-go">// New SDK pattern with better types
func NewProvider(config *Config) (*Provider, error) {
    if err := config.Validate(); err != nil {
        return nil, err
    }
    return &amp;Provider{config: config}, nil
}

func (p *Provider) CreateVM(ctx context.Context, req *CreateVMRequest) (*CreateVMResponse, error) {
    // Implementation with proper context and structured types
}
</code></pre>
<p><strong>Migration Bridge:</strong></p>
<pre><code class="language-go">// sdk/v2/compat/v1.go - Compatibility layer
package compat

import (
    v1 "github.com/projectbeskar/virtrigaud/sdk/provider"
    v2 "github.com/projectbeskar/virtrigaud/sdk/v2/provider"
)

// Bridge for gradual migration
func AdaptV1Provider(v1Provider v1.Provider) v2.Provider {
    return &amp;v1ProviderAdapter{old: v1Provider}
}

type v1ProviderAdapter struct {
    old v1.Provider
}

func (a *v1ProviderAdapter) CreateVM(ctx context.Context, req *v2.CreateVMRequest) (*v2.CreateVMResponse, error) {
    // Convert v2 request to v1 format
    err := a.old.CreateVM(req.Name, int(req.Spec.CPU), int(req.Spec.Memory))
    
    // Convert v1 response to v2 format
    if err != nil {
        return nil, err
    }
    
    return &amp;v2.CreateVMResponse{
        Status: "Created",
    }, nil
}
</code></pre>
<h3 id="configuration-migration"><a class="header" href="#configuration-migration">Configuration Migration</a></h3>
<h4 id="example-configuration-schema-changes"><a class="header" href="#example-configuration-schema-changes">Example: Configuration Schema Changes</a></h4>
<p><strong>v1 Configuration:</strong></p>
<pre><code class="language-yaml"># provider-config-v1.yaml
provider:
  type: "vsphere"
  server: "vcenter.example.com"
  username: "admin"
  password: "secret"
</code></pre>
<p><strong>v2 Configuration:</strong></p>
<pre><code class="language-yaml"># provider-config-v2.yaml
apiVersion: config.virtrigaud.io/v2
kind: ProviderConfig
metadata:
  name: vsphere-config
spec:
  type: "vsphere"
  connection:
    endpoint: "vcenter.example.com"
    authentication:
      method: "basic"
      secretRef:
        name: "vsphere-credentials"
  features:
    snapshots: true
    cloning: true
</code></pre>
<p><strong>Migration Command:</strong></p>
<pre><code class="language-bash"># Automatic migration tool
vrtg-provider config migrate \
  --from provider-config-v1.yaml \
  --to provider-config-v2.yaml \
  --create-secret vsphere-credentials
</code></pre>
<h2 id="release-planning"><a class="header" href="#release-planning">Release Planning</a></h2>
<h3 id="release-cadence"><a class="header" href="#release-cadence">Release Cadence</a></h3>
<ul>
<li><strong>Patch releases</strong>: As needed for critical bugs/security</li>
<li><strong>Minor releases</strong>: Every 2-3 months</li>
<li><strong>Major releases</strong>: Every 12-18 months</li>
</ul>
<h3 id="feature-lifecycle"><a class="header" href="#feature-lifecycle">Feature Lifecycle</a></h3>
<pre><code>Experimental ‚Üí Alpha ‚Üí Beta ‚Üí Stable ‚Üí Deprecated ‚Üí Removed
     |          |       |       |         |          |
     |          |       |       |         |          +-- After support window
     |          |       |       |         +-- 2 releases notice
     |          |       |       +-- Production ready
     |          |       +-- Pre-release testing
     |          +-- Public preview
     +-- Internal/development only
</code></pre>
<h3 id="release-branch-strategy"><a class="header" href="#release-branch-strategy">Release Branch Strategy</a></h3>
<pre><code>main                    # Current development
‚îú‚îÄ‚îÄ release-0.1        # Patch releases for v0.1.x
‚îú‚îÄ‚îÄ release-0.2        # Patch releases for v0.2.x
‚îî‚îÄ‚îÄ release-1.0        # Patch releases for v1.0.x
</code></pre>
<h3 id="support-matrix-1"><a class="header" href="#support-matrix-1">Support Matrix</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Version</th><th>Status</th><th>Support Level</th><th>End of Life</th></tr></thead><tbody>
<tr><td>1.0.x</td><td>Stable</td><td>Full support</td><td>2026-01-01</td></tr>
<tr><td>0.2.x</td><td>Stable</td><td>Security only</td><td>2025-06-01</td></tr>
<tr><td>0.1.x</td><td>Deprecated</td><td>None</td><td>2025-01-01</td></tr>
</tbody></table>
</div>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<h3 id="for-provider-developers"><a class="header" href="#for-provider-developers">For Provider Developers</a></h3>
<ol>
<li>
<p><strong>Version Dependencies Carefully</strong></p>
<pre><code class="language-go">// Use specific versions, not floating
require github.com/projectbeskar/virtrigaud/sdk v1.2.3
</code></pre>
</li>
<li>
<p><strong>Test Compatibility Early</strong></p>
<pre><code class="language-bash"># Test against multiple SDK versions
go mod edit -require=github.com/projectbeskar/virtrigaud/sdk@v1.1.0
go test ./...
go mod edit -require=github.com/projectbeskar/virtrigaud/sdk@v1.2.0
go test ./...
</code></pre>
</li>
<li>
<p><strong>Handle Deprecations Gracefully</strong></p>
<pre><code class="language-go">// Check for deprecated features
if provider.IsDeprecated("vm.legacy-create") {
    log.Warn("Using deprecated API, migrate to vm.create")
}
</code></pre>
</li>
<li>
<p><strong>Document Breaking Changes</strong></p>
<pre><code class="language-markdown"># CHANGELOG.md
## [2.0.0] - 2025-01-15
### BREAKING CHANGES
- Removed deprecated `CreateVM` method, use `CreateVMRequest` instead
- Changed configuration format, see migration guide

### Migration Guide
Old: `provider.CreateVM("vm1", 2, 4096)`
New: `provider.CreateVM(ctx, &amp;CreateVMRequest{...})`
</code></pre>
</li>
</ol>
<h3 id="for-users"><a class="header" href="#for-users">For Users</a></h3>
<ol>
<li>
<p><strong>Pin Versions in Production</strong></p>
<pre><code class="language-yaml"># Helm values
image:
  tag: "1.2.3"  # Not "latest"
</code></pre>
</li>
<li>
<p><strong>Test Upgrades in Staging</strong></p>
<pre><code class="language-bash"># Upgrade strategy
helm upgrade provider-test virtrigaud/provider \
  --version 1.3.0 \
  --namespace staging
</code></pre>
</li>
<li>
<p><strong>Monitor Deprecation Warnings</strong></p>
<pre><code class="language-bash"># Check for deprecation warnings
kubectl logs -l app=provider | grep -i deprecat
</code></pre>
</li>
<li>
<p><strong>Plan Migration Windows</strong></p>
<pre><code class="language-yaml"># Schedule upgrades during maintenance windows
# Have rollback plans ready
# Test compatibility thoroughly
</code></pre>
</li>
</ol>
<h2 id="future-considerations"><a class="header" href="#future-considerations">Future Considerations</a></h2>
<h3 id="long-term-compatibility"><a class="header" href="#long-term-compatibility">Long-term Compatibility</a></h3>
<ul>
<li><strong>10-year Support Goal</strong>: Core APIs should remain usable for 10 years</li>
<li><strong>Gradual Evolution</strong>: Prefer gradual evolution over revolutionary changes</li>
<li><strong>Ecosystem Stability</strong>: Consider impact on the entire provider ecosystem</li>
</ul>
<h3 id="emerging-standards"><a class="header" href="#emerging-standards">Emerging Standards</a></h3>
<ul>
<li><strong>OCI Compliance</strong>: Align with OCI runtime and image standards</li>
<li><strong>CNCF Integration</strong>: Follow CNCF project graduation requirements</li>
<li><strong>Industry Standards</strong>: Adopt relevant industry standards as they emerge</li>
</ul>
<h3 id="technology-evolution"><a class="header" href="#technology-evolution">Technology Evolution</a></h3>
<ul>
<li><strong>Go Version Support</strong>: Support 2-3 latest Go versions</li>
<li><strong>Kubernetes Compatibility</strong>: Support 3-4 latest Kubernetes versions</li>
<li><strong>gRPC Evolution</strong>: Adapt to gRPC and protobuf improvements</li>
</ul>
<p>This versioning strategy ensures VirtRigaud can evolve while maintaining stability and compatibility for the provider ecosystem.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-vm-lifecycle-management"><a class="header" href="#advanced-vm-lifecycle-management">Advanced VM Lifecycle Management</a></h1>
<p>This document describes the advanced VM lifecycle features in VirtRigaud, including reconfiguration, snapshots, cloning, multi-VM sets, and placement policies.</p>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>VirtRigaud Stage E introduces comprehensive VM lifecycle management capabilities that go beyond basic create/delete operations:</p>
<ul>
<li><strong>VM Reconfiguration</strong>: Modify CPU, memory, and disk resources of running VMs</li>
<li><strong>Snapshot Management</strong>: Create, delete, and revert VM snapshots</li>
<li><strong>VM Cloning</strong>: Create new VMs from existing ones with linked clone support</li>
<li><strong>Multi-VM Sets</strong>: Manage groups of VMs with rolling updates</li>
<li><strong>Placement Policies</strong>: Advanced placement rules and anti-affinity constraints</li>
<li><strong>Image Preparation</strong>: Automated image import and preparation workflows</li>
</ul>
<h2 id="vm-reconfiguration"><a class="header" href="#vm-reconfiguration">VM Reconfiguration</a></h2>
<h3 id="online-vs-offline-reconfiguration"><a class="header" href="#online-vs-offline-reconfiguration">Online vs Offline Reconfiguration</a></h3>
<p>VirtRigaud supports both online (hot) and offline reconfiguration depending on provider capabilities:</p>
<p><strong>vSphere</strong>: Supports online CPU/memory changes and hot disk expansion
<strong>Libvirt</strong>: Typically requires power cycle for resource changes</p>
<h3 id="example-cpumemory-upgrade"><a class="header" href="#example-cpumemory-upgrade">Example: CPU/Memory Upgrade</a></h3>
<pre><code class="language-yaml"># Original VM with 2 CPU, 4GB RAM
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server
spec:
  resources:
    cpu: 2
    memoryMiB: 4096

# Patch to upgrade resources
# kubectl patch vm web-server --type merge -p '{"spec":{"resources":{"cpu":4,"memoryMiB":8192}}}'
</code></pre>
<p>The controller will:</p>
<ol>
<li>Detect resource changes in VM spec</li>
<li>Attempt online reconfiguration if supported</li>
<li>If offline required, orchestrate graceful power cycle:
<ul>
<li>Set condition <code>ReconfigurePendingPowerCycle=True</code></li>
<li>Power off VM gracefully</li>
<li>Apply reconfiguration</li>
<li>Power on VM</li>
<li>Update <code>status.lastReconfigureTime</code></li>
</ul>
</li>
</ol>
<h3 id="disk-expansion"><a class="header" href="#disk-expansion">Disk Expansion</a></h3>
<pre><code class="language-yaml">spec:
  disks:
    - name: data
      sizeGiB: 100  # Expanded from 50GB
      expandPolicy: "Online"  # Try online first
</code></pre>
<h2 id="snapshot-management-2"><a class="header" href="#snapshot-management-2">Snapshot Management</a></h2>
<h3 id="creating-snapshots"><a class="header" href="#creating-snapshots">Creating Snapshots</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSnapshot
metadata:
  name: pre-maintenance-backup
spec:
  vmRef:
    name: web-server
  nameHint: "maintenance-backup"
  memory: true  # Include memory state
  description: "Backup before maintenance"
  retentionPolicy:
    maxAge: "7d"
    deleteOnVMDelete: true
</code></pre>
<h3 id="snapshot-lifecycle"><a class="header" href="#snapshot-lifecycle">Snapshot Lifecycle</a></h3>
<ol>
<li><strong>Creating</strong>: Snapshot creation in progress</li>
<li><strong>Ready</strong>: Snapshot available for use</li>
<li><strong>Deleting</strong>: Snapshot being removed</li>
<li><strong>Failed</strong>: Snapshot operation failed</li>
</ol>
<h3 id="reverting-to-snapshots"><a class="header" href="#reverting-to-snapshots">Reverting to Snapshots</a></h3>
<pre><code class="language-yaml"># Patch VM to revert to snapshot
spec:
  snapshot:
    revertToRef:
      name: pre-maintenance-backup
</code></pre>
<p>The controller will:</p>
<ol>
<li>Power off VM if running</li>
<li>Call provider‚Äôs SnapshotRevert RPC</li>
<li>Power on VM</li>
<li>Clear <code>revertToRef</code> when complete</li>
</ol>
<h2 id="vm-cloning"><a class="header" href="#vm-cloning">VM Cloning</a></h2>
<h3 id="basic-cloning"><a class="header" href="#basic-cloning">Basic Cloning</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClone
metadata:
  name: web-server-clone
spec:
  sourceRef:
    name: web-server
  target:
    name: web-server-test
    classRef:
      name: test-class
  linked: true  # Faster, space-efficient
  powerOn: true
</code></pre>
<h3 id="clone-customization"><a class="header" href="#clone-customization">Clone Customization</a></h3>
<pre><code class="language-yaml">spec:
  customization:
    hostname: web-server-test
    networks:
      - name: primary
        ipAddress: "192.168.1.100"
        gateway: "192.168.1.1"
        dns: ["8.8.8.8"]
    userData:
      cloudInit:
        inline: |
          #cloud-config
          runcmd:
            - echo "Test environment" &gt; /etc/motd
</code></pre>
<h2 id="multi-vm-sets-vmset"><a class="header" href="#multi-vm-sets-vmset">Multi-VM Sets (VMSet)</a></h2>
<p>VMSets provide declarative management of multiple VMs with rolling updates.</p>
<h3 id="basic-vmset"><a class="header" href="#basic-vmset">Basic VMSet</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMSet
metadata:
  name: web-tier
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-server
  template:
    metadata:
      labels:
        app: web-server
    spec:
      providerRef:
        name: vsphere-prod
      classRef:
        name: web-class
      imageRef:
        name: nginx-image
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
</code></pre>
<h3 id="rolling-updates"><a class="header" href="#rolling-updates">Rolling Updates</a></h3>
<p>When you update the template spec, VMSet will:</p>
<ol>
<li>Create new VMs with updated configuration</li>
<li>Wait for new VMs to be ready</li>
<li>Delete old VMs respecting <code>maxUnavailable</code></li>
<li>Continue until all replicas are updated</li>
</ol>
<h2 id="placement-policies-1"><a class="header" href="#placement-policies-1">Placement Policies</a></h2>
<h3 id="advanced-placement-rules"><a class="header" href="#advanced-placement-rules">Advanced Placement Rules</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMPlacementPolicy
metadata:
  name: production-policy
spec:
  hard:
    clusters: ["prod-cluster-1", "prod-cluster-2"]
    datastores: ["ssd-datastore-1", "ssd-datastore-2"]
    hosts: ["esxi-01", "esxi-02", "esxi-03"]
  soft:
    folders: ["/Production/WebServers"]
    zones: ["zone-a", "zone-b"]
  antiAffinity:
    hostAntiAffinity: true      # Spread across hosts
    clusterAntiAffinity: false
    datastoreAntiAffinity: true # Spread across datastores
</code></pre>
<h3 id="using-placement-policies"><a class="header" href="#using-placement-policies">Using Placement Policies</a></h3>
<pre><code class="language-yaml">spec:
  placementRef:
    name: production-policy
</code></pre>
<p>The provider will attempt to satisfy:</p>
<ol>
<li><strong>Hard constraints</strong>: Must be satisfied</li>
<li><strong>Soft constraints</strong>: Best effort</li>
<li><strong>Anti-affinity rules</strong>: Avoid co-location</li>
</ol>
<h2 id="image-preparation"><a class="header" href="#image-preparation">Image Preparation</a></h2>
<h3 id="automated-image-import"><a class="header" href="#automated-image-import">Automated Image Import</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-22-04
spec:
  vsphere:
    ovaURL: "https://releases.ubuntu.com/22.04/ubuntu-22.04-server.ova"
    checksum: "sha256:abcd1234..."
  libvirt:
    url: "https://cloud-images.ubuntu.com/22.04/ubuntu-22.04-server.img"
    format: "qcow2"
  prepare:
    onMissing: "Import"  # Auto-import if missing
    validateChecksum: true
    timeout: "30m"
    retries: 3
    storage:
      vsphere:
        datastore: "images-datastore"
        folder: "/Templates"
        thinProvisioned: true
</code></pre>
<h3 id="image-preparation-phases"><a class="header" href="#image-preparation-phases">Image Preparation Phases</a></h3>
<ol>
<li><strong>Pending</strong>: Waiting to start preparation</li>
<li><strong>Importing</strong>: Downloading/importing image</li>
<li><strong>Preparing</strong>: Processing image (conversion, etc.)</li>
<li><strong>Ready</strong>: Image ready for use</li>
<li><strong>Failed</strong>: Preparation failed</li>
</ol>
<h2 id="provider-capabilities"><a class="header" href="#provider-capabilities">Provider Capabilities</a></h2>
<p>Different providers support different features. Query capabilities:</p>
<pre><code class="language-yaml"># Example capabilities response
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
status:
  capabilities:
    supportsReconfigureOnline: true      # vSphere: true, Libvirt: false
    supportsDiskExpansionOnline: true    # vSphere: true, Libvirt: false
    supportsSnapshots: true              # Both: true
    supportsMemorySnapshots: true        # vSphere: true, Libvirt: varies
    supportsLinkedClones: true           # Both: true
    supportsImageImport: true            # Both: true
    supportedDiskTypes: ["thin", "thick"]
    supportedNetworkTypes: ["VMXNET3", "E1000"]
</code></pre>
<h2 id="observability"><a class="header" href="#observability">Observability</a></h2>
<h3 id="metrics"><a class="header" href="#metrics">Metrics</a></h3>
<p>New metrics for advanced lifecycle operations:</p>
<pre><code>virtrigaud_vm_reconfigure_total{provider_type,outcome}
virtrigaud_vm_snapshot_total{action,provider_type,outcome}
virtrigaud_vm_clone_total{linked,provider_type,outcome}
virtrigaud_vm_image_prepare_total{provider_type,outcome}
</code></pre>
<h3 id="events"><a class="header" href="#events">Events</a></h3>
<p>Detailed events for lifecycle operations:</p>
<pre><code>Normal   SnapshotCreating    Started snapshot creation
Normal   SnapshotReady       Snapshot created successfully
Normal   ReconfigureStarted  Started VM reconfiguration
Warning  ReconfigurePowerCycle  Reconfiguration requires power cycle
Normal   CloneCompleted      VM clone created successfully
</code></pre>
<h3 id="conditions"><a class="header" href="#conditions">Conditions</a></h3>
<p>Comprehensive condition reporting:</p>
<p><strong>VM Conditions</strong>:</p>
<ul>
<li><code>Ready</code>: VM is ready for use</li>
<li><code>Provisioning</code>: VM is being created</li>
<li><code>Reconfiguring</code>: VM is being reconfigured</li>
<li><code>ReconfigurePendingPowerCycle</code>: Needs power cycle for changes</li>
</ul>
<p><strong>Snapshot Conditions</strong>:</p>
<ul>
<li><code>Ready</code>: Snapshot is ready</li>
<li><code>Creating</code>: Snapshot being created</li>
<li><code>Deleting</code>: Snapshot being deleted</li>
</ul>
<p><strong>Clone Conditions</strong>:</p>
<ul>
<li><code>Ready</code>: Clone completed successfully</li>
<li><code>Cloning</code>: Clone operation in progress</li>
<li><code>Customizing</code>: Applying customizations</li>
</ul>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<h3 id="snapshot-management-3"><a class="header" href="#snapshot-management-3">Snapshot Management</a></h3>
<ol>
<li><strong>Retention Policies</strong>: Always set appropriate retention policies</li>
<li><strong>Memory Snapshots</strong>: Use sparingly due to storage overhead</li>
<li><strong>Cleanup</strong>: Implement automated cleanup for old snapshots</li>
<li><strong>Testing</strong>: Test snapshot revert procedures regularly</li>
</ol>
<h3 id="vm-reconfiguration-1"><a class="header" href="#vm-reconfiguration-1">VM Reconfiguration</a></h3>
<ol>
<li><strong>Gradual Changes</strong>: Make incremental resource changes</li>
<li><strong>Monitoring</strong>: Monitor VM performance after changes</li>
<li><strong>Rollback Plan</strong>: Have snapshots before major changes</li>
<li><strong>Capacity Planning</strong>: Ensure host resources before scaling up</li>
</ol>
<h3 id="placement-policies-2"><a class="header" href="#placement-policies-2">Placement Policies</a></h3>
<ol>
<li><strong>Start Simple</strong>: Begin with basic constraints</li>
<li><strong>Test Anti-Affinity</strong>: Verify rules work as expected</li>
<li><strong>Monitor Placement</strong>: Check actual VM placement matches policy</li>
<li><strong>Balance Performance</strong>: Don‚Äôt over-constrain placement</li>
</ol>
<h3 id="multi-vm-operations"><a class="header" href="#multi-vm-operations">Multi-VM Operations</a></h3>
<ol>
<li><strong>Rolling Updates</strong>: Use appropriate <code>maxUnavailable</code> settings</li>
<li><strong>Health Checks</strong>: Implement proper readiness checks</li>
<li><strong>Monitoring</strong>: Monitor rollout progress</li>
<li><strong>Rollback Strategy</strong>: Plan for rollback scenarios</li>
</ol>
<h2 id="troubleshooting-8"><a class="header" href="#troubleshooting-8">Troubleshooting</a></h2>
<h3 id="common-issues-5"><a class="header" href="#common-issues-5">Common Issues</a></h3>
<p><strong>Reconfiguration Fails</strong>:</p>
<ul>
<li>Check provider capabilities</li>
<li>Verify resource availability on host</li>
<li>Check for VM tools/agent issues</li>
</ul>
<p><strong>Snapshot Operations Fail</strong>:</p>
<ul>
<li>Verify storage backend supports snapshots</li>
<li>Check available storage space</li>
<li>Ensure VM is not in transitional state</li>
</ul>
<p><strong>Clone Customization Issues</strong>:</p>
<ul>
<li>Verify network configuration</li>
<li>Check cloud-init/guest tools</li>
<li>Validate IP address availability</li>
</ul>
<p><strong>Placement Policy Violations</strong>:</p>
<ul>
<li>Check resource availability in target locations</li>
<li>Verify anti-affinity rules aren‚Äôt too restrictive</li>
<li>Review cluster resource distribution</li>
</ul>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<pre><code class="language-bash"># Check VM reconfiguration status
kubectl describe vm web-server

# Monitor snapshot progress
kubectl get vmsnapshots -w

# Check clone status
kubectl describe vmclone web-server-clone

# Review placement policy usage
kubectl describe vmplacementpolicy production-policy

# Check VMSet rollout
kubectl describe vmset web-tier
</code></pre>
<h2 id="migration-from-basic-vms"><a class="header" href="#migration-from-basic-vms">Migration from Basic VMs</a></h2>
<p>Existing VMs can be enhanced with advanced features:</p>
<ol>
<li><strong>Add Placement Policy</strong>: Update VM spec with <code>placementRef</code></li>
<li><strong>Enable Reconfiguration</strong>: Add resource overrides</li>
<li><strong>Create Snapshots</strong>: Deploy VMSnapshot resources</li>
<li><strong>Scale with VMSets</strong>: Migrate to VMSet for multi-instance workloads</li>
</ol>
<p>The controller maintains backward compatibility with existing VM definitions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nested-virtualization-support"><a class="header" href="#nested-virtualization-support">Nested Virtualization Support</a></h1>
<p>This document describes how to enable and configure nested virtualization in VirtRigaud virtual machines across different hypervisor providers.</p>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Nested virtualization allows virtual machines to run hypervisors and create their own virtual machines. This is useful for:</p>
<ul>
<li>Development and testing of virtualization software</li>
<li>Running container orchestration platforms like Kubernetes</li>
<li>Creating nested lab environments</li>
<li>Educational purposes for learning virtualization concepts</li>
</ul>
<p>VirtRigaud supports nested virtualization through the <code>PerformanceProfile</code> configuration in VMClass resources.</p>
<h2 id="prerequisites-6"><a class="header" href="#prerequisites-6">Prerequisites</a></h2>
<h3 id="vsphere-provider-1"><a class="header" href="#vsphere-provider-1">vSphere Provider</a></h3>
<ul>
<li>ESXi 6.0 or later</li>
<li>VM hardware version 9 or later (recommended: version 14+)</li>
<li>ESXi host must have VT-x/AMD-V enabled in BIOS</li>
<li>Sufficient CPU and memory resources on the ESXi host</li>
</ul>
<h3 id="libvirt-provider"><a class="header" href="#libvirt-provider">LibVirt Provider</a></h3>
<ul>
<li>QEMU/KVM hypervisor</li>
<li>Host CPU with VT-x (Intel) or AMD-V (AMD) support</li>
<li>Nested virtualization enabled in host kernel modules</li>
<li>libvirt 1.2.13 or later</li>
</ul>
<h3 id="proxmox-provider"><a class="header" href="#proxmox-provider">Proxmox Provider</a></h3>
<ul>
<li>Proxmox VE 6.0 or later</li>
<li>Host CPU with nested virtualization support</li>
<li>Nested virtualization enabled in Proxmox configuration</li>
</ul>
<h2 id="enabling-nested-virtualization"><a class="header" href="#enabling-nested-virtualization">Enabling Nested Virtualization</a></h2>
<p>Nested virtualization is configured at the VMClass level using the <code>PerformanceProfile</code> section:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: nested-vm-class
  namespace: virtrigaud-system
spec:
  cpu: 4
  memory: 8Gi
  firmware: UEFI  # Recommended for modern features
  
  # Enable nested virtualization
  performanceProfile:
    nestedVirtualization: true
    # Optional: Enable additional features
    virtualizationBasedSecurity: true
    cpuHotAddEnabled: true
    memoryHotAddEnabled: true
  
  # Optional: Security features that work well with nested virtualization
  securityProfile:
    secureBoot: false  # May interfere with some nested hypervisors
    tpmEnabled: false  # Optional, depending on nested OS requirements
    vtdEnabled: true   # Enable VT-d/AMD-Vi for better performance
  
  diskDefaults:
    type: thin
    size: 100Gi  # Larger disk for nested VMs
</code></pre>
<h2 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h2>
<p>Here‚Äôs a complete example showing how to create a VM with nested virtualization support:</p>
<pre><code class="language-yaml">---
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: hypervisor-class
  namespace: default
spec:
  cpu: 8
  memory: 16Gi
  firmware: UEFI
  
  performanceProfile:
    nestedVirtualization: true
    virtualizationBasedSecurity: false  # May conflict with nested hypervisors
    cpuHotAddEnabled: true
    memoryHotAddEnabled: true
    latencySensitivity: low  # Better performance for nested VMs
    hyperThreadingPolicy: prefer
  
  securityProfile:
    secureBoot: false  # Disable for compatibility
    tpmEnabled: false
    vtdEnabled: true   # Enable for better I/O performance
  
  resourceLimits:
    cpuReservation: 4000  # Reserve 4GHz for nested VMs
    memoryReservation: 8Gi
  
  diskDefaults:
    type: thin
    size: 200Gi
    storageClass: fast-ssd

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMImage
metadata:
  name: ubuntu-server-22-04
  namespace: default
spec:
  source:
    libvirt:
      url: "https://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64.img"
      checksum: "sha256:de5e632e17b8965f2baf4ea6d2b824788e154d9a65df4fd419ec4019898e15cd"

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: nested-hypervisor
  namespace: default
spec:
  providerRef:
    name: my-provider
  classRef:
    name: hypervisor-class
  imageRef:
    name: ubuntu-server-22-04
  
  userData:
    cloudInit:
      inline: |
        #cloud-config
        hostname: nested-hypervisor
        users:
          - name: ubuntu
            sudo: ALL=(ALL) NOPASSWD:ALL
            ssh_authorized_keys:
              - ssh-rsa AAAAB3NzaC1yc2E... # Your SSH key
        
        packages:
          - qemu-kvm
          - libvirt-daemon-system
          - libvirt-clients
          - bridge-utils
          - virt-manager
        
        runcmd:
          # Enable nested virtualization verification
          - echo "Checking nested virtualization support..."
          - cat /proc/cpuinfo | grep -E "(vmx|svm)"
          - ls -la /dev/kvm
          
          # Configure libvirt
          - systemctl enable libvirtd
          - systemctl start libvirtd
          - usermod -aG libvirt ubuntu
          
          # Verify nested KVM support
          - modprobe kvm_intel nested=1 || modprobe kvm_amd nested=1
          - echo "Nested virtualization setup complete"
  
  powerState: On
</code></pre>
<h2 id="provider-specific-configuration"><a class="header" href="#provider-specific-configuration">Provider-Specific Configuration</a></h2>
<h3 id="vsphere-provider-2"><a class="header" href="#vsphere-provider-2">vSphere Provider</a></h3>
<p>For vSphere, nested virtualization is enabled using the following VM configuration:</p>
<ul>
<li><code>vhv.enable = TRUE</code> - Enables hardware-assisted virtualization</li>
<li><code>vhv.allowNestedPageTables = TRUE</code> - Improves nested VM performance</li>
<li>Hardware version 14+ recommended for best compatibility</li>
</ul>
<p>Additional considerations:</p>
<ul>
<li>Use UEFI firmware for modern guest operating systems</li>
<li>Ensure sufficient CPU and memory allocation</li>
<li>Consider enabling VT-d for better I/O performance</li>
</ul>
<h3 id="libvirt-provider-1"><a class="header" href="#libvirt-provider-1">LibVirt Provider</a></h3>
<p>For LibVirt/KVM, nested virtualization requires:</p>
<ul>
<li>Host kernel modules: <code>kvm_intel nested=1</code> or <code>kvm_amd nested=1</code></li>
<li>CPU features: <code>vmx</code> (Intel) or <code>svm</code> (AMD) passed through to guest</li>
<li>QEMU machine type: <code>q35</code> recommended for modern features</li>
</ul>
<p>The LibVirt provider automatically configures:</p>
<pre><code class="language-xml">&lt;cpu mode='host-model' check='partial'&gt;
  &lt;feature policy='require' name='vmx'/&gt;  &lt;!-- Intel --&gt;
  &lt;feature policy='require' name='svm'/&gt;  &lt;!-- AMD --&gt;
&lt;/cpu&gt;
</code></pre>
<h3 id="proxmox-provider-1"><a class="header" href="#proxmox-provider-1">Proxmox Provider</a></h3>
<p>For Proxmox VE, nested virtualization is configured through:</p>
<ul>
<li>CPU type: <code>host</code> or <code>kvm64</code> with nested features</li>
<li>Enable nested virtualization in VM CPU configuration</li>
<li>Ensure host has nested virtualization enabled</li>
</ul>
<h2 id="verification"><a class="header" href="#verification">Verification</a></h2>
<p>After creating a VM with nested virtualization enabled, verify the setup:</p>
<h3 id="on-linux-guests"><a class="header" href="#on-linux-guests">On Linux Guests</a></h3>
<pre><code class="language-bash"># Check for virtualization extensions
grep -E "(vmx|svm)" /proc/cpuinfo

# Verify KVM device availability
ls -la /dev/kvm

# Check nested virtualization status
cat /sys/module/kvm_intel/parameters/nested  # Intel
cat /sys/module/kvm_amd/parameters/nested    # AMD

# Test with a simple nested VM
virt-host-validate
</code></pre>
<h3 id="on-windows-guests"><a class="header" href="#on-windows-guests">On Windows Guests</a></h3>
<pre><code class="language-powershell"># Check Hyper-V compatibility
systeminfo | findstr /i hyper

# Verify virtualization extensions
Get-ComputerInfo | Select-Object HyperV*
</code></pre>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<h3 id="cpu-allocation"><a class="header" href="#cpu-allocation">CPU Allocation</a></h3>
<ul>
<li>Allocate sufficient CPU cores (minimum 4, recommended 8+)</li>
<li>Consider CPU reservation for consistent performance</li>
<li>Enable CPU hot-add for flexibility</li>
</ul>
<h3 id="memory-configuration"><a class="header" href="#memory-configuration">Memory Configuration</a></h3>
<ul>
<li>Allocate generous memory (minimum 8GB, recommended 16GB+)</li>
<li>Consider memory reservation for nested VMs</li>
<li>Enable memory hot-add for dynamic scaling</li>
</ul>
<h3 id="storage"><a class="header" href="#storage">Storage</a></h3>
<ul>
<li>Use fast storage (SSD/NVMe) for better nested VM performance</li>
<li>Allocate sufficient disk space for multiple nested VMs</li>
<li>Consider thin provisioning for efficient space usage</li>
</ul>
<h3 id="network"><a class="header" href="#network">Network</a></h3>
<ul>
<li>Configure appropriate network topology</li>
<li>Consider SR-IOV for high-performance networking</li>
<li>Plan IP address allocation for nested environments</li>
</ul>
<h2 id="troubleshooting-9"><a class="header" href="#troubleshooting-9">Troubleshooting</a></h2>
<h3 id="common-issues-6"><a class="header" href="#common-issues-6">Common Issues</a></h3>
<ol>
<li>
<p><strong>Nested virtualization not working</strong></p>
<ul>
<li>Verify host CPU supports VT-x/AMD-V</li>
<li>Check host BIOS settings</li>
<li>Ensure hypervisor nested virtualization is enabled</li>
</ul>
</li>
<li>
<p><strong>Poor performance in nested VMs</strong></p>
<ul>
<li>Increase CPU and memory allocation</li>
<li>Enable CPU/memory reservations</li>
<li>Use faster storage</li>
<li>Verify nested page tables are enabled</li>
</ul>
</li>
<li>
<p><strong>Guest OS doesn‚Äôt detect virtualization extensions</strong></p>
<ul>
<li>Check VM hardware version (vSphere)</li>
<li>Verify CPU feature passthrough (LibVirt)</li>
<li>Ensure proper CPU type configuration (Proxmox)</li>
</ul>
</li>
</ol>
<h3 id="debugging-commands"><a class="header" href="#debugging-commands">Debugging Commands</a></h3>
<pre><code class="language-bash"># Check virtualization support on host
lscpu | grep Virtualization

# Verify KVM nested support
cat /sys/module/kvm_*/parameters/nested

# Check VM CPU features (inside guest)
lscpu | grep -E "(vmx|svm|Virtualization)"

# Test nested VM creation
virt-install --name test-nested --memory 1024 --vcpus 1 --disk size=10 --cdrom /path/to/iso
</code></pre>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<h3 id="isolation"><a class="header" href="#isolation">Isolation</a></h3>
<ul>
<li>Nested VMs add additional attack surface</li>
<li>Consider network isolation for nested environments</li>
<li>Implement proper access controls</li>
</ul>
<h3 id="resource-limits"><a class="header" href="#resource-limits">Resource Limits</a></h3>
<ul>
<li>Set appropriate resource limits to prevent resource exhaustion</li>
<li>Monitor nested VM resource usage</li>
<li>Implement quotas for nested environments</li>
</ul>
<h3 id="updates-and-patches"><a class="header" href="#updates-and-patches">Updates and Patches</a></h3>
<ul>
<li>Keep host hypervisor updated</li>
<li>Maintain guest hypervisor software</li>
<li>Apply security patches to nested VMs</li>
</ul>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<ol>
<li>
<p><strong>Planning</strong></p>
<ul>
<li>Design nested architecture carefully</li>
<li>Plan resource allocation in advance</li>
<li>Consider network topology requirements</li>
</ul>
</li>
<li>
<p><strong>Configuration</strong></p>
<ul>
<li>Use UEFI firmware for modern features</li>
<li>Enable VT-d/AMD-Vi for better performance</li>
<li>Configure appropriate CPU and memory reservations</li>
</ul>
</li>
<li>
<p><strong>Monitoring</strong></p>
<ul>
<li>Monitor resource usage at all levels</li>
<li>Set up alerting for resource exhaustion</li>
<li>Track performance metrics</li>
</ul>
</li>
<li>
<p><strong>Maintenance</strong></p>
<ul>
<li>Regular backup of nested environments</li>
<li>Plan for hypervisor updates</li>
<li>Test disaster recovery procedures</li>
</ul>
</li>
</ol>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<h3 id="vsphere-provider-3"><a class="header" href="#vsphere-provider-3">vSphere Provider</a></h3>
<ul>
<li>Requires ESXi 6.0+ and hardware version 9+</li>
<li>Performance overhead of 10-20% typical</li>
<li>Some advanced features may not be available in nested VMs</li>
</ul>
<h3 id="libvirt-provider-2"><a class="header" href="#libvirt-provider-2">LibVirt Provider</a></h3>
<ul>
<li>Requires host kernel support</li>
<li>Performance depends on host CPU features</li>
<li>Limited to x86_64 architecture</li>
</ul>
<h3 id="proxmox-provider-2"><a class="header" href="#proxmox-provider-2">Proxmox Provider</a></h3>
<ul>
<li>Requires Proxmox VE 6.0+</li>
<li>Performance overhead varies by workload</li>
<li>Some clustering features may not work in nested environments</li>
</ul>
<h2 id="support-matrix-2"><a class="header" href="#support-matrix-2">Support Matrix</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Min Version</th><th>Nested Support</th><th>Performance</th><th>Security Features</th></tr></thead><tbody>
<tr><td>vSphere</td><td>ESXi 6.0</td><td>Full</td><td>Good</td><td>TPM, Secure Boot</td></tr>
<tr><td>LibVirt</td><td>1.2.13</td><td>Full</td><td>Good</td><td>TPM, Secure Boot</td></tr>
<tr><td>Proxmox</td><td>PVE 6.0</td><td>Planned</td><td>Good</td><td>Limited</td></tr>
</tbody></table>
</div>
<p>For more information, see the provider-specific documentation in the <code>docs/providers/</code> directory.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="graceful-shutdown-feature"><a class="header" href="#graceful-shutdown-feature">Graceful Shutdown Feature</a></h1>
<p>The virtrigaud VM management platform now supports graceful shutdown of virtual machines to prevent data corruption and ensure proper cleanup of running processes.</p>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Graceful shutdown uses VM guest tools (VMware Tools, QEMU Guest Agent, etc.) to properly shut down the operating system before powering off the virtual machine. This prevents data corruption and allows applications to save their state properly.</p>
<h2 id="power-states"><a class="header" href="#power-states">Power States</a></h2>
<p>virtrigaud supports three power states:</p>
<ul>
<li><code>On</code>: Power on the VM</li>
<li><code>Off</code>: Hard power off (immediate shutdown without guest OS notification)</li>
<li><code>OffGraceful</code>: Graceful shutdown using guest tools with automatic fallback to hard power off</li>
</ul>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: my-vm
spec:
  powerState: OffGraceful  # Use graceful shutdown
  # ... other configuration
</code></pre>
<h3 id="advanced-configuration-with-lifecycle-hooks"><a class="header" href="#advanced-configuration-with-lifecycle-hooks">Advanced Configuration with Lifecycle Hooks</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: my-vm
spec:
  powerState: OffGraceful
  
  lifecycle:
    # Timeout for graceful shutdown (default: 60s)
    gracefulShutdownTimeout: "120s"
    
    # Pre-stop hook runs before shutdown
    preStop:
      exec:
        command:
          - "/bin/bash"
          - "-c"
          - |
            # Save application state
            systemctl stop my-application
            # Sync filesystem
            sync
</code></pre>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<h3 id="vsphere-provider-4"><a class="header" href="#vsphere-provider-4">vSphere Provider</a></h3>
<ol>
<li><strong>Guest Tools Check</strong>: Verifies VMware Tools is installed and running</li>
<li><strong>Graceful Shutdown</strong>: Calls <code>vm.ShutdownGuest()</code> to initiate OS shutdown</li>
<li><strong>Monitoring</strong>: Polls VM power state every 2 seconds</li>
<li><strong>Timeout Handling</strong>: Falls back to hard power off if timeout is reached</li>
<li><strong>Fallback</strong>: Uses <code>vm.PowerOff()</code> if graceful shutdown fails</li>
</ol>
<h3 id="libvirt-provider-3"><a class="header" href="#libvirt-provider-3">Libvirt Provider</a></h3>
<ol>
<li><strong>Graceful Attempt</strong>: Uses <code>virsh shutdown</code> command</li>
<li><strong>Fallback</strong>: Falls back to <code>virsh destroy</code> if shutdown fails</li>
<li><strong>Guest Agent</strong>: Requires QEMU Guest Agent for best results</li>
</ol>
<h3 id="proxmox-provider-3"><a class="header" href="#proxmox-provider-3">Proxmox Provider</a></h3>
<ol>
<li><strong>API Call</strong>: Uses Proxmox <code>shutdown</code> API endpoint</li>
<li><strong>Built-in Timeout</strong>: Proxmox handles timeout and fallback internally</li>
</ol>
<h2 id="default-timeouts"><a class="header" href="#default-timeouts">Default Timeouts</a></h2>
<ul>
<li><strong>vSphere</strong>: 60 seconds (configurable via gRPC request)</li>
<li><strong>Libvirt</strong>: Immediate fallback if <code>virsh shutdown</code> fails</li>
<li><strong>Proxmox</strong>: Managed by Proxmox server configuration</li>
</ul>
<h2 id="requirements-4"><a class="header" href="#requirements-4">Requirements</a></h2>
<h3 id="vmware-vsphere"><a class="header" href="#vmware-vsphere">VMware vSphere</a></h3>
<ul>
<li>VMware Tools must be installed and running in the guest OS</li>
<li>Guest OS must support ACPI shutdown signals</li>
</ul>
<h3 id="libvirtkvm-1"><a class="header" href="#libvirtkvm-1">Libvirt/KVM</a></h3>
<ul>
<li>QEMU Guest Agent recommended for reliable graceful shutdown</li>
<li>Guest OS must support ACPI shutdown signals</li>
</ul>
<h3 id="proxmox-1"><a class="header" href="#proxmox-1">Proxmox</a></h3>
<ul>
<li>QEMU Guest Agent recommended</li>
<li>Guest OS must support ACPI shutdown signals</li>
</ul>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<ol>
<li><strong>Always Install Guest Tools</strong>: Ensure VMware Tools or QEMU Guest Agent is installed</li>
<li><strong>Test Graceful Shutdown</strong>: Verify your VMs respond properly to shutdown signals</li>
<li><strong>Set Appropriate Timeouts</strong>: Allow enough time for applications to shut down gracefully</li>
<li><strong>Use Lifecycle Hooks</strong>: Implement pre-stop hooks for critical applications</li>
<li><strong>Monitor Logs</strong>: Check provider logs to verify graceful shutdown is working</li>
</ol>
<h2 id="troubleshooting-10"><a class="header" href="#troubleshooting-10">Troubleshooting</a></h2>
<h3 id="graceful-shutdown-not-working"><a class="header" href="#graceful-shutdown-not-working">Graceful Shutdown Not Working</a></h3>
<ol>
<li>
<p><strong>Check Guest Tools Status</strong>:</p>
<pre><code class="language-bash"># For VMware
vmware-toolbox-cmd stat running

# For QEMU/KVM
systemctl status qemu-guest-agent
</code></pre>
</li>
<li>
<p><strong>Verify ACPI Support</strong>:</p>
<pre><code class="language-bash"># Check if ACPI shutdown is supported
cat /proc/acpi/button/power/*/info
</code></pre>
</li>
<li>
<p><strong>Test Manual Shutdown</strong>:</p>
<pre><code class="language-bash"># Test graceful shutdown manually
sudo shutdown -h now
</code></pre>
</li>
</ol>
<h3 id="timeout-issues"><a class="header" href="#timeout-issues">Timeout Issues</a></h3>
<p>If VMs consistently hit the graceful shutdown timeout:</p>
<ol>
<li><strong>Increase Timeout</strong>: Set a longer <code>gracefulShutdownTimeout</code></li>
<li><strong>Optimize Applications</strong>: Ensure applications shut down quickly</li>
<li><strong>Check System Resources</strong>: Verify the system isn‚Äôt under heavy load</li>
</ol>
<h3 id="fallback-to-hard-power-off"><a class="header" href="#fallback-to-hard-power-off">Fallback to Hard Power Off</a></h3>
<p>The provider will automatically fall back to hard power off if:</p>
<ul>
<li>Guest tools are not available</li>
<li>Graceful shutdown times out</li>
<li>Guest tools command fails</li>
</ul>
<p>This ensures VMs are always powered off even if graceful shutdown isn‚Äôt possible.</p>
<h2 id="examples-4"><a class="header" href="#examples-4">Examples</a></h2>
<p>See <code>examples/graceful-shutdown-vm.yaml</code> for complete examples of using graceful shutdown with various configurations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provider-architecture"><a class="header" href="#provider-architecture">Provider Architecture</a></h1>
<p>This document describes the provider architecture in VirtRigaud.</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>VirtRigaud uses a <strong>Remote Provider</strong> architecture where providers run as independent pods, communicating with the manager controller via gRPC. This design provides scalability, security, and reliability benefits.</p>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   VirtualMachine ‚îÇ    ‚îÇ     Provider      ‚îÇ    ‚îÇ Provider Runtime‚îÇ
‚îÇ      CRD        ‚îÇ    ‚îÇ       CRD         ‚îÇ    ‚îÇ   Deployment    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                        ‚îÇ                        ‚îÇ
         ‚îÇ                        ‚îÇ                        ‚îÇ
         v                        v                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ    Manager      ‚îÇ    ‚îÇ Provider          ‚îÇ              ‚îÇ
‚îÇ   Controller    ‚îÇ    ‚îÇ Controller        ‚îÇ              ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                   ‚îÇ              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ - Creates Deploy  ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ VM Reconcile‚îÇ    ‚îÇ - Creates Service ‚îÇ              ‚îÇ
‚îÇ   ‚îÇ             ‚îÇ    ‚îÇ - Updates Status  ‚îÇ              ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ                   ‚îÇ              ‚îÇ
‚îÇ                 ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                                       ‚îÇ
‚îÇ   ‚îÇ gRPC Client ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ   ‚îÇ             ‚îÇ        gRPC Connection
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§        Port 9090
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h2 id="provider-components"><a class="header" href="#provider-components">Provider Components</a></h2>
<h3 id="1-provider-runtime-deployments"><a class="header" href="#1-provider-runtime-deployments">1. Provider Runtime Deployments</a></h3>
<p>Each Provider resource automatically creates:</p>
<ul>
<li><strong>Deployment</strong>: Runs provider-specific containers</li>
<li><strong>Service</strong>: ClusterIP service for gRPC communication</li>
<li><strong>ConfigMaps</strong>: Provider configuration</li>
<li><strong>Secret mounts</strong>: Credentials for hypervisor access</li>
</ul>
<h3 id="configuration-flow-provider-resource--provider-pod"><a class="header" href="#configuration-flow-provider-resource--provider-pod">Configuration Flow: Provider Resource ‚Üí Provider Pod</a></h3>
<p>The VirtRigaud Provider Controller automatically translates your Provider resource configuration into the appropriate command-line arguments and environment variables for the provider pod.</p>
<h4 id="command-line-arguments"><a class="header" href="#command-line-arguments">Command-Line Arguments</a></h4>
<p>The controller generates these arguments from your Provider spec:</p>
<div class="table-wrapper"><table><thead><tr><th>Provider Field</th><th>Generated Argument</th><th>Example</th></tr></thead><tbody>
<tr><td><code>spec.type</code></td><td><code>--provider-type</code></td><td><code>--provider-type=vsphere</code></td></tr>
<tr><td><code>spec.endpoint</code></td><td><code>--provider-endpoint</code></td><td><code>--provider-endpoint=https://vcenter.example.com</code></td></tr>
<tr><td><code>spec.runtime.service.port</code></td><td><code>--grpc-addr</code></td><td><code>--grpc-addr=:9090</code></td></tr>
<tr><td>(hardcoded)</td><td><code>--metrics-addr</code></td><td><code>--metrics-addr=:8080</code></td></tr>
<tr><td>(optional)</td><td><code>--tls-enabled</code></td><td><code>--tls-enabled=false</code></td></tr>
</tbody></table>
</div>
<h4 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h4>
<p>The controller also sets these environment variables:</p>
<div class="table-wrapper"><table><thead><tr><th>Provider Field</th><th>Environment Variable</th><th>Example</th></tr></thead><tbody>
<tr><td><code>spec.type</code></td><td><code>PROVIDER_TYPE</code></td><td><code>vsphere</code></td></tr>
<tr><td><code>spec.endpoint</code></td><td><code>PROVIDER_ENDPOINT</code></td><td><code>https://vcenter.example.com</code></td></tr>
<tr><td><code>metadata.namespace</code></td><td><code>PROVIDER_NAMESPACE</code></td><td><code>default</code></td></tr>
<tr><td><code>metadata.name</code></td><td><code>PROVIDER_NAME</code></td><td><code>vsphere-datacenter</code></td></tr>
<tr><td>(optional)</td><td><code>TLS_ENABLED</code></td><td><code>false</code></td></tr>
</tbody></table>
</div>
<h4 id="secret-volume-mounts"><a class="header" href="#secret-volume-mounts">Secret Volume Mounts</a></h4>
<p>Credentials from <code>spec.credentialSecretRef</code> are automatically mounted at:</p>
<ul>
<li><strong>Mount Path</strong>: <code>/etc/virtrigaud/credentials/</code></li>
<li><strong>Files Created</strong>: Each secret key becomes a file
<ul>
<li><code>username</code> ‚Üí <code>/etc/virtrigaud/credentials/username</code></li>
<li><code>password</code> ‚Üí <code>/etc/virtrigaud/credentials/password</code></li>
<li><code>token</code> ‚Üí <code>/etc/virtrigaud/credentials/token</code></li>
</ul>
</li>
</ul>
<h4 id="complete-example-1"><a class="header" href="#complete-example-1">Complete Example</a></h4>
<p>When you create this Provider resource:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-datacenter
  namespace: default
spec:
  type: vsphere
  endpoint: "https://vcenter.example.com:443"
  credentialSecretRef:
    name: vsphere-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0"
    service:
      port: 9090
</code></pre>
<p>The controller automatically creates a deployment with:</p>
<p><strong>Command-line arguments:</strong></p>
<pre><code class="language-bash">/provider-vsphere \
  --grpc-addr=:9090 \
  --metrics-addr=:8080 \
  --provider-type=vsphere \
  --provider-endpoint=https://vcenter.example.com:443 \
  --tls-enabled=false
</code></pre>
<p><strong>Environment variables:</strong></p>
<pre><code class="language-bash">PROVIDER_TYPE=vsphere
PROVIDER_ENDPOINT=https://vcenter.example.com:443
PROVIDER_NAMESPACE=default
PROVIDER_NAME=vsphere-datacenter
TLS_ENABLED=false
</code></pre>
<p><strong>Volume mounts:</strong></p>
<pre><code class="language-bash">/etc/virtrigaud/credentials/username  # Contains: admin@vsphere.local
/etc/virtrigaud/credentials/password  # Contains: your-password
</code></pre>
<h3 id="-key-point-you-dont-configure-this-manually"><a class="header" href="#-key-point-you-dont-configure-this-manually"><strong>‚úÖ Key Point: You Don‚Äôt Configure This Manually</strong></a></h3>
<p>The beauty of VirtRigaud‚Äôs Remote Provider architecture is that <strong>you never need to manually configure command-line arguments or environment variables</strong>. Simply create the Provider resource, and the controller handles all the deployment details automatically!</p>
<h3 id="2-provider-images"><a class="header" href="#2-provider-images">2. Provider Images</a></h3>
<p>Specialized images for each provider type:</p>
<ul>
<li><strong>ghcr.io/projectbeskar/virtrigaud/provider-vsphere</strong>: vSphere provider with govmomi</li>
<li><strong>ghcr.io/projectbeskar/virtrigaud/provider-libvirt</strong>: LibVirt provider via virsh commands</li>
<li><strong>ghcr.io/projectbeskar/virtrigaud/provider-proxmox</strong>: Proxmox VE provider</li>
<li><strong>ghcr.io/projectbeskar/virtrigaud/provider-mock</strong>: Mock provider for testing</li>
</ul>
<h3 id="3-grpc-communication"><a class="header" href="#3-grpc-communication">3. gRPC Communication</a></h3>
<ul>
<li><strong>Protocol</strong>: gRPC with protocol buffers</li>
<li><strong>Security</strong>: Secure communication over TLS (optional)</li>
<li><strong>Health</strong>: Built-in health checks and graceful shutdown</li>
<li><strong>Metrics</strong>: Prometheus metrics on port 8080</li>
</ul>
<h2 id="provider-configuration-1"><a class="header" href="#provider-configuration-1">Provider Configuration</a></h2>
<h3 id="basic-provider-setup-1"><a class="header" href="#basic-provider-setup-1">Basic Provider Setup</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: vsphere-credentials
  namespace: default
type: Opaque
stringData:
  username: "admin@vsphere.local"
  password: "your-password"

---
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: vsphere-datacenter
  namespace: default
spec:
  type: vsphere
  endpoint: "https://vcenter.example.com:443"
  credentialSecretRef:
    name: vsphere-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0"
    service:
      port: 9090
</code></pre>
<h3 id="advanced-configuration-1"><a class="header" href="#advanced-configuration-1">Advanced Configuration</a></h3>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: libvirt-cluster
  namespace: production
spec:
  type: libvirt
  endpoint: "qemu+ssh://admin@kvm.example.com/system"
  credentialSecretRef:
    name: libvirt-credentials
  defaults:
    cluster: production
  rateLimit:
    qps: 20
    burst: 50
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.0"
    replicas: 3
    
    service:
      port: 9090
      
    resources:
      requests:
        cpu: "200m"
        memory: "256Mi"
      limits:
        cpu: "2"
        memory: "2Gi"
        
    # High availability setup
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: libvirt-cluster
          topologyKey: kubernetes.io/hostname
          
    # Node placement
    nodeSelector:
      workload-type: compute
      
    tolerations:
    - key: "compute-dedicated"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
      
    # Environment variables
    env:
    - name: LIBVIRT_DEBUG
      value: "1"
    - name: PROVIDER_TIMEOUT
      value: "300s"
</code></pre>
<h2 id="security-model"><a class="header" href="#security-model">Security Model</a></h2>
<h3 id="pod-security"><a class="header" href="#pod-security">Pod Security</a></h3>
<ul>
<li><strong>Non-root execution</strong>: All containers run as non-root users</li>
<li><strong>Read-only filesystem</strong>: Immutable container filesystem</li>
<li><strong>Minimal capabilities</strong>: Reduced Linux capabilities</li>
<li><strong>Security contexts</strong>: Enforced via deployment templates</li>
</ul>
<h3 id="credential-isolation"><a class="header" href="#credential-isolation">Credential Isolation</a></h3>
<ul>
<li><strong>Separated secrets</strong>: Each provider has dedicated credential secrets</li>
<li><strong>Scoped access</strong>: Providers only access their own hypervisor credentials</li>
<li><strong>RBAC isolation</strong>: Fine-grained RBAC per provider namespace</li>
</ul>
<h3 id="network-security"><a class="header" href="#network-security">Network Security</a></h3>
<ul>
<li><strong>Service mesh ready</strong>: Compatible with Istio/Linkerd</li>
<li><strong>Network policies</strong>: Optional traffic restrictions</li>
<li><strong>TLS support</strong>: Secure gRPC communication (configurable)</li>
</ul>
<h2 id="communication-protocol"><a class="header" href="#communication-protocol">Communication Protocol</a></h2>
<h3 id="grpc-service-definition"><a class="header" href="#grpc-service-definition">gRPC Service Definition</a></h3>
<pre><code class="language-protobuf">service Provider {
  rpc Validate(ValidateRequest) returns (ValidateResponse);
  rpc Create(CreateRequest) returns (CreateResponse);
  rpc Delete(DeleteRequest) returns (TaskResponse);
  rpc Power(PowerRequest) returns (TaskResponse);
  rpc Reconfigure(ReconfigureRequest) returns (TaskResponse);
  rpc Describe(DescribeRequest) returns (DescribeResponse);
  rpc TaskStatus(TaskStatusRequest) returns (TaskStatusResponse);
  rpc ListCapabilities(CapabilitiesRequest) returns (CapabilitiesResponse);
}
</code></pre>
<h3 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h3>
<ul>
<li><strong>Retry logic</strong>: Exponential backoff for transient failures</li>
<li><strong>Circuit breakers</strong>: Prevent cascade failures</li>
<li><strong>Timeout controls</strong>: Configurable per-operation timeouts</li>
<li><strong>Status reporting</strong>: Conditions reflected in Kubernetes status</li>
</ul>
<h2 id="observability-1"><a class="header" href="#observability-1">Observability</a></h2>
<h3 id="metrics-1"><a class="header" href="#metrics-1">Metrics</a></h3>
<p>Provider pods expose Prometheus metrics on port 8080:</p>
<pre><code># Request metrics
provider_grpc_requests_total{method="Create",status="success"} 42
provider_grpc_request_duration_seconds{method="Create",quantile="0.95"} 2.5

# VM metrics  
provider_vms_total{state="running"} 15
provider_vms_total{state="stopped"} 3

# Health metrics
provider_health_status{provider="vsphere-datacenter"} 1
provider_hypervisor_connection_status{endpoint="vcenter.example.com"} 1
</code></pre>
<h3 id="logging"><a class="header" href="#logging">Logging</a></h3>
<ul>
<li><strong>Structured logs</strong>: JSON format with correlation IDs</li>
<li><strong>Log levels</strong>: Configurable verbosity (debug, info, warn, error)</li>
<li><strong>Request tracing</strong>: Context propagation across gRPC calls</li>
</ul>
<h3 id="health-checks-1"><a class="header" href="#health-checks-1">Health Checks</a></h3>
<ul>
<li><strong>Kubernetes probes</strong>: Liveness and readiness probes</li>
<li><strong>gRPC health protocol</strong>: Standard health check implementation</li>
<li><strong>Hypervisor connectivity</strong>: Validates connection to external systems</li>
</ul>
<h2 id="deployment-patterns"><a class="header" href="#deployment-patterns">Deployment Patterns</a></h2>
<h3 id="single-provider-setup"><a class="header" href="#single-provider-setup">Single Provider Setup</a></h3>
<pre><code class="language-yaml"># Simple development setup
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: dev-vsphere
spec:
  type: vsphere
  endpoint: "https://vcenter-dev.example.com:443"
  credentialSecretRef:
    name: dev-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0"
</code></pre>
<h3 id="high-availability-setup-1"><a class="header" href="#high-availability-setup-1">High Availability Setup</a></h3>
<pre><code class="language-yaml"># Production HA setup
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: prod-vsphere
spec:
  type: vsphere
  endpoint: "https://vcenter-prod.example.com:443"
  credentialSecretRef:
    name: prod-credentials
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0"
    replicas: 3
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: prod-vsphere
          topologyKey: kubernetes.io/hostname
</code></pre>
<h3 id="multi-environment-setup"><a class="header" href="#multi-environment-setup">Multi-Environment Setup</a></h3>
<pre><code class="language-yaml"># Development environment
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: dev-libvirt
  namespace: development
spec:
  type: libvirt
  endpoint: "qemu+ssh://dev@libvirt-dev.example.com/system"
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.0"
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"

---
# Production environment  
apiVersion: infra.virtrigaud.io/v1beta1
kind: Provider
metadata:
  name: prod-libvirt
  namespace: production
spec:
  type: libvirt
  endpoint: "qemu+ssh://prod@libvirt-prod.example.com/system"
  runtime:
    mode: Remote
    image: "ghcr.io/projectbeskar/virtrigaud/provider-libvirt:v0.2.0"
    replicas: 2
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "2"
        memory: "2Gi"
</code></pre>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<ul>
<li><strong>Horizontal scaling</strong>: Multiple provider replicas per hypervisor</li>
<li><strong>Resource isolation</strong>: Independent resource allocation per provider</li>
<li><strong>Load distribution</strong>: gRPC load balancing across provider instances</li>
</ul>
<h3 id="security-1"><a class="header" href="#security-1">Security</a></h3>
<ul>
<li><strong>Credential isolation</strong>: Hypervisor credentials isolated to provider pods</li>
<li><strong>Network segmentation</strong>: Providers can run in separate namespaces</li>
<li><strong>Least privilege</strong>: Manager runs without direct hypervisor access</li>
</ul>
<h3 id="reliability"><a class="header" href="#reliability">Reliability</a></h3>
<ul>
<li><strong>Fault isolation</strong>: Provider failures don‚Äôt affect the manager</li>
<li><strong>Independent updates</strong>: Provider images updated separately</li>
<li><strong>Circuit breaking</strong>: Automatic failure detection and recovery</li>
</ul>
<h3 id="operational-excellence"><a class="header" href="#operational-excellence">Operational Excellence</a></h3>
<ul>
<li><strong>Rolling updates</strong>: Zero-downtime provider updates</li>
<li><strong>Health monitoring</strong>: Built-in health checks and metrics</li>
<li><strong>Debugging</strong>: Isolated provider logs and observability</li>
</ul>
<h2 id="troubleshooting-11"><a class="header" href="#troubleshooting-11">Troubleshooting</a></h2>
<h3 id="common-issues-7"><a class="header" href="#common-issues-7">Common Issues</a></h3>
<ol>
<li>
<p><strong>Image Pull Failures</strong></p>
<pre><code class="language-bash"># Check image availability
docker pull ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0

# Verify imagePullSecrets if using private registry
kubectl get secret regcred -o yaml
</code></pre>
</li>
<li>
<p><strong>Network Connectivity</strong></p>
<pre><code class="language-bash"># Test provider service
kubectl get svc virtrigaud-provider-*

# Check provider pod logs
kubectl logs -l app.kubernetes.io/name=virtrigaud-provider
</code></pre>
</li>
<li>
<p><strong>Credential Issues</strong></p>
<pre><code class="language-bash"># Verify secret exists and is mounted
kubectl get secret vsphere-credentials
kubectl describe pod virtrigaud-provider-*
</code></pre>
</li>
</ol>
<h3 id="debugging-commands-1"><a class="header" href="#debugging-commands-1">Debugging Commands</a></h3>
<pre><code class="language-bash"># Check provider status
kubectl describe provider vsphere-datacenter

# Check provider deployment
kubectl get deployment -l app.kubernetes.io/instance=vsphere-datacenter

# Check provider pods
kubectl get pods -l app.kubernetes.io/instance=vsphere-datacenter

# View provider logs
kubectl logs -l app.kubernetes.io/instance=vsphere-datacenter -f

# Check provider metrics
kubectl port-forward svc/virtrigaud-provider-vsphere-datacenter 8080:8080
curl http://localhost:8080/metrics
</code></pre>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<pre><code class="language-yaml"># Optimize for high-volume workloads
spec:
  rateLimit:
    qps: 100        # Increase API rate limit
    burst: 200      # Allow burst capacity
  runtime:
    replicas: 5     # Scale out for throughput
    resources:
      requests:
        cpu: "1"    # Guarantee CPU resources
        memory: "1Gi"
      limits:
        cpu: "4"    # Allow burst CPU
        memory: "4Gi"
</code></pre>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<h3 id="resource-management-1"><a class="header" href="#resource-management-1">Resource Management</a></h3>
<ul>
<li><strong>Right-sizing</strong>: Start with small requests, monitor and adjust</li>
<li><strong>Limits</strong>: Always set memory limits to prevent OOM kills</li>
<li><strong>QoS</strong>: Use Guaranteed QoS for production workloads</li>
</ul>
<h3 id="security-2"><a class="header" href="#security-2">Security</a></h3>
<ul>
<li><strong>Secrets rotation</strong>: Implement regular credential rotation</li>
<li><strong>Network policies</strong>: Restrict provider-to-hypervisor traffic</li>
<li><strong>RBAC</strong>: Use dedicated service accounts per provider</li>
</ul>
<h3 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h3>
<ul>
<li><strong>Alerting</strong>: Set up alerts on provider health metrics</li>
<li><strong>Dashboards</strong>: Create Grafana dashboards for provider metrics</li>
<li><strong>Log aggregation</strong>: Centralize logs for debugging and auditing</li>
</ul>
<h2 id="migration-and-upgrades"><a class="header" href="#migration-and-upgrades">Migration and Upgrades</a></h2>
<h3 id="provider-image-updates"><a class="header" href="#provider-image-updates">Provider Image Updates</a></h3>
<pre><code class="language-bash"># Update provider image
kubectl patch provider vsphere-datacenter -p '
{
  "spec": {
    "runtime": {
      "image": "ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.0"
    }
  }
}'

# Monitor rollout
kubectl rollout status deployment virtrigaud-provider-vsphere-datacenter
</code></pre>
<h3 id="configuration-changes"><a class="header" href="#configuration-changes">Configuration Changes</a></h3>
<pre><code class="language-bash"># Update provider configuration
kubectl edit provider vsphere-datacenter

# Verify changes applied
kubectl describe provider vsphere-datacenter
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtrigaud-observability-guide"><a class="header" href="#virtrigaud-observability-guide">VirtRigaud Observability Guide</a></h1>
<p>This document describes the comprehensive observability features of VirtRigaud, including structured logging, metrics, tracing, and monitoring.</p>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>VirtRigaud provides production-grade observability through:</p>
<ul>
<li><strong>Structured JSON Logging</strong> with correlation IDs and automatic secret redaction</li>
<li><strong>Comprehensive Prometheus Metrics</strong> for all components and operations</li>
<li><strong>OpenTelemetry Tracing</strong> with gRPC instrumentation</li>
<li><strong>Health Endpoints</strong> for liveness and readiness probes</li>
<li><strong>Grafana Dashboards</strong> for visualization</li>
<li><strong>Prometheus Alerts</strong> for proactive monitoring</li>
</ul>
<h2 id="logging-1"><a class="header" href="#logging-1">Logging</a></h2>
<h3 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h3>
<p>Configure logging via environment variables:</p>
<pre><code class="language-bash">LOG_LEVEL=info              # debug, info, warn, error
LOG_FORMAT=json             # json or console
LOG_SAMPLING=true           # Enable log sampling
LOG_DEVELOPMENT=false       # Development mode
</code></pre>
<h3 id="correlation-ids"><a class="header" href="#correlation-ids">Correlation IDs</a></h3>
<p>All log entries include correlation fields:</p>
<pre><code class="language-json">{
  "level": "info",
  "ts": "2025-01-27T10:30:45.123Z",
  "msg": "VM operation started",
  "correlationID": "req-12345",
  "vm": "default/web-server-1",
  "provider": "default/vsphere-prod",
  "providerType": "vsphere",
  "taskRef": "task-67890",
  "reconcile": "uuid-abcdef"
}
</code></pre>
<h3 id="secret-redaction"><a class="header" href="#secret-redaction">Secret Redaction</a></h3>
<p>Sensitive information is automatically redacted:</p>
<pre><code class="language-json">{
  "msg": "Connecting to provider",
  "endpoint": "vcenter://user:[REDACTED]@vc.example.com/Datacenter",
  "userData": "[REDACTED]"
}
</code></pre>
<h2 id="metrics-catalog"><a class="header" href="#metrics-catalog">Metrics Catalog</a></h2>
<h3 id="manager-metrics"><a class="header" href="#manager-metrics">Manager Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Type</th><th>Description</th><th>Labels</th></tr></thead><tbody>
<tr><td><code>virtrigaud_manager_reconcile_total</code></td><td>Counter</td><td>Total reconcile operations</td><td><code>kind</code>, <code>outcome</code></td></tr>
<tr><td><code>virtrigaud_manager_reconcile_duration_seconds</code></td><td>Histogram</td><td>Reconcile duration</td><td><code>kind</code></td></tr>
<tr><td><code>virtrigaud_queue_depth</code></td><td>Gauge</td><td>Work queue depth</td><td><code>kind</code></td></tr>
</tbody></table>
</div>
<h3 id="provider-metrics"><a class="header" href="#provider-metrics">Provider Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Type</th><th>Description</th><th>Labels</th></tr></thead><tbody>
<tr><td><code>virtrigaud_provider_rpc_requests_total</code></td><td>Counter</td><td>RPC requests</td><td><code>provider_type</code>, <code>method</code>, <code>code</code></td></tr>
<tr><td><code>virtrigaud_provider_rpc_latency_seconds</code></td><td>Histogram</td><td>RPC latency</td><td><code>provider_type</code>, <code>method</code></td></tr>
<tr><td><code>virtrigaud_provider_tasks_inflight</code></td><td>Gauge</td><td>Inflight tasks</td><td><code>provider_type</code>, <code>provider</code></td></tr>
</tbody></table>
</div>
<h3 id="vm-operation-metrics"><a class="header" href="#vm-operation-metrics">VM Operation Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Type</th><th>Description</th><th>Labels</th></tr></thead><tbody>
<tr><td><code>virtrigaud_vm_operations_total</code></td><td>Counter</td><td>VM operations</td><td><code>operation</code>, <code>provider_type</code>, <code>provider</code>, <code>outcome</code></td></tr>
<tr><td><code>virtrigaud_ip_discovery_duration_seconds</code></td><td>Histogram</td><td>IP discovery time</td><td><code>provider_type</code></td></tr>
</tbody></table>
</div>
<h3 id="circuit-breaker-metrics"><a class="header" href="#circuit-breaker-metrics">Circuit Breaker Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Type</th><th>Description</th><th>Labels</th></tr></thead><tbody>
<tr><td><code>virtrigaud_circuit_breaker_state</code></td><td>Gauge</td><td>CB state (0=closed, 1=half-open, 2=open)</td><td><code>provider_type</code>, <code>provider</code></td></tr>
<tr><td><code>virtrigaud_circuit_breaker_failures_total</code></td><td>Counter</td><td>CB failures</td><td><code>provider_type</code>, <code>provider</code></td></tr>
</tbody></table>
</div>
<h3 id="error-metrics"><a class="header" href="#error-metrics">Error Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Type</th><th>Description</th><th>Labels</th></tr></thead><tbody>
<tr><td><code>virtrigaud_errors_total</code></td><td>Counter</td><td>Errors by reason</td><td><code>reason</code>, <code>component</code></td></tr>
</tbody></table>
</div>
<h2 id="tracing"><a class="header" href="#tracing">Tracing</a></h2>
<h3 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h3>
<p>Enable OpenTelemetry tracing:</p>
<pre><code class="language-bash">VIRTRIGAUD_TRACING_ENABLED=true
VIRTRIGAUD_TRACING_ENDPOINT=http://jaeger:14268/api/traces
VIRTRIGAUD_TRACING_SAMPLING_RATIO=0.1
VIRTRIGAUD_TRACING_INSECURE=true
</code></pre>
<h3 id="span-structure"><a class="header" href="#span-structure">Span Structure</a></h3>
<p>Key spans include:</p>
<ul>
<li><code>vm.reconcile</code> - Full VM reconciliation</li>
<li><code>vm.create</code> - VM creation operation</li>
<li><code>provider.validate</code> - Provider validation</li>
<li><code>rpc.Create</code> - gRPC calls to providers</li>
</ul>
<h3 id="trace-attributes"><a class="header" href="#trace-attributes">Trace Attributes</a></h3>
<p>Standard attributes:</p>
<pre><code>vm.namespace = "default"
vm.name = "web-server-1"
provider.type = "vsphere"
operation = "Create"
task.ref = "task-12345"
</code></pre>
<h2 id="health-endpoints"><a class="header" href="#health-endpoints">Health Endpoints</a></h2>
<h3 id="http-endpoints"><a class="header" href="#http-endpoints">HTTP Endpoints</a></h3>
<p>All components expose health endpoints on port 8080:</p>
<ul>
<li><code>GET /healthz</code> - Liveness probe (always returns 200)</li>
<li><code>GET /readyz</code> - Readiness probe (checks dependencies)</li>
<li><code>GET /health</code> - Detailed health status (JSON)</li>
</ul>
<h3 id="grpc-health"><a class="header" href="#grpc-health">gRPC Health</a></h3>
<p>Providers implement <code>grpc.health.v1.Health</code> service for health checks.</p>
<h2 id="grafana-dashboards"><a class="header" href="#grafana-dashboards">Grafana Dashboards</a></h2>
<h3 id="manager-dashboard"><a class="header" href="#manager-dashboard">Manager Dashboard</a></h3>
<ul>
<li>Reconcile rates and duration</li>
<li>Queue depth monitoring</li>
<li>Error rate tracking</li>
<li>Resource usage (CPU/memory)</li>
</ul>
<h3 id="provider-dashboard"><a class="header" href="#provider-dashboard">Provider Dashboard</a></h3>
<ul>
<li>RPC latency and error rates</li>
<li>Task monitoring</li>
<li>Circuit breaker status</li>
<li>Provider-specific metrics</li>
</ul>
<h3 id="vm-lifecycle-dashboard"><a class="header" href="#vm-lifecycle-dashboard">VM Lifecycle Dashboard</a></h3>
<ul>
<li>Creation success rates</li>
<li>IP discovery times</li>
<li>Failure analysis</li>
<li>Provider comparison</li>
</ul>
<h2 id="prometheus-alerts"><a class="header" href="#prometheus-alerts">Prometheus Alerts</a></h2>
<h3 id="critical-alerts"><a class="header" href="#critical-alerts">Critical Alerts</a></h3>
<ul>
<li><code>VirtrigaudProviderDown</code> - Provider unavailable</li>
<li><code>VirtrigaudManagerDown</code> - Manager unavailable</li>
</ul>
<h3 id="warning-alerts"><a class="header" href="#warning-alerts">Warning Alerts</a></h3>
<ul>
<li><code>VirtrigaudProviderErrorRateHigh</code> - High error rate (&gt;50%)</li>
<li><code>VirtrigaudReconcileStuck</code> - Slow reconciles (&gt;5min)</li>
<li><code>VirtrigaudQueueBackedUp</code> - Queue depth &gt;100</li>
<li><code>VirtrigaudCircuitBreakerOpen</code> - CB protection active</li>
</ul>
<h2 id="configuration-reference"><a class="header" href="#configuration-reference">Configuration Reference</a></h2>
<h3 id="complete-environment-variables"><a class="header" href="#complete-environment-variables">Complete Environment Variables</a></h3>
<pre><code class="language-bash"># Logging
LOG_LEVEL=info
LOG_FORMAT=json
LOG_SAMPLING=true
LOG_DEVELOPMENT=false

# Tracing
VIRTRIGAUD_TRACING_ENABLED=false
VIRTRIGAUD_TRACING_ENDPOINT=""
VIRTRIGAUD_TRACING_SAMPLING_RATIO=0.1
VIRTRIGAUD_TRACING_INSECURE=true

# RPC Timeouts
RPC_TIMEOUT_DESCRIBE=30s
RPC_TIMEOUT_MUTATING=4m
RPC_TIMEOUT_VALIDATE=10s
RPC_TIMEOUT_TASK_STATUS=10s

# Retry Configuration
RETRY_MAX_ATTEMPTS=5
RETRY_BASE_DELAY=500ms
RETRY_MAX_DELAY=30s
RETRY_MULTIPLIER=2.0
RETRY_JITTER=true

# Circuit Breaker
CB_FAILURE_THRESHOLD=10
CB_RESET_SECONDS=60s
CB_HALF_OPEN_MAX_CALLS=3

# Rate Limiting
RATE_LIMIT_QPS=10
RATE_LIMIT_BURST=20

# Workers
WORKERS_PER_KIND=2
MAX_INFLIGHT_TASKS=100

# Feature Gates
FEATURE_GATES=""

# Performance
VIRTRIGAUD_PPROF_ENABLED=false
VIRTRIGAUD_PPROF_ADDR=:6060
</code></pre>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<h3 id="servicemonitor"><a class="header" href="#servicemonitor">ServiceMonitor</a></h3>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: virtrigaud-manager
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud
  endpoints:
  - port: metrics
    interval: 30s
</code></pre>
<h3 id="prometheusrule"><a class="header" href="#prometheusrule">PrometheusRule</a></h3>
<p>Deploy alerts:</p>
<pre><code class="language-bash">kubectl apply -f deploy/observability/prometheus/alerts.yaml
</code></pre>
<h3 id="grafana-dashboards-1"><a class="header" href="#grafana-dashboards-1">Grafana Dashboards</a></h3>
<p>Import dashboards from <code>deploy/observability/grafana/</code></p>
<h2 id="troubleshooting-12"><a class="header" href="#troubleshooting-12">Troubleshooting</a></h2>
<h3 id="high-error-rates"><a class="header" href="#high-error-rates">High Error Rates</a></h3>
<ol>
<li>Check provider health: <code>kubectl get providers</code></li>
<li>Review error metrics: <code>virtrigaud_errors_total</code></li>
<li>Check circuit breaker state</li>
<li>Review provider logs</li>
</ol>
<h3 id="slow-operations"><a class="header" href="#slow-operations">Slow Operations</a></h3>
<ol>
<li>Check RPC latency metrics</li>
<li>Review reconcile duration</li>
<li>Check resource constraints</li>
<li>Monitor task queue depth</li>
</ol>
<h3 id="memory-issues"><a class="header" href="#memory-issues">Memory Issues</a></h3>
<ol>
<li>Monitor <code>process_resident_memory_bytes</code></li>
<li>Check for goroutine leaks: <code>go_goroutines</code></li>
<li>Review heap usage: <code>go_memstats_heap_inuse_bytes</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="security-policy"><a class="header" href="#security-policy">Security Policy</a></h1>
<h2 id="supported-versions"><a class="header" href="#supported-versions">Supported Versions</a></h2>
<p>We actively support the following versions of VirtRigaud with security updates:</p>
<div class="table-wrapper"><table><thead><tr><th>Version</th><th>Supported</th></tr></thead><tbody>
<tr><td>0.1.x</td><td>:white_check_mark:</td></tr>
<tr><td>&lt; 0.1</td><td>:x:</td></tr>
</tbody></table>
</div>
<h2 id="reporting-a-vulnerability"><a class="header" href="#reporting-a-vulnerability">Reporting a Vulnerability</a></h2>
<p>The VirtRigaud team takes security vulnerabilities seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.</p>
<h3 id="how-to-report"><a class="header" href="#how-to-report">How to Report</a></h3>
<p><strong>Please do not report security vulnerabilities through public GitHub issues.</strong></p>
<p>Instead, please send an email to security@virtrigaud.io with the following information:</p>
<ul>
<li>A description of the vulnerability</li>
<li>Steps to reproduce the issue</li>
<li>Potential impact</li>
<li>Any possible mitigations you‚Äôve identified</li>
</ul>
<p>You should receive a response within 48 hours. If for some reason you do not, please follow up via email to ensure we received your original message.</p>
<h3 id="what-to-expect"><a class="header" href="#what-to-expect">What to Expect</a></h3>
<ul>
<li><strong>Acknowledgment</strong>: We will acknowledge receipt of your vulnerability report within 48 hours.</li>
<li><strong>Assessment</strong>: We will assess the vulnerability and determine its severity within 5 business days.</li>
<li><strong>Mitigation</strong>: For confirmed vulnerabilities, we will work on a fix and coordinate disclosure timeline with you.</li>
<li><strong>Recognition</strong>: We will credit you in our security advisory and release notes (unless you prefer to remain anonymous).</li>
</ul>
<h3 id="disclosure-policy"><a class="header" href="#disclosure-policy">Disclosure Policy</a></h3>
<ul>
<li>We ask that you do not publicly disclose the vulnerability until we have had a chance to address it.</li>
<li>We will coordinate with you on an appropriate disclosure timeline.</li>
<li>We typically aim to disclose within 90 days of initial report.</li>
</ul>
<h2 id="security-considerations-1"><a class="header" href="#security-considerations-1">Security Considerations</a></h2>
<h3 id="general-security"><a class="header" href="#general-security">General Security</a></h3>
<ul>
<li>VirtRigaud runs with minimal privileges and follows security best practices</li>
<li>All communications with providers use TLS encryption</li>
<li>Sensitive data (credentials, user data) is properly handled and never logged</li>
<li>RBAC is enforced to limit access to resources</li>
</ul>
<h3 id="supply-chain-security"><a class="header" href="#supply-chain-security">Supply Chain Security</a></h3>
<ul>
<li>All container images are signed with Cosign</li>
<li>Software Bill of Materials (SBOM) is provided for all releases</li>
<li>Container images are scanned for vulnerabilities</li>
<li>Dependencies are regularly updated</li>
</ul>
<h3 id="network-security-1"><a class="header" href="#network-security-1">Network Security</a></h3>
<ul>
<li>Network policies are provided to restrict traffic</li>
<li>mTLS is supported for provider communications</li>
<li>No unnecessary ports are exposed</li>
</ul>
<h3 id="access-control"><a class="header" href="#access-control">Access Control</a></h3>
<ul>
<li>RBAC roles follow principle of least privilege</li>
<li>Service accounts are properly scoped</li>
<li>Admission webhooks enforce security policies</li>
</ul>
<h2 id="vulnerability-management"><a class="header" href="#vulnerability-management">Vulnerability Management</a></h2>
<h3 id="scanning"><a class="header" href="#scanning">Scanning</a></h3>
<p>We regularly scan our codebase and dependencies for known vulnerabilities using:</p>
<ul>
<li>GitHub Security Advisories</li>
<li>Trivy for container scanning</li>
<li>Go vulnerability database</li>
<li>OWASP dependency checking</li>
</ul>
<h3 id="response-process"><a class="header" href="#response-process">Response Process</a></h3>
<ol>
<li><strong>Detection</strong>: Vulnerability discovered through scanning or reporting</li>
<li><strong>Assessment</strong>: Determine severity and impact</li>
<li><strong>Patching</strong>: Develop and test fix</li>
<li><strong>Release</strong>: Create security release with patch</li>
<li><strong>Notification</strong>: Inform users through security advisory</li>
</ol>
<h3 id="severity-classification"><a class="header" href="#severity-classification">Severity Classification</a></h3>
<p>We use the following severity levels:</p>
<ul>
<li><strong>Critical</strong>: Immediate action required, patch within 24 hours</li>
<li><strong>High</strong>: Patch within 7 days</li>
<li><strong>Medium</strong>: Patch within 30 days</li>
<li><strong>Low</strong>: Patch in next regular release</li>
</ul>
<h2 id="security-features"><a class="header" href="#security-features">Security Features</a></h2>
<h3 id="authentication-and-authorization"><a class="header" href="#authentication-and-authorization">Authentication and Authorization</a></h3>
<ul>
<li>Integration with Kubernetes RBAC</li>
<li>Support for external identity providers</li>
<li>Service account token projection</li>
<li>Webhook authentication</li>
</ul>
<h3 id="encryption"><a class="header" href="#encryption">Encryption</a></h3>
<ul>
<li>TLS 1.2+ for all communications</li>
<li>Certificate rotation and management</li>
<li>Support for custom CA certificates</li>
<li>Secrets encryption at rest (Kubernetes level)</li>
</ul>
<h3 id="audit-and-monitoring"><a class="header" href="#audit-and-monitoring">Audit and Monitoring</a></h3>
<ul>
<li>Comprehensive audit logging</li>
<li>Security event monitoring</li>
<li>Metrics for security-relevant events</li>
<li>Integration with security monitoring tools</li>
</ul>
<h2 id="best-practices-for-users"><a class="header" href="#best-practices-for-users">Best Practices for Users</a></h2>
<h3 id="deployment-security"><a class="header" href="#deployment-security">Deployment Security</a></h3>
<ol>
<li><strong>Use namespace isolation</strong>: Deploy in dedicated namespace</li>
<li><strong>Apply network policies</strong>: Restrict network access</li>
<li><strong>Enable Pod Security Standards</strong>: Use strict or baseline profiles</li>
<li><strong>Regular updates</strong>: Keep VirtRigaud and dependencies updated</li>
<li><strong>Monitor security advisories</strong>: Subscribe to security notifications</li>
</ol>
<h3 id="credential-management"><a class="header" href="#credential-management">Credential Management</a></h3>
<ol>
<li><strong>Use external secret management</strong>: HashiCorp Vault, External Secrets Operator</li>
<li><strong>Rotate credentials regularly</strong>: Implement credential rotation</li>
<li><strong>Principle of least privilege</strong>: Grant minimal required permissions</li>
<li><strong>Secure storage</strong>: Never store credentials in Git or plain text</li>
</ol>
<h3 id="network-security-2"><a class="header" href="#network-security-2">Network Security</a></h3>
<ol>
<li><strong>Enable TLS</strong>: Use TLS for all communications</li>
<li><strong>Network segmentation</strong>: Isolate provider networks</li>
<li><strong>Firewall rules</strong>: Restrict hypervisor access</li>
<li><strong>VPN access</strong>: Use VPN for remote hypervisor access</li>
</ol>
<h3 id="monitoring-and-alerting"><a class="header" href="#monitoring-and-alerting">Monitoring and Alerting</a></h3>
<ol>
<li><strong>Security monitoring</strong>: Monitor for security events</li>
<li><strong>Failed authentication alerts</strong>: Alert on authentication failures</li>
<li><strong>Unusual activity</strong>: Monitor for unexpected behavior</li>
<li><strong>Compliance scanning</strong>: Regular security scans</li>
</ol>
<h2 id="compliance"><a class="header" href="#compliance">Compliance</a></h2>
<p>VirtRigaud is designed to support compliance with various security frameworks:</p>
<ul>
<li><strong>SOC 2</strong>: Control implementation guidance available</li>
<li><strong>ISO 27001</strong>: Security control mapping provided</li>
<li><strong>CIS Kubernetes Benchmark</strong>: Alignment with security benchmarks</li>
<li><strong>NIST Cybersecurity Framework</strong>: Control implementation guidance</li>
</ul>
<h2 id="security-tools-and-integrations"><a class="header" href="#security-tools-and-integrations">Security Tools and Integrations</a></h2>
<h3 id="supported-security-tools"><a class="header" href="#supported-security-tools">Supported Security Tools</a></h3>
<ul>
<li><strong>Falco</strong>: Runtime security monitoring</li>
<li><strong>OPA Gatekeeper</strong>: Policy enforcement</li>
<li><strong>Twistlock/Prisma</strong>: Container security scanning</li>
<li><strong>Aqua Security</strong>: Container and runtime security</li>
<li><strong>Cilium</strong>: Network security and observability</li>
</ul>
<h3 id="security-configurations"><a class="header" href="#security-configurations">Security Configurations</a></h3>
<p>Example security-hardened configurations are provided in:</p>
<ul>
<li><code>examples/security/strict-rbac.yaml</code></li>
<li><code>examples/security/network-policies.yaml</code></li>
<li><code>examples/security/pod-security-policies.yaml</code></li>
<li><code>examples/security/external-secrets.yaml</code></li>
</ul>
<h2 id="contact"><a class="header" href="#contact">Contact</a></h2>
<p>For security-related questions that are not vulnerabilities, you can:</p>
<ul>
<li>Open a GitHub Discussion in the Security category</li>
<li>Email security@virtrigaud.io</li>
<li>Join the #virtrigaud-security channel on Kubernetes Slack</li>
</ul>
<h2 id="recognition"><a class="header" href="#recognition">Recognition</a></h2>
<p>We maintain a security hall of fame for researchers who have helped improve VirtRigaud security:</p>
<ul>
<li><a href="docs/security/contributors.html">Security Contributors</a></li>
</ul>
<p>Thank you to all the security researchers who have contributed to making VirtRigaud more secure!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtrigaud-resilience-guide"><a class="header" href="#virtrigaud-resilience-guide">VirtRigaud Resilience Guide</a></h1>
<p>This document describes the resilience patterns and error handling mechanisms in VirtRigaud.</p>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>VirtRigaud implements comprehensive resilience patterns:</p>
<ul>
<li><strong>Error Taxonomy</strong> - Structured error classification</li>
<li><strong>Circuit Breakers</strong> - Protection against cascading failures</li>
<li><strong>Exponential Backoff</strong> - Intelligent retry strategies</li>
<li><strong>Timeout Policies</strong> - Prevent resource exhaustion</li>
<li><strong>Rate Limiting</strong> - Provider protection</li>
</ul>
<h2 id="error-taxonomy"><a class="header" href="#error-taxonomy">Error Taxonomy</a></h2>
<h3 id="error-types"><a class="header" href="#error-types">Error Types</a></h3>
<p>VirtRigaud classifies all errors into specific categories:</p>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Retryable</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><code>NotFound</code></td><td>No</td><td>Resource doesn‚Äôt exist</td><td>VM not found</td></tr>
<tr><td><code>InvalidSpec</code></td><td>No</td><td>Invalid configuration</td><td>Malformed VM spec</td></tr>
<tr><td><code>Unauthorized</code></td><td>No</td><td>Authentication failed</td><td>Invalid credentials</td></tr>
<tr><td><code>NotSupported</code></td><td>No</td><td>Unsupported operation</td><td>Feature not available</td></tr>
<tr><td><code>Retryable</code></td><td>Yes</td><td>Transient error</td><td>Network timeout</td></tr>
<tr><td><code>Unavailable</code></td><td>Yes</td><td>Service unavailable</td><td>Provider down</td></tr>
<tr><td><code>RateLimit</code></td><td>Yes</td><td>Rate limited</td><td>API quota exceeded</td></tr>
<tr><td><code>Timeout</code></td><td>Yes</td><td>Operation timeout</td><td>Long-running task</td></tr>
<tr><td><code>QuotaExceeded</code></td><td>No</td><td>Resource quota hit</td><td>Storage full</td></tr>
<tr><td><code>Conflict</code></td><td>No</td><td>Resource conflict</td><td>Duplicate name</td></tr>
</tbody></table>
</div>
<h3 id="error-creation"><a class="header" href="#error-creation">Error Creation</a></h3>
<pre><code class="language-go">import "github.com/projectbeskar/virtrigaud/internal/providers/contracts"

// Create specific error types
err := contracts.NewNotFoundError("VM not found", originalErr)
err := contracts.NewRetryableError("Network timeout", originalErr)
err := contracts.NewUnavailableError("Provider unavailable", originalErr)

// Check if error is retryable
if providerErr, ok := err.(*contracts.ProviderError); ok {
    if providerErr.IsRetryable() {
        // Retry the operation
    }
}
</code></pre>
<h2 id="circuit-breaker-pattern"><a class="header" href="#circuit-breaker-pattern">Circuit Breaker Pattern</a></h2>
<h3 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h3>
<pre><code class="language-go">import "github.com/projectbeskar/virtrigaud/internal/resilience"

config := &amp;resilience.Config{
    FailureThreshold: 10,              // Open after 10 failures
    ResetTimeout:     60 * time.Second, // Try again after 60s
    HalfOpenMaxCalls: 3,               // Allow 3 test calls
}

cb := resilience.NewCircuitBreaker("provider-vsphere", "vsphere", "prod", config)
</code></pre>
<h3 id="usage"><a class="header" href="#usage">Usage</a></h3>
<pre><code class="language-go">err := cb.Call(ctx, func(ctx context.Context) error {
    // Call the potentially failing operation
    return provider.Create(ctx, request)
})

if err != nil {
    // Handle error (may be circuit breaker protection)
    log.Error(err, "Operation failed")
}
</code></pre>
<h3 id="states"><a class="header" href="#states">States</a></h3>
<ol>
<li><strong>Closed</strong> - Normal operation, failures are counted</li>
<li><strong>Open</strong> - Fast-fail mode, requests are rejected immediately</li>
<li><strong>Half-Open</strong> - Testing mode, limited requests allowed</li>
</ol>
<h3 id="metrics-2"><a class="header" href="#metrics-2">Metrics</a></h3>
<p>Circuit breaker state is exposed via metrics:</p>
<pre><code>virtrigaud_circuit_breaker_state{provider_type="vsphere",provider="prod"} 0
virtrigaud_circuit_breaker_failures_total{provider_type="vsphere",provider="prod"} 5
</code></pre>
<h2 id="retry-strategies"><a class="header" href="#retry-strategies">Retry Strategies</a></h2>
<h3 id="exponential-backoff"><a class="header" href="#exponential-backoff">Exponential Backoff</a></h3>
<pre><code class="language-go">import "github.com/projectbeskar/virtrigaud/internal/resilience"

config := &amp;resilience.RetryConfig{
    MaxAttempts: 5,
    BaseDelay:   500 * time.Millisecond,
    MaxDelay:    30 * time.Second,
    Multiplier:  2.0,
    Jitter:      true,
}

err := resilience.Retry(ctx, config, func(ctx context.Context, attempt int) error {
    return provider.Describe(ctx, vmID)
})
</code></pre>
<h3 id="backoff-calculation"><a class="header" href="#backoff-calculation">Backoff Calculation</a></h3>
<p>For attempt <code>n</code>:</p>
<pre><code>delay = BaseDelay √ó Multiplier^n
delay = min(delay, MaxDelay)
if Jitter:
    delay += random(0, delay * 0.1)
</code></pre>
<p>Example delays with <code>BaseDelay=500ms</code>, <code>Multiplier=2.0</code>:</p>
<ul>
<li>Attempt 0: 500ms</li>
<li>Attempt 1: 1s</li>
<li>Attempt 2: 2s</li>
<li>Attempt 3: 4s</li>
<li>Attempt 4: 8s</li>
</ul>
<h3 id="predefined-configurations"><a class="header" href="#predefined-configurations">Predefined Configurations</a></h3>
<pre><code class="language-go">// For frequent, low-latency operations
aggressive := resilience.AggressiveRetryConfig()
// MaxAttempts: 10, BaseDelay: 100ms, Multiplier: 1.5

// For expensive operations
conservative := resilience.ConservativeRetryConfig()
// MaxAttempts: 3, BaseDelay: 1s, Multiplier: 3.0

// Disable retries
none := resilience.NoRetryConfig()
// MaxAttempts: 1
</code></pre>
<h2 id="combined-resilience-policies"><a class="header" href="#combined-resilience-policies">Combined Resilience Policies</a></h2>
<h3 id="policy-builder"><a class="header" href="#policy-builder">Policy Builder</a></h3>
<pre><code class="language-go">policy := resilience.NewPolicyBuilder("vm-operations").
    WithRetry(resilience.DefaultRetryConfig()).
    WithCircuitBreaker(circuitBreaker).
    Build()

err := policy.Execute(ctx, func(ctx context.Context) error {
    return provider.Create(ctx, request)
})
</code></pre>
<h3 id="integration-example"><a class="header" href="#integration-example">Integration Example</a></h3>
<pre><code class="language-go">// In VirtualMachine controller
func (r *VirtualMachineReconciler) createVM(ctx context.Context, vm *v1beta1.VirtualMachine) error {
    // Get circuit breaker for this provider
    cb := r.CircuitBreakerRegistry.GetOrCreate(
        "vm-operations", 
        provider.Spec.Type, 
        provider.Name,
    )
    
    // Create resilience policy
    policy := resilience.NewPolicyBuilder("create-vm").
        WithRetry(&amp;resilience.RetryConfig{
            MaxAttempts: 3,
            BaseDelay:   1 * time.Second,
            MaxDelay:    30 * time.Second,
            Multiplier:  2.0,
            Jitter:      true,
        }).
        WithCircuitBreaker(cb).
        Build()
    
    // Execute with resilience
    return policy.Execute(ctx, func(ctx context.Context) error {
        resp, err := provider.Create(ctx, createReq)
        if err != nil {
            return err
        }
        
        vm.Status.ID = resp.ID
        vm.Status.TaskRef = resp.TaskRef
        return nil
    })
}
</code></pre>
<h2 id="timeout-policies"><a class="header" href="#timeout-policies">Timeout Policies</a></h2>
<h3 id="rpc-timeouts"><a class="header" href="#rpc-timeouts">RPC Timeouts</a></h3>
<p>Different operations have different timeout requirements:</p>
<pre><code class="language-go">// Operation-specific timeouts
config := &amp;config.RPCConfig{
    TimeoutDescribe:   30 * time.Second,  // Quick status check
    TimeoutMutating:   4 * time.Minute,   // Create/Delete/Power
    TimeoutValidate:   10 * time.Second,  // Provider validation
    TimeoutTaskStatus: 10 * time.Second,  // Task polling
}

// Usage in gRPC client
timeout := config.GetRPCTimeout("Create")
ctx, cancel := context.WithTimeout(ctx, timeout)
defer cancel()

resp, err := client.Create(ctx, request)
</code></pre>
<h3 id="context-propagation"><a class="header" href="#context-propagation">Context Propagation</a></h3>
<p>Always respect context deadlines:</p>
<pre><code class="language-go">func (p *Provider) Create(ctx context.Context, req CreateRequest) error {
    // Check if context is already cancelled
    select {
    case &lt;-ctx.Done():
        return ctx.Err()
    default:
    }
    
    // Perform operation with context
    return p.performCreate(ctx, req)
}
</code></pre>
<h2 id="rate-limiting"><a class="header" href="#rate-limiting">Rate Limiting</a></h2>
<h3 id="provider-protection"><a class="header" href="#provider-protection">Provider Protection</a></h3>
<pre><code class="language-go">import "golang.org/x/time/rate"

// Configure rate limiter
limiter := rate.NewLimiter(
    rate.Limit(config.RateLimit.QPS),    // 10 requests per second
    config.RateLimit.Burst,              // Allow bursts of 20
)

// Check rate limit before operation
if !limiter.Allow() {
    return contracts.NewRateLimitError("Rate limit exceeded", nil)
}

// Proceed with operation
return provider.Create(ctx, request)
</code></pre>
<h3 id="per-provider-limits"><a class="header" href="#per-provider-limits">Per-Provider Limits</a></h3>
<p>Each provider instance has its own rate limiter:</p>
<pre><code class="language-go">type ProviderManager struct {
    limiters map[string]*rate.Limiter
}

func (pm *ProviderManager) getLimiter(providerType, provider string) *rate.Limiter {
    key := fmt.Sprintf("%s:%s", providerType, provider)
    if limiter, exists := pm.limiters[key]; exists {
        return limiter
    }
    
    // Create new limiter
    limiter := rate.NewLimiter(rate.Limit(10), 20)
    pm.limiters[key] = limiter
    return limiter
}
</code></pre>
<h2 id="condition-mapping"><a class="header" href="#condition-mapping">Condition Mapping</a></h2>
<h3 id="vm-conditions"><a class="header" href="#vm-conditions">VM Conditions</a></h3>
<p>VirtRigaud sets standard conditions based on operations:</p>
<div class="table-wrapper"><table><thead><tr><th>Condition</th><th>Status</th><th>Reason</th><th>Description</th></tr></thead><tbody>
<tr><td><code>Ready</code></td><td>True</td><td><code>VMReady</code></td><td>VM is ready for use</td></tr>
<tr><td><code>Ready</code></td><td>False</td><td><code>ProviderError</code></td><td>Provider operation failed</td></tr>
<tr><td><code>Ready</code></td><td>False</td><td><code>ValidationError</code></td><td>Spec validation failed</td></tr>
<tr><td><code>Provisioning</code></td><td>True</td><td><code>Creating</code></td><td>VM creation in progress</td></tr>
<tr><td><code>Provisioning</code></td><td>False</td><td><code>CreateFailed</code></td><td>VM creation failed</td></tr>
</tbody></table>
</div>
<h3 id="provider-conditions"><a class="header" href="#provider-conditions">Provider Conditions</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Condition</th><th>Status</th><th>Reason</th><th>Description</th></tr></thead><tbody>
<tr><td><code>ProviderRuntimeReady</code></td><td>True</td><td><code>DeploymentReady</code></td><td>Remote runtime ready</td></tr>
<tr><td><code>ProviderRuntimeReady</code></td><td>False</td><td><code>DeploymentError</code></td><td>Deployment failed</td></tr>
<tr><td><code>ProviderAvailable</code></td><td>True</td><td><code>HealthCheckPassed</code></td><td>Provider healthy</td></tr>
<tr><td><code>ProviderAvailable</code></td><td>False</td><td><code>HealthCheckFailed</code></td><td>Provider unhealthy</td></tr>
</tbody></table>
</div>
<h3 id="error-to-condition-mapping"><a class="header" href="#error-to-condition-mapping">Error to Condition Mapping</a></h3>
<pre><code class="language-go">func mapErrorToCondition(err error) metav1.Condition {
    if providerErr, ok := err.(*contracts.ProviderError); ok {
        switch providerErr.Type {
        case contracts.ErrorTypeNotFound:
            return metav1.Condition{
                Type:    "Ready",
                Status:  metav1.ConditionFalse,
                Reason:  "ResourceNotFound",
                Message: providerErr.Message,
            }
        case contracts.ErrorTypeUnauthorized:
            return metav1.Condition{
                Type:    "Ready", 
                Status:  metav1.ConditionFalse,
                Reason:  "AuthenticationFailed",
                Message: providerErr.Message,
            }
        case contracts.ErrorTypeUnavailable:
            return metav1.Condition{
                Type:    "Ready",
                Status:  metav1.ConditionFalse,
                Reason:  "ProviderUnavailable", 
                Message: providerErr.Message,
            }
        }
    }
    
    // Default error condition
    return metav1.Condition{
        Type:    "Ready",
        Status:  metav1.ConditionFalse,
        Reason:  "InternalError",
        Message: err.Error(),
    }
}
</code></pre>
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h3>
<ol>
<li><strong>Always classify errors</strong> - Use appropriate error types</li>
<li><strong>Preserve context</strong> - Wrap errors with additional context</li>
<li><strong>Avoid retrying non-retryable errors</strong> - Check error type first</li>
<li><strong>Set meaningful conditions</strong> - Help users understand state</li>
</ol>
<h3 id="circuit-breakers"><a class="header" href="#circuit-breakers">Circuit Breakers</a></h3>
<ol>
<li><strong>Per-provider instances</strong> - Isolate failures</li>
<li><strong>Appropriate thresholds</strong> - Balance protection vs availability</li>
<li><strong>Monitor state changes</strong> - Alert on circuit breaker trips</li>
<li><strong>Manual override</strong> - Provide way to reset if needed</li>
</ol>
<h3 id="timeouts"><a class="header" href="#timeouts">Timeouts</a></h3>
<ol>
<li><strong>Operation-appropriate</strong> - Different timeouts for different ops</li>
<li><strong>Propagate context</strong> - Always pass context through</li>
<li><strong>Handle cancellation</strong> - Check context.Done() regularly</li>
<li><strong>Resource cleanup</strong> - Ensure resources are freed on timeout</li>
</ol>
<h3 id="rate-limiting-1"><a class="header" href="#rate-limiting-1">Rate Limiting</a></h3>
<ol>
<li><strong>Provider protection</strong> - Prevent overwhelming providers</li>
<li><strong>Burst handling</strong> - Allow reasonable bursts</li>
<li><strong>Back-pressure</strong> - Surface rate limits to users</li>
<li><strong>Fair sharing</strong> - Consider tenant isolation</li>
</ol>
<h2 id="configuration-examples"><a class="header" href="#configuration-examples">Configuration Examples</a></h2>
<h3 id="development-environment"><a class="header" href="#development-environment">Development Environment</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: virtrigaud-config
data:
  # Relaxed timeouts for development
  RPC_TIMEOUT_MUTATING: "10m"
  
  # Aggressive retries for flaky dev environments  
  RETRY_MAX_ATTEMPTS: "10"
  RETRY_BASE_DELAY: "100ms"
  
  # Lower circuit breaker threshold
  CB_FAILURE_THRESHOLD: "5"
  CB_RESET_SECONDS: "30s"
</code></pre>
<h3 id="production-environment"><a class="header" href="#production-environment">Production Environment</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: virtrigaud-config
data:
  # Strict timeouts
  RPC_TIMEOUT_MUTATING: "4m"
  RPC_TIMEOUT_DESCRIBE: "30s"
  
  # Conservative retries
  RETRY_MAX_ATTEMPTS: "3"
  RETRY_BASE_DELAY: "1s"
  RETRY_MAX_DELAY: "60s"
  
  # Higher circuit breaker threshold
  CB_FAILURE_THRESHOLD: "15" 
  CB_RESET_SECONDS: "120s"
  
  # Rate limiting
  RATE_LIMIT_QPS: "20"
  RATE_LIMIT_BURST: "50"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtrigaud-upgrade-guide"><a class="header" href="#virtrigaud-upgrade-guide">VirtRigaud Upgrade Guide</a></h1>
<p>This guide covers upgrading VirtRigaud installations, including CRD updates and breaking changes.</p>
<h2 id="quick-upgrade"><a class="header" href="#quick-upgrade">Quick Upgrade</a></h2>
<h3 id="helm-based-upgrade-recommended"><a class="header" href="#helm-based-upgrade-recommended">Helm-based Upgrade (Recommended)</a></h3>
<pre><code class="language-bash"># 1. Update Helm repository
helm repo update

# 2. Check for breaking changes
helm diff upgrade virtrigaud virtrigaud/virtrigaud --version v0.2.1

# 3. Upgrade CRDs first (required for schema changes)
helm pull virtrigaud/virtrigaud --version v0.2.1 --untar
kubectl apply -f virtrigaud/crds/

# 4. Upgrade VirtRigaud
helm upgrade virtrigaud virtrigaud/virtrigaud \
  --namespace virtrigaud-system \
  --version v0.2.1
</code></pre>
<h3 id="alternative-direct-crd-download"><a class="header" href="#alternative-direct-crd-download">Alternative: Direct CRD Download</a></h3>
<pre><code class="language-bash"># Download and apply CRDs from release
curl -L "https://github.com/projectbeskar/virtrigaud/releases/download/v0.2.1/virtrigaud-crds.yaml" | kubectl apply -f -

# Upgrade application
helm upgrade virtrigaud virtrigaud/virtrigaud --version v0.2.1
</code></pre>
<h2 id="version-specific-upgrade-notes"><a class="header" href="#version-specific-upgrade-notes">Version-Specific Upgrade Notes</a></h2>
<h3 id="v020--v021"><a class="header" href="#v020--v021">v0.2.0 ‚Üí v0.2.1</a></h3>
<p><strong>Breaking Changes:</strong></p>
<ul>
<li>‚úÖ PowerState validation fixed (OffGraceful now supported)</li>
<li>‚úÖ Hardware version management added (vSphere only)</li>
<li>‚úÖ Disk size configuration respected</li>
</ul>
<p><strong>Required Actions:</strong></p>
<ol>
<li><strong>CRD Update Required</strong>: New powerState validation and schema changes</li>
<li><strong>Provider Image Update</strong>: Ensure providers use v0.2.1+ images for new features</li>
<li><strong>Field Testing</strong>: Verify OffGraceful, hardware version, and disk sizing work correctly</li>
</ol>
<p><strong>Upgrade Steps:</strong></p>
<pre><code class="language-bash"># 1. Backup existing resources
kubectl get virtualmachines,vmclasses,providers -A -o yaml &gt; virtrigaud-backup-v021.yaml

# 2. Update CRDs (fixes OffGraceful validation)
kubectl apply -f https://github.com/projectbeskar/virtrigaud/releases/download/v0.2.1/virtrigaud-crds.yaml

# 3. Upgrade VirtRigaud
helm upgrade virtrigaud virtrigaud/virtrigaud --version v0.2.1

# 4. Verify OffGraceful works
kubectl patch virtualmachine &lt;vm-name&gt; --type='merge' -p='{"spec":{"powerState":"OffGraceful"}}'
</code></pre>
<h2 id="rollback-procedures"><a class="header" href="#rollback-procedures">Rollback Procedures</a></h2>
<h3 id="rollback-to-previous-version"><a class="header" href="#rollback-to-previous-version">Rollback to Previous Version</a></h3>
<pre><code class="language-bash"># 1. Rollback application
helm rollback virtrigaud &lt;revision&gt;

# 2. Rollback CRDs (if schema breaking changes)
kubectl apply -f https://github.com/projectbeskar/virtrigaud/releases/download/v0.2.0/virtrigaud-crds.yaml

# 3. Verify resources still work
kubectl get virtualmachines -A
</code></pre>
<h3 id="emergency-recovery"><a class="header" href="#emergency-recovery">Emergency Recovery</a></h3>
<pre><code class="language-bash"># 1. Restore from backup
kubectl apply -f virtrigaud-backup-v021.yaml

# 2. Check controller logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-manager

# 3. Force reconciliation
kubectl annotate virtualmachine &lt;vm-name&gt; virtrigaud.io/force-sync="$(date)"
</code></pre>
<h2 id="automated-upgrade-with-gitops"><a class="header" href="#automated-upgrade-with-gitops">Automated Upgrade with GitOps</a></h2>
<h3 id="argocd-2"><a class="header" href="#argocd-2">ArgoCD</a></h3>
<pre><code class="language-yaml">apiVersion: argoproj.io/v1beta1
kind: Application
metadata:
  name: virtrigaud
spec:
  source:
    chart: virtrigaud
    repoURL: https://projectbeskar.github.io/virtrigaud
    targetRevision: "0.2.1"
    helm:
      parameters:
      - name: manager.image.tag
        value: "v0.2.1"
  syncPolicy:
    syncOptions:
    - CreateNamespace=true
    - Replace=true  # Required for CRD updates
</code></pre>
<h3 id="flux-2"><a class="header" href="#flux-2">Flux</a></h3>
<pre><code class="language-yaml">apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: virtrigaud
spec:
  chart:
    spec:
      chart: virtrigaud
      version: "0.2.1"
      sourceRef:
        kind: HelmRepository
        name: virtrigaud
  upgrade:
    crds: CreateReplace  # Ensure CRDs are updated
</code></pre>
<h2 id="troubleshooting-upgrades"><a class="header" href="#troubleshooting-upgrades">Troubleshooting Upgrades</a></h2>
<h3 id="crd-validation-errors"><a class="header" href="#crd-validation-errors">CRD Validation Errors</a></h3>
<pre><code class="language-bash"># Check CRD status
kubectl get crd virtualmachines.infra.virtrigaud.io -o yaml

# Fix validation conflicts
kubectl patch crd virtualmachines.infra.virtrigaud.io --type='json' -p='[{"op": "remove", "path": "/spec/versions/0/schema/openAPIV3Schema/properties/spec/properties/powerState/allOf"}]'
</code></pre>
<h3 id="provider-image-mismatch"><a class="header" href="#provider-image-mismatch">Provider Image Mismatch</a></h3>
<pre><code class="language-bash"># Check provider images
kubectl get providers -o jsonpath='{.items[*].spec.runtime.image}'

# Update provider image
kubectl patch provider &lt;provider-name&gt; --type='merge' -p='{"spec":{"runtime":{"image":"ghcr.io/projectbeskar/virtrigaud/provider-vsphere:v0.2.1"}}}'
</code></pre>
<h3 id="resource-conflicts"><a class="header" href="#resource-conflicts">Resource Conflicts</a></h3>
<pre><code class="language-bash"># Check for resource conflicts
kubectl get events --sort-by=.metadata.creationTimestamp

# Force resource refresh
kubectl delete pod -l app.kubernetes.io/name=virtrigaud -n virtrigaud-system
</code></pre>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<h3 id="pre-upgrade-checklist"><a class="header" href="#pre-upgrade-checklist">Pre-Upgrade Checklist</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
Backup all VirtRigaud resources</li>
<li><input disabled="" type="checkbox"/>
Check for breaking changes in release notes</li>
<li><input disabled="" type="checkbox"/>
Test upgrade in staging environment</li>
<li><input disabled="" type="checkbox"/>
Verify provider connectivity</li>
<li><input disabled="" type="checkbox"/>
Plan rollback strategy</li>
</ul>
<h3 id="post-upgrade-verification"><a class="header" href="#post-upgrade-verification">Post-Upgrade Verification</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
All CRDs updated successfully</li>
<li><input disabled="" type="checkbox"/>
Controller manager running</li>
<li><input disabled="" type="checkbox"/>
Providers healthy and responsive</li>
<li><input disabled="" type="checkbox"/>
Existing VMs still manageable</li>
<li><input disabled="" type="checkbox"/>
New features working (OffGraceful, hardware version, etc.)</li>
</ul>
<h3 id="monitoring-during-upgrade"><a class="header" href="#monitoring-during-upgrade">Monitoring During Upgrade</a></h3>
<pre><code class="language-bash"># Watch controller logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-manager -f

# Monitor VM status
kubectl get virtualmachines -A --watch

# Check provider health
kubectl get providers -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[0].type,MESSAGE:.status.conditions[0].message
</code></pre>
<h2 id="support-and-recovery"><a class="header" href="#support-and-recovery">Support and Recovery</a></h2>
<p>If you encounter issues during upgrade:</p>
<ol>
<li><strong>Check Release Notes</strong>: https://github.com/projectbeskar/virtrigaud/releases</li>
<li><strong>Review Logs</strong>: Controller and provider logs for error details</li>
<li><strong>Community Support</strong>: GitHub issues and discussions</li>
<li><strong>Emergency Rollback</strong>: Use documented rollback procedures</li>
</ol>
<p>Remember: Always test upgrades in non-production environments first!</p>
<h2 id="development-workflow-v021"><a class="header" href="#development-workflow-v021">Development Workflow (v0.2.1+)</a></h2>
<h3 id="automated-crd-synchronization"><a class="header" href="#automated-crd-synchronization">Automated CRD Synchronization</a></h3>
<p>Starting with v0.2.1, VirtRigaud includes automated tooling to ensure CRDs stay in sync between development and Helm chart deployments.</p>
<h4 id="for-developers"><a class="header" href="#for-developers">For Developers</a></h4>
<pre><code class="language-bash"># Generate and sync CRDs automatically
make sync-helm-crds

# Verify CRDs are in sync
make verify-helm-crds

# Package Helm chart with latest CRDs
make helm-package
</code></pre>
<h4 id="pre-commit-hooks"><a class="header" href="#pre-commit-hooks">Pre-commit Hooks</a></h4>
<p>Install pre-commit hooks to automatically sync CRDs:</p>
<pre><code class="language-bash"># Install pre-commit
pip install pre-commit

# Install hooks
pre-commit install

# CRDs will now sync automatically on commits that modify:
# - api/**.go files
# - config/crd/**.yaml files
</code></pre>
<h4 id="cicd-integration"><a class="header" href="#cicd-integration">CI/CD Integration</a></h4>
<p>The CI/CD pipeline now automatically:</p>
<ol>
<li><strong>Validates CRD sync</strong> on every pull request</li>
<li><strong>Syncs CRDs</strong> before Helm chart packaging in releases</li>
<li><strong>Fails builds</strong> if Helm chart CRDs are out of sync</li>
</ol>
<p>This prevents the v0.2.1-rc2 issue where OffGraceful validation failed due to stale Helm chart CRDs.</p>
<h3 id="repository-workflow"><a class="header" href="#repository-workflow">Repository Workflow</a></h3>
<pre><code class="language-bash"># 1. Make API changes
vim api/infra.virtrigaud.io/v1beta1/virtualmachine_types.go

# 2. Generate and sync CRDs (automated by pre-commit)
make sync-helm-crds

# 3. Commit (hooks will verify sync)
git add .
git commit -m "feat: add new VM power states"

# 4. CI validates everything is in sync
git push origin feature-branch
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vsphere-hardware-version-management"><a class="header" href="#vsphere-hardware-version-management">vSphere Hardware Version Management</a></h1>
<p>This document describes how to configure and upgrade VM hardware compatibility versions in VMware vSphere environments using virtrigaud.</p>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>VMware vSphere virtual machines have a hardware compatibility version (also called virtual hardware version) that determines which features and capabilities are available to the VM. Higher hardware versions provide access to newer features but require compatible ESXi hosts.</p>
<p><strong>Note: Hardware version management is specific to VMware vSphere and is not available for other providers (LibVirt, Proxmox, etc.).</strong></p>
<h2 id="hardware-version-numbers"><a class="header" href="#hardware-version-numbers">Hardware Version Numbers</a></h2>
<p>Common hardware versions and their corresponding VMware products:</p>
<div class="table-wrapper"><table><thead><tr><th>Hardware Version</th><th>vSphere/ESXi Version</th><th>Key Features</th></tr></thead><tbody>
<tr><td>10</td><td>ESXi 5.5</td><td>Legacy baseline</td></tr>
<tr><td>11</td><td>ESXi 6.0</td><td>Enhanced graphics, larger VM memory</td></tr>
<tr><td>13</td><td>ESXi 6.5</td><td>Enhanced security, more CPU/memory</td></tr>
<tr><td>14</td><td>ESXi 6.7</td><td>Persistent memory, enhanced security</td></tr>
<tr><td>15</td><td>ESXi 6.7 U2</td><td>Enhanced graphics, more vCPU</td></tr>
<tr><td>17</td><td>ESXi 7.0</td><td>TPM 2.0, enhanced security</td></tr>
<tr><td>18</td><td>ESXi 7.0 U1</td><td>Enhanced networking</td></tr>
<tr><td>19</td><td>ESXi 7.0 U2</td><td>Precision time protocol</td></tr>
<tr><td>20</td><td>ESXi 7.0 U3</td><td>Enhanced graphics, more memory</td></tr>
<tr><td>21</td><td>ESXi 8.0</td><td>Latest features, DPU support</td></tr>
</tbody></table>
</div>
<h2 id="setting-hardware-version-during-vm-creation"><a class="header" href="#setting-hardware-version-during-vm-creation">Setting Hardware Version During VM Creation</a></h2>
<p>Configure the hardware version in the VMClass using the <code>extraConfig</code> field:</p>
<pre><code class="language-yaml">apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: modern-vm-class
  namespace: virtrigaud-system
spec:
  cpu: 4
  memory: 8Gi
  firmware: UEFI
  
  # vSphere-specific hardware version configuration
  extraConfig:
    vsphere.hardwareVersion: "21"  # Use latest hardware version
  
  diskDefaults:
    type: thin
    sizeGiB: 50
---
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: modern-vm
  namespace: default
spec:
  providerRef:
    name: vsphere-provider
    namespace: virtrigaud-system
  
  classRef:
    name: modern-vm-class  # Uses hardware version 21
    namespace: virtrigaud-system
  
  imageRef:
    name: ubuntu-22-04
    namespace: virtrigaud-system
</code></pre>
<h2 id="upgrading-hardware-version-for-existing-vms"><a class="header" href="#upgrading-hardware-version-for-existing-vms">Upgrading Hardware Version for Existing VMs</a></h2>
<p>You can upgrade the hardware version of existing VMs using the dedicated hardware upgrade API:</p>
<h3 id="using-kubectl-with-raw-grpc"><a class="header" href="#using-kubectl-with-raw-grpc">Using kubectl with Raw gRPC</a></h3>
<pre><code class="language-bash"># First, ensure the VM is powered off
kubectl patch vm my-vm --type='merge' -p='{"spec":{"powerState":"Off"}}'

# Wait for VM to be powered off, then upgrade hardware version
# Note: This requires direct access to the provider gRPC endpoint
# A kubectl plugin or controller extension would be needed for this operation
</code></pre>
<h3 id="programmatic-upgrade-example-go-code"><a class="header" href="#programmatic-upgrade-example-go-code">Programmatic Upgrade (Example Go Code)</a></h3>
<pre><code class="language-go">package main

import (
    "context"
    "fmt"
    "log"
    
    providerv1 "github.com/projectbeskar/virtrigaud/proto/rpc/provider/v1"
    "google.golang.org/grpc"
)

func upgradeVMHardwareVersion(vmID string, targetVersion int32) error {
    // Connect to vSphere provider
    conn, err := grpc.Dial("vsphere-provider:9090", grpc.WithInsecure())
    if err != nil {
        return fmt.Errorf("failed to connect: %w", err)
    }
    defer conn.Close()
    
    client := providerv1.NewProviderClient(conn)
    
    // Upgrade hardware version
    req := &amp;providerv1.HardwareUpgradeRequest{
        Id:            vmID,
        TargetVersion: targetVersion,
    }
    
    resp, err := client.HardwareUpgrade(context.Background(), req)
    if err != nil {
        return fmt.Errorf("hardware upgrade failed: %w", err)
    }
    
    log.Printf("Hardware upgrade completed: %+v", resp)
    return nil
}
</code></pre>
<h2 id="requirements-and-limitations"><a class="header" href="#requirements-and-limitations">Requirements and Limitations</a></h2>
<h3 id="prerequisites-7"><a class="header" href="#prerequisites-7">Prerequisites</a></h3>
<ol>
<li><strong>VM Must Be Powered Off</strong>: Hardware version upgrades require the VM to be completely powered off</li>
<li><strong>ESXi Host Compatibility</strong>: Target hardware version must be supported by the ESXi host</li>
<li><strong>VMware Tools</strong>: For best results, ensure VMware Tools is installed and up-to-date</li>
<li><strong>Backup Recommended</strong>: Take a snapshot before upgrading hardware version</li>
</ol>
<h3 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h3>
<ol>
<li><strong>One-Way Operation</strong>: Hardware version upgrades cannot be downgraded</li>
<li><strong>vSphere Only</strong>: This feature is not available for LibVirt, Proxmox, or other providers</li>
<li><strong>Host Requirements</strong>: Upgrading to newer versions may prevent VM from running on older ESXi hosts</li>
<li><strong>Compatibility</strong>: Some older guest operating systems may not support newer hardware versions</li>
</ol>
<h2 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h2>
<h3 id="choosing-hardware-version"><a class="header" href="#choosing-hardware-version">Choosing Hardware Version</a></h3>
<ol>
<li><strong>Match ESXi Version</strong>: Use the hardware version that matches your ESXi environment</li>
<li><strong>Conservative Approach</strong>: Don‚Äôt always use the latest version unless you need specific features</li>
<li><strong>Test First</strong>: Test hardware version upgrades in development before production</li>
</ol>
<h3 id="upgrade-process"><a class="header" href="#upgrade-process">Upgrade Process</a></h3>
<ol>
<li><strong>Plan Maintenance Window</strong>: VMs must be powered off during upgrade</li>
<li><strong>Backup First</strong>: Always take a snapshot before upgrading</li>
<li><strong>Batch Operations</strong>: Group VMs by hardware requirements for efficient upgrades</li>
<li><strong>Verify Compatibility</strong>: Ensure all ESXi hosts in your cluster support the target version</li>
</ol>
<h3 id="example-vmclass-configurations"><a class="header" href="#example-vmclass-configurations">Example VMClass Configurations</a></h3>
<h4 id="legacy-environment-esxi-65"><a class="header" href="#legacy-environment-esxi-65">Legacy Environment (ESXi 6.5)</a></h4>
<pre><code class="language-yaml">extraConfig:
  vsphere.hardwareVersion: "13"
</code></pre>
<h4 id="modern-environment-esxi-70"><a class="header" href="#modern-environment-esxi-70">Modern Environment (ESXi 7.0)</a></h4>
<pre><code class="language-yaml">extraConfig:
  vsphere.hardwareVersion: "17"
</code></pre>
<h4 id="latest-features-esxi-80"><a class="header" href="#latest-features-esxi-80">Latest Features (ESXi 8.0)</a></h4>
<pre><code class="language-yaml">extraConfig:
  vsphere.hardwareVersion: "21"
</code></pre>
<h2 id="troubleshooting-13"><a class="header" href="#troubleshooting-13">Troubleshooting</a></h2>
<h3 id="common-issues-8"><a class="header" href="#common-issues-8">Common Issues</a></h3>
<ol>
<li>
<p><strong>VM Not Powered Off</strong></p>
<pre><code>Error: VM must be powered off for hardware upgrade, current state: poweredOn
</code></pre>
<p><strong>Solution</strong>: Power off the VM first using <code>powerState: Off</code></p>
</li>
<li>
<p><strong>Unsupported Hardware Version</strong></p>
<pre><code>Error: target version vmx-21 is not supported by ESXi host
</code></pre>
<p><strong>Solution</strong>: Check ESXi host compatibility and use a supported version</p>
</li>
<li>
<p><strong>Version Not Newer</strong></p>
<pre><code>Error: target version vmx-15 is not newer than current version vmx-17
</code></pre>
<p><strong>Solution</strong>: Hardware versions can only be upgraded, not downgraded</p>
</li>
</ol>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>After upgrading, verify the hardware version:</p>
<pre><code class="language-bash"># Check VM configuration in vSphere
kubectl get vm my-vm -o jsonpath='{.status.provider}'
</code></pre>
<h2 id="integration-examples"><a class="header" href="#integration-examples">Integration Examples</a></h2>
<h3 id="complete-vm-lifecycle-with-hardware-version"><a class="header" href="#complete-vm-lifecycle-with-hardware-version">Complete VM Lifecycle with Hardware Version</a></h3>
<pre><code class="language-yaml"># 1. Create VMClass with specific hardware version
apiVersion: infra.virtrigaud.io/v1beta1
kind: VMClass
metadata:
  name: production-vm-class
spec:
  cpu: 8
  memory: 16Gi
  firmware: UEFI
  extraConfig:
    vsphere.hardwareVersion: "19"  # ESXi 7.0 U2 compatible

---
# 2. Create VM using the class
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: production-vm
spec:
  powerState: On
  providerRef:
    name: vsphere-provider
    namespace: virtrigaud-system
  classRef:
    name: production-vm-class
    namespace: virtrigaud-system
  imageRef:
    name: ubuntu-22-04
    namespace: virtrigaud-system

---
# 3. Update to newer hardware version (requires separate upgrade operation)
# This would typically be done through a controller or manual gRPC call
# after powering off the VM
</code></pre>
<p>This vSphere-specific feature provides fine-grained control over VM hardware capabilities while maintaining compatibility with your ESXi infrastructure.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bearer-token-authentication"><a class="header" href="#bearer-token-authentication">Bearer Token Authentication</a></h1>
<p>This guide covers how to configure bearer token authentication for VirtRigaud providers using JWT tokens and RBAC.</p>
<h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Bearer token authentication provides a stateless, scalable authentication mechanism using JSON Web Tokens (JWT). This approach is suitable for:</p>
<ul>
<li><strong>Multi-tenant environments</strong>: Different tokens for different tenants</li>
<li><strong>API-based access</strong>: External systems accessing provider services</li>
<li><strong>Short-lived sessions</strong>: Tokens with configurable expiration</li>
<li><strong>Fine-grained permissions</strong>: Token-based RBAC</li>
</ul>
<h2 id="jwt-token-structure"><a class="header" href="#jwt-token-structure">JWT Token Structure</a></h2>
<h3 id="token-claims"><a class="header" href="#token-claims">Token Claims</a></h3>
<pre><code class="language-json">{
  "iss": "virtrigaud-manager",
  "sub": "provider-client",
  "aud": "virtrigaud-provider",
  "exp": 1640995200,
  "iat": 1640908800,
  "nbf": 1640908800,
  "scope": "vm:create vm:read vm:update vm:delete",
  "tenant": "default",
  "provider": "vsphere",
  "jti": "unique-token-id"
}
</code></pre>
<h3 id="scopes-definition"><a class="header" href="#scopes-definition">Scopes Definition</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Scope</th><th>Description</th></tr></thead><tbody>
<tr><td><code>vm:create</code></td><td>Create virtual machines</td></tr>
<tr><td><code>vm:read</code></td><td>Read virtual machine information</td></tr>
<tr><td><code>vm:update</code></td><td>Update virtual machine configuration</td></tr>
<tr><td><code>vm:delete</code></td><td>Delete virtual machines</td></tr>
<tr><td><code>vm:power</code></td><td>Control virtual machine power state</td></tr>
<tr><td><code>vm:snapshot</code></td><td>Create and manage snapshots</td></tr>
<tr><td><code>vm:clone</code></td><td>Clone virtual machines</td></tr>
<tr><td><code>admin</code></td><td>Full administrative access</td></tr>
</tbody></table>
</div>
<h2 id="token-generation"><a class="header" href="#token-generation">Token Generation</a></h2>
<h3 id="jwt-signing-key"><a class="header" href="#jwt-signing-key">JWT Signing Key</a></h3>
<pre><code class="language-bash"># Generate RS256 private key
openssl genrsa -out jwt-private-key.pem 2048

# Extract public key
openssl rsa -in jwt-private-key.pem -pubout -out jwt-public-key.pem

# Store as Kubernetes secret
kubectl create secret generic jwt-keys \
  --from-file=private-key=jwt-private-key.pem \
  --from-file=public-key=jwt-public-key.pem \
  --namespace=virtrigaud-system
</code></pre>
<h3 id="token-generation-service"><a class="header" href="#token-generation-service">Token Generation Service</a></h3>
<pre><code class="language-go">package auth

import (
    "crypto/rsa"
    "time"
    
    "github.com/golang-jwt/jwt/v4"
)

type TokenClaims struct {
    Issuer    string   `json:"iss"`
    Subject   string   `json:"sub"`
    Audience  string   `json:"aud"`
    ExpiresAt int64    `json:"exp"`
    IssuedAt  int64    `json:"iat"`
    NotBefore int64    `json:"nbf"`
    Scope     string   `json:"scope"`
    Tenant    string   `json:"tenant"`
    Provider  string   `json:"provider"`
    ID        string   `json:"jti"`
    jwt.RegisteredClaims
}

type TokenService struct {
    privateKey *rsa.PrivateKey
    publicKey  *rsa.PublicKey
    issuer     string
}

func NewTokenService(privateKey *rsa.PrivateKey, publicKey *rsa.PublicKey, issuer string) *TokenService {
    return &amp;TokenService{
        privateKey: privateKey,
        publicKey:  publicKey,
        issuer:     issuer,
    }
}

func (ts *TokenService) GenerateToken(subject, tenant, provider string, scopes []string, duration time.Duration) (string, error) {
    now := time.Now()
    claims := &amp;TokenClaims{
        Issuer:    ts.issuer,
        Subject:   subject,
        Audience:  "virtrigaud-provider",
        ExpiresAt: now.Add(duration).Unix(),
        IssuedAt:  now.Unix(),
        NotBefore: now.Unix(),
        Scope:     strings.Join(scopes, " "),
        Tenant:    tenant,
        Provider:  provider,
        ID:        generateJTI(),
    }
    
    token := jwt.NewWithClaims(jwt.SigningMethodRS256, claims)
    return token.SignedString(ts.privateKey)
}

func (ts *TokenService) ValidateToken(tokenString string) (*TokenClaims, error) {
    token, err := jwt.ParseWithClaims(tokenString, &amp;TokenClaims{}, func(token *jwt.Token) (interface{}, error) {
        if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok {
            return nil, fmt.Errorf("unexpected signing method: %v", token.Header["alg"])
        }
        return ts.publicKey, nil
    })
    
    if err != nil {
        return nil, err
    }
    
    if claims, ok := token.Claims.(*TokenClaims); ok &amp;&amp; token.Valid {
        return claims, nil
    }
    
    return nil, fmt.Errorf("invalid token")
}

func generateJTI() string {
    return uuid.New().String()
}
</code></pre>
<h2 id="provider-authentication-interceptor"><a class="header" href="#provider-authentication-interceptor">Provider Authentication Interceptor</a></h2>
<h3 id="grpc-interceptor"><a class="header" href="#grpc-interceptor">gRPC Interceptor</a></h3>
<pre><code class="language-go">package middleware

import (
    "context"
    "strings"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/codes"
    "google.golang.org/grpc/metadata"
    "google.golang.org/grpc/status"
)

type AuthInterceptor struct {
    tokenService *auth.TokenService
    rbac         *RBACManager
}

func NewAuthInterceptor(tokenService *auth.TokenService, rbac *RBACManager) *AuthInterceptor {
    return &amp;AuthInterceptor{
        tokenService: tokenService,
        rbac:         rbac,
    }
}

func (ai *AuthInterceptor) Unary() grpc.UnaryServerInterceptor {
    return func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        // Skip authentication for health checks
        if strings.HasSuffix(info.FullMethod, "/Health/Check") {
            return handler(ctx, req)
        }
        
        token, err := ai.extractToken(ctx)
        if err != nil {
            return nil, status.Errorf(codes.Unauthenticated, "missing or invalid token: %v", err)
        }
        
        claims, err := ai.tokenService.ValidateToken(token)
        if err != nil {
            return nil, status.Errorf(codes.Unauthenticated, "invalid token: %v", err)
        }
        
        // Check authorization
        if !ai.rbac.IsAuthorized(claims, info.FullMethod) {
            return nil, status.Errorf(codes.PermissionDenied, "insufficient permissions")
        }
        
        // Add claims to context
        ctx = context.WithValue(ctx, "claims", claims)
        
        return handler(ctx, req)
    }
}

func (ai *AuthInterceptor) extractToken(ctx context.Context) (string, error) {
    md, ok := metadata.FromIncomingContext(ctx)
    if !ok {
        return "", fmt.Errorf("missing metadata")
    }
    
    authHeaders := md.Get("authorization")
    if len(authHeaders) == 0 {
        return "", fmt.Errorf("missing authorization header")
    }
    
    authHeader := authHeaders[0]
    if !strings.HasPrefix(authHeader, "Bearer ") {
        return "", fmt.Errorf("invalid authorization header format")
    }
    
    return strings.TrimPrefix(authHeader, "Bearer "), nil
}
</code></pre>
<h3 id="rbac-manager"><a class="header" href="#rbac-manager">RBAC Manager</a></h3>
<pre><code class="language-go">package middleware

import (
    "strings"
)

type Permission struct {
    Resource string
    Action   string
}

type RBACManager struct {
    permissions map[string][]Permission
}

func NewRBACManager() *RBACManager {
    return &amp;RBACManager{
        permissions: map[string][]Permission{
            // RPC method to required permissions mapping
            "/provider.v1.ProviderService/CreateVM": {
                {Resource: "vm", Action: "create"},
            },
            "/provider.v1.ProviderService/GetVM": {
                {Resource: "vm", Action: "read"},
            },
            "/provider.v1.ProviderService/UpdateVM": {
                {Resource: "vm", Action: "update"},
            },
            "/provider.v1.ProviderService/DeleteVM": {
                {Resource: "vm", Action: "delete"},
            },
            "/provider.v1.ProviderService/PowerVM": {
                {Resource: "vm", Action: "power"},
            },
            "/provider.v1.ProviderService/CreateSnapshot": {
                {Resource: "vm", Action: "snapshot"},
            },
            "/provider.v1.ProviderService/CloneVM": {
                {Resource: "vm", Action: "clone"},
            },
        },
    }
}

func (rbac *RBACManager) IsAuthorized(claims *auth.TokenClaims, method string) bool {
    requiredPerms, exists := rbac.permissions[method]
    if !exists {
        // Allow if no specific permissions required
        return true
    }
    
    userScopes := strings.Split(claims.Scope, " ")
    
    // Check if user has admin scope
    for _, scope := range userScopes {
        if scope == "admin" {
            return true
        }
    }
    
    // Check specific permissions
    for _, requiredPerm := range requiredPerms {
        requiredScope := requiredPerm.Resource + ":" + requiredPerm.Action
        
        hasPermission := false
        for _, userScope := range userScopes {
            if userScope == requiredScope {
                hasPermission = true
                break
            }
        }
        
        if !hasPermission {
            return false
        }
    }
    
    return true
}
</code></pre>
<h2 id="kubernetes-rbac-integration"><a class="header" href="#kubernetes-rbac-integration">Kubernetes RBAC Integration</a></h2>
<h3 id="serviceaccount-and-clusterrole"><a class="header" href="#serviceaccount-and-clusterrole">ServiceAccount and ClusterRole</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: virtrigaud-token-manager
  namespace: virtrigaud-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: virtrigaud-token-manager
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: virtrigaud-token-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: virtrigaud-token-manager
subjects:
  - kind: ServiceAccount
    name: virtrigaud-token-manager
    namespace: virtrigaud-system
</code></pre>
<h3 id="token-management-configmap"><a class="header" href="#token-management-configmap">Token Management ConfigMap</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: token-config
  namespace: virtrigaud-system
data:
  config.yaml: |
    tokenService:
      issuer: "virtrigaud-manager"
      defaultDuration: "1h"
      maxDuration: "24h"
      
    scopes:
      - name: "vm:create"
        description: "Create virtual machines"
      - name: "vm:read"
        description: "Read virtual machine information"
      - name: "vm:update"
        description: "Update virtual machine configuration"
      - name: "vm:delete"
        description: "Delete virtual machines"
      - name: "vm:power"
        description: "Control virtual machine power state"
      - name: "vm:snapshot"
        description: "Create and manage snapshots"
      - name: "vm:clone"
        description: "Clone virtual machines"
      - name: "admin"
        description: "Full administrative access"
        
    tenants:
      - name: "default"
        description: "Default tenant"
        allowedScopes: ["vm:create", "vm:read", "vm:update", "vm:delete", "vm:power"]
      - name: "development"
        description: "Development environment"
        allowedScopes: ["vm:create", "vm:read", "vm:update", "vm:delete", "vm:power", "vm:snapshot", "vm:clone"]
      - name: "production"
        description: "Production environment"
        allowedScopes: ["vm:read", "vm:power"]
</code></pre>
<h2 id="client-configuration"><a class="header" href="#client-configuration">Client Configuration</a></h2>
<h3 id="manager-client-setup"><a class="header" href="#manager-client-setup">Manager Client Setup</a></h3>
<pre><code class="language-go">package client

import (
    "context"
    "time"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/metadata"
)

type AuthenticatedClient struct {
    client providerv1.ProviderServiceClient
    token  string
}

func NewAuthenticatedClient(endpoint, token string) (*AuthenticatedClient, error) {
    conn, err := grpc.Dial(endpoint, grpc.WithInsecure())
    if err != nil {
        return nil, err
    }
    
    return &amp;AuthenticatedClient{
        client: providerv1.NewProviderServiceClient(conn),
        token:  token,
    }, nil
}

func (ac *AuthenticatedClient) CreateVM(ctx context.Context, req *providerv1.CreateVMRequest) (*providerv1.CreateVMResponse, error) {
    ctx = ac.addAuthHeader(ctx)
    return ac.client.CreateVM(ctx, req)
}

func (ac *AuthenticatedClient) addAuthHeader(ctx context.Context) context.Context {
    md := metadata.Pairs("authorization", "Bearer "+ac.token)
    return metadata.NewOutgoingContext(ctx, md)
}
</code></pre>
<h3 id="token-refresh"><a class="header" href="#token-refresh">Token Refresh</a></h3>
<pre><code class="language-go">package auth

import (
    "sync"
    "time"
)

type TokenManager struct {
    tokenService *TokenService
    currentToken string
    expiresAt    time.Time
    mutex        sync.RWMutex
    
    subject  string
    tenant   string
    provider string
    scopes   []string
}

func NewTokenManager(tokenService *TokenService, subject, tenant, provider string, scopes []string) *TokenManager {
    return &amp;TokenManager{
        tokenService: tokenService,
        subject:      subject,
        tenant:       tenant,
        provider:     provider,
        scopes:       scopes,
    }
}

func (tm *TokenManager) GetToken() (string, error) {
    tm.mutex.RLock()
    if tm.currentToken != "" &amp;&amp; time.Now().Before(tm.expiresAt.Add(-5*time.Minute)) {
        token := tm.currentToken
        tm.mutex.RUnlock()
        return token, nil
    }
    tm.mutex.RUnlock()
    
    return tm.refreshToken()
}

func (tm *TokenManager) refreshToken() (string, error) {
    tm.mutex.Lock()
    defer tm.mutex.Unlock()
    
    // Double-check after acquiring write lock
    if tm.currentToken != "" &amp;&amp; time.Now().Before(tm.expiresAt.Add(-5*time.Minute)) {
        return tm.currentToken, nil
    }
    
    token, err := tm.tokenService.GenerateToken(tm.subject, tm.tenant, tm.provider, tm.scopes, time.Hour)
    if err != nil {
        return "", err
    }
    
    tm.currentToken = token
    tm.expiresAt = time.Now().Add(time.Hour)
    
    return token, nil
}
</code></pre>
<h2 id="helm-chart-integration"><a class="header" href="#helm-chart-integration">Helm Chart Integration</a></h2>
<h3 id="provider-runtime-with-bearer-token-auth"><a class="header" href="#provider-runtime-with-bearer-token-auth">Provider Runtime with Bearer Token Auth</a></h3>
<pre><code class="language-yaml"># values-bearer-auth.yaml
auth:
  type: "bearer"
  jwt:
    publicKeySecret: "jwt-keys"
    publicKeyKey: "public-key"
    issuer: "virtrigaud-manager"
    audience: "virtrigaud-provider"

# Environment variables for authentication
env:
  - name: AUTH_TYPE
    value: "bearer"
  - name: JWT_PUBLIC_KEY_PATH
    value: "/etc/jwt/public-key"
  - name: JWT_ISSUER
    value: "virtrigaud-manager"
  - name: JWT_AUDIENCE
    value: "virtrigaud-provider"

# Mount JWT public key
volumes:
  - name: jwt-public-key
    secret:
      secretName: jwt-keys

volumeMounts:
  - name: jwt-public-key
    mountPath: /etc/jwt
    readOnly: true
</code></pre>
<h2 id="monitoring-and-logging"><a class="header" href="#monitoring-and-logging">Monitoring and Logging</a></h2>
<h3 id="authentication-metrics"><a class="header" href="#authentication-metrics">Authentication Metrics</a></h3>
<pre><code class="language-go">package metrics

import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    authenticationAttempts = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "virtrigaud_authentication_attempts_total",
            Help: "Total number of authentication attempts",
        },
        []string{"method", "result", "tenant"},
    )
    
    authenticationDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "virtrigaud_authentication_duration_seconds",
            Help: "Duration of authentication operations",
        },
        []string{"method", "result"},
    )
    
    activeTokens = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "virtrigaud_active_tokens",
            Help: "Number of active tokens by tenant",
        },
        []string{"tenant", "provider"},
    )
)

func RecordAuthAttempt(method, result, tenant string) {
    authenticationAttempts.WithLabelValues(method, result, tenant).Inc()
}

func RecordAuthDuration(method, result string, duration time.Duration) {
    authenticationDuration.WithLabelValues(method, result).Observe(duration.Seconds())
}
</code></pre>
<h3 id="audit-logging"><a class="header" href="#audit-logging">Audit Logging</a></h3>
<pre><code class="language-go">package audit

import (
    "context"
    "encoding/json"
    "time"
    
    "go.uber.org/zap"
)

type AuditEvent struct {
    Timestamp time.Time `json:"timestamp"`
    EventType string    `json:"event_type"`
    Subject   string    `json:"subject"`
    Tenant    string    `json:"tenant"`
    Provider  string    `json:"provider"`
    Resource  string    `json:"resource"`
    Action    string    `json:"action"`
    Result    string    `json:"result"`
    Error     string    `json:"error,omitempty"`
    Metadata  map[string]interface{} `json:"metadata,omitempty"`
}

type AuditLogger struct {
    logger *zap.Logger
}

func NewAuditLogger(logger *zap.Logger) *AuditLogger {
    return &amp;AuditLogger{logger: logger}
}

func (al *AuditLogger) LogAuthEvent(ctx context.Context, eventType, subject, tenant, provider, result string, err error) {
    event := AuditEvent{
        Timestamp: time.Now(),
        EventType: eventType,
        Subject:   subject,
        Tenant:    tenant,
        Provider:  provider,
        Result:    result,
    }
    
    if err != nil {
        event.Error = err.Error()
    }
    
    eventJSON, _ := json.Marshal(event)
    al.logger.Info("audit_event", zap.String("event", string(eventJSON)))
}
</code></pre>
<h2 id="security-best-practices-1"><a class="header" href="#security-best-practices-1">Security Best Practices</a></h2>
<h3 id="1-token-validation"><a class="header" href="#1-token-validation">1. Token Validation</a></h3>
<pre><code class="language-go">// Always validate all token claims
func validateTokenClaims(claims *TokenClaims) error {
    now := time.Now()
    
    // Check expiration
    if claims.ExpiresAt &lt; now.Unix() {
        return fmt.Errorf("token expired")
    }
    
    // Check not before
    if claims.NotBefore &gt; now.Unix() {
        return fmt.Errorf("token not yet valid")
    }
    
    // Check issuer
    if claims.Issuer != expectedIssuer {
        return fmt.Errorf("invalid issuer")
    }
    
    // Check audience
    if claims.Audience != expectedAudience {
        return fmt.Errorf("invalid audience")
    }
    
    return nil
}
</code></pre>
<h3 id="2-rate-limiting"><a class="header" href="#2-rate-limiting">2. Rate Limiting</a></h3>
<pre><code class="language-go">// Implement rate limiting for token generation
type RateLimiter struct {
    requests map[string][]time.Time
    mutex    sync.RWMutex
    limit    int
    window   time.Duration
}

func (rl *RateLimiter) Allow(key string) bool {
    rl.mutex.Lock()
    defer rl.mutex.Unlock()
    
    now := time.Now()
    requests := rl.requests[key]
    
    // Remove old requests outside the window
    var validRequests []time.Time
    for _, req := range requests {
        if now.Sub(req) &lt; rl.window {
            validRequests = append(validRequests, req)
        }
    }
    
    // Check if we've exceeded the limit
    if len(validRequests) &gt;= rl.limit {
        return false
    }
    
    // Add the current request
    validRequests = append(validRequests, now)
    rl.requests[key] = validRequests
    
    return true
}
</code></pre>
<h3 id="3-token-blacklisting"><a class="header" href="#3-token-blacklisting">3. Token Blacklisting</a></h3>
<pre><code class="language-go">// Implement token blacklisting for revoked tokens
type TokenBlacklist struct {
    blacklistedTokens map[string]time.Time
    mutex             sync.RWMutex
}

func (tb *TokenBlacklist) IsBlacklisted(jti string) bool {
    tb.mutex.RLock()
    defer tb.mutex.RUnlock()
    
    expiresAt, exists := tb.blacklistedTokens[jti]
    if !exists {
        return false
    }
    
    // Remove expired entries
    if time.Now().After(expiresAt) {
        delete(tb.blacklistedTokens, jti)
        return false
    }
    
    return true
}

func (tb *TokenBlacklist) BlacklistToken(jti string, expiresAt time.Time) {
    tb.mutex.Lock()
    defer tb.mutex.Unlock()
    tb.blacklistedTokens[jti] = expiresAt
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mtls-security-configuration"><a class="header" href="#mtls-security-configuration">mTLS Security Configuration</a></h1>
<p>This guide covers how to configure mutual TLS (mTLS) authentication between VirtRigaud managers and providers.</p>
<h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>mTLS provides strong authentication and encryption for gRPC communication between the VirtRigaud manager and provider services. It ensures:</p>
<ul>
<li><strong>Authentication</strong>: Both client and server verify each other‚Äôs certificates</li>
<li><strong>Encryption</strong>: All traffic is encrypted in transit</li>
<li><strong>Certificate Pinning</strong>: Specific certificate authorities are trusted</li>
<li><strong>Certificate Rotation</strong>: Automated certificate renewal</li>
</ul>
<h2 id="certificate-management"><a class="header" href="#certificate-management">Certificate Management</a></h2>
<h3 id="1-generate-ca-certificate"><a class="header" href="#1-generate-ca-certificate">1. Generate CA Certificate</a></h3>
<pre><code class="language-bash"># Create CA private key
openssl genrsa -out ca-key.pem 4096

# Create CA certificate
openssl req -new -x509 -key ca-key.pem -out ca-cert.pem -days 365 \
  -subj "/C=US/ST=CA/L=San Francisco/O=VirtRigaud/CN=VirtRigaud CA"
</code></pre>
<h3 id="2-generate-server-certificate-provider"><a class="header" href="#2-generate-server-certificate-provider">2. Generate Server Certificate (Provider)</a></h3>
<pre><code class="language-bash"># Create server private key
openssl genrsa -out server-key.pem 4096

# Create server certificate signing request
openssl req -new -key server-key.pem -out server-csr.pem \
  -subj "/C=US/ST=CA/L=San Francisco/O=VirtRigaud/CN=provider-service"

# Sign server certificate
openssl x509 -req -in server-csr.pem -CA ca-cert.pem -CAkey ca-key.pem \
  -CAcreateserial -out server-cert.pem -days 365 \
  -extensions v3_req -extfile &lt;(cat &lt;&lt;EOF
[v3_req]
keyUsage = keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names
[alt_names]
DNS.1 = provider-service
DNS.2 = provider-service.default.svc.cluster.local
DNS.3 = localhost
IP.1 = 127.0.0.1
EOF
)
</code></pre>
<h3 id="3-generate-client-certificate-manager"><a class="header" href="#3-generate-client-certificate-manager">3. Generate Client Certificate (Manager)</a></h3>
<pre><code class="language-bash"># Create client private key
openssl genrsa -out client-key.pem 4096

# Create client certificate signing request
openssl req -new -key client-key.pem -out client-csr.pem \
  -subj "/C=US/ST=CA/L=San Francisco/O=VirtRigaud/CN=manager-client"

# Sign client certificate
openssl x509 -req -in client-csr.pem -CA ca-cert.pem -CAkey ca-key.pem \
  -CAcreateserial -out client-cert.pem -days 365 \
  -extensions v3_req -extfile &lt;(cat &lt;&lt;EOF
[v3_req]
keyUsage = keyEncipherment, dataEncipherment
extendedKeyUsage = clientAuth
EOF
)
</code></pre>
<h2 id="kubernetes-secret-configuration"><a class="header" href="#kubernetes-secret-configuration">Kubernetes Secret Configuration</a></h2>
<h3 id="provider-tls-secret"><a class="header" href="#provider-tls-secret">Provider TLS Secret</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: provider-tls
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: # base64 encoded server-cert.pem
  tls.key: # base64 encoded server-key.pem
  ca.crt: # base64 encoded ca-cert.pem
</code></pre>
<h3 id="manager-tls-secret"><a class="header" href="#manager-tls-secret">Manager TLS Secret</a></h3>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: manager-tls
  namespace: virtrigaud-system
type: kubernetes.io/tls
data:
  tls.crt: # base64 encoded client-cert.pem
  tls.key: # base64 encoded client-key.pem
  ca.crt: # base64 encoded ca-cert.pem
</code></pre>
<h2 id="provider-configuration-2"><a class="header" href="#provider-configuration-2">Provider Configuration</a></h2>
<h3 id="sdk-server-configuration"><a class="header" href="#sdk-server-configuration">SDK Server Configuration</a></h3>
<pre><code class="language-go">package main

import (
    "crypto/tls"
    "crypto/x509"
    "fmt"
    "io/ioutil"
    
    "github.com/projectbeskar/virtrigaud/sdk/provider/server"
)

func main() {
    // Load certificates
    cert, err := tls.LoadX509KeyPair("/etc/tls/tls.crt", "/etc/tls/tls.key")
    if err != nil {
        panic(fmt.Sprintf("Failed to load server certificates: %v", err))
    }
    
    // Load CA certificate for client verification
    caCert, err := ioutil.ReadFile("/etc/tls/ca.crt")
    if err != nil {
        panic(fmt.Sprintf("Failed to load CA certificate: %v", err))
    }
    
    caCertPool := x509.NewCertPool()
    if !caCertPool.AppendCertsFromPEM(caCert) {
        panic("Failed to parse CA certificate")
    }
    
    // Configure TLS
    tlsConfig := &amp;tls.Config{
        Certificates: []tls.Certificate{cert},
        ClientAuth:   tls.RequireAndVerifyClientCert,
        ClientCAs:    caCertPool,
        MinVersion:   tls.VersionTLS12,
        CipherSuites: []uint16{
            tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,
            tls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,
            tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
            tls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,
        },
    }
    
    // Create server with mTLS
    srv, err := server.New(&amp;server.Config{
        Port:      9443,
        TLS:       tlsConfig,
        EnableTLS: true,
    })
    if err != nil {
        panic(fmt.Sprintf("Failed to create server: %v", err))
    }
    
    // Register your provider implementation here
    // providerv1.RegisterProviderServiceServer(srv.GRPCServer(), &amp;YourProvider{})
    
    if err := srv.Serve(); err != nil {
        panic(fmt.Sprintf("Server failed: %v", err))
    }
}
</code></pre>
<h3 id="helm-chart-values-provider-runtime"><a class="header" href="#helm-chart-values-provider-runtime">Helm Chart Values (Provider Runtime)</a></h3>
<pre><code class="language-yaml"># values-mtls.yaml
tls:
  enabled: true
  secretName: provider-tls

# Mount TLS certificates
volumes:
  - name: tls-certs
    secret:
      secretName: provider-tls

volumeMounts:
  - name: tls-certs
    mountPath: /etc/tls
    readOnly: true

# Environment variables for TLS
env:
  - name: TLS_ENABLED
    value: "true"
  - name: TLS_CERT_PATH
    value: "/etc/tls/tls.crt"
  - name: TLS_KEY_PATH
    value: "/etc/tls/tls.key"
  - name: TLS_CA_PATH
    value: "/etc/tls/ca.crt"
</code></pre>
<h2 id="manager-configuration"><a class="header" href="#manager-configuration">Manager Configuration</a></h2>
<h3 id="client-tls-configuration"><a class="header" href="#client-tls-configuration">Client TLS Configuration</a></h3>
<pre><code class="language-go">// In manager code
func createProviderClient(endpoint string) (providerv1.ProviderServiceClient, error) {
    // Load client certificates
    cert, err := tls.LoadX509KeyPair("/etc/manager-tls/tls.crt", "/etc/manager-tls/tls.key")
    if err != nil {
        return nil, fmt.Errorf("failed to load client certificates: %w", err)
    }
    
    // Load CA certificate for server verification
    caCert, err := ioutil.ReadFile("/etc/manager-tls/ca.crt")
    if err != nil {
        return nil, fmt.Errorf("failed to load CA certificate: %w", err)
    }
    
    caCertPool := x509.NewCertPool()
    if !caCertPool.AppendCertsFromPEM(caCert) {
        return nil, fmt.Errorf("failed to parse CA certificate")
    }
    
    // Configure TLS
    tlsConfig := &amp;tls.Config{
        Certificates: []tls.Certificate{cert},
        RootCAs:      caCertPool,
        ServerName:   "provider-service", // Must match server certificate CN/SAN
        MinVersion:   tls.VersionTLS12,
    }
    
    // Create gRPC connection with mTLS
    conn, err := grpc.Dial(endpoint,
        grpc.WithTransportCredentials(credentials.NewTLS(tlsConfig)),
    )
    if err != nil {
        return nil, fmt.Errorf("failed to connect: %w", err)
    }
    
    return providerv1.NewProviderServiceClient(conn), nil
}
</code></pre>
<h2 id="certificate-rotation"><a class="header" href="#certificate-rotation">Certificate Rotation</a></h2>
<h3 id="using-cert-manager"><a class="header" href="#using-cert-manager">Using cert-manager</a></h3>
<pre><code class="language-yaml"># Install cert-manager first
# kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.12.0/cert-manager.yaml

apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: virtrigaud-ca-issuer
spec:
  ca:
    secretName: virtrigaud-ca-secret

---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: provider-tls
  namespace: default
spec:
  secretName: provider-tls
  issuerRef:
    name: virtrigaud-ca-issuer
    kind: ClusterIssuer
  commonName: provider-service
  dnsNames:
    - provider-service
    - provider-service.default.svc.cluster.local
  duration: 8760h # 1 year
  renewBefore: 720h # 30 days before expiry
</code></pre>
<h3 id="manual-rotation-script"><a class="header" href="#manual-rotation-script">Manual Rotation Script</a></h3>
<pre><code class="language-bash">#!/bin/bash
# rotate-certs.sh

NAMESPACE=${1:-default}
SECRET_NAME=${2:-provider-tls}

echo "Rotating certificates for $SECRET_NAME in namespace $NAMESPACE"

# Generate new certificates (using the same process as above)
# ...

# Update Kubernetes secret
kubectl create secret tls $SECRET_NAME \
  --cert=server-cert.pem \
  --key=server-key.pem \
  --namespace=$NAMESPACE \
  --dry-run=client -o yaml | kubectl apply -f -

# Add CA certificate to the secret
kubectl patch secret $SECRET_NAME -n $NAMESPACE \
  --patch="$(cat &lt;&lt;EOF
data:
  ca.crt: $(base64 -w 0 ca-cert.pem)
EOF
)"

# Restart provider deployment to pick up new certificates
kubectl rollout restart deployment/provider-deployment -n $NAMESPACE

echo "Certificate rotation completed"
</code></pre>
<h2 id="security-best-practices-2"><a class="header" href="#security-best-practices-2">Security Best Practices</a></h2>
<h3 id="1-certificate-validation"><a class="header" href="#1-certificate-validation">1. Certificate Validation</a></h3>
<pre><code class="language-go">// Always validate certificate chains
func validateCertificate(cert *x509.Certificate, caCert *x509.Certificate) error {
    roots := x509.NewCertPool()
    roots.AddCert(caCert)
    
    opts := x509.VerifyOptions{
        Roots: roots,
        KeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth},
    }
    
    _, err := cert.Verify(opts)
    return err
}
</code></pre>
<h3 id="2-certificate-pinning"><a class="header" href="#2-certificate-pinning">2. Certificate Pinning</a></h3>
<pre><code class="language-go">// Pin specific certificate or CA
func createTLSConfigWithPinning(expectedCertFingerprint string) *tls.Config {
    return &amp;tls.Config{
        VerifyPeerCertificate: func(rawCerts [][]byte, verifiedChains [][]*x509.Certificate) error {
            if len(rawCerts) == 0 {
                return fmt.Errorf("no certificates provided")
            }
            
            cert, err := x509.ParseCertificate(rawCerts[0])
            if err != nil {
                return err
            }
            
            fingerprint := sha256.Sum256(cert.Raw)
            if hex.EncodeToString(fingerprint[:]) != expectedCertFingerprint {
                return fmt.Errorf("certificate fingerprint mismatch")
            }
            
            return nil
        },
    }
}
</code></pre>
<h3 id="3-monitoring-and-alerting"><a class="header" href="#3-monitoring-and-alerting">3. Monitoring and Alerting</a></h3>
<pre><code class="language-yaml"># Prometheus AlertManager rules
groups:
  - name: virtrigaud.certificates
    rules:
      - alert: CertificateExpiringSoon
        expr: (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 &lt; 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Certificate expiring soon"
          description: "Certificate {{ $labels.name }} expires in less than 30 days"
      
      - alert: CertificateExpired
        expr: cert_manager_certificate_expiration_timestamp_seconds &lt; time()
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Certificate expired"
          description: "Certificate {{ $labels.name }} has expired"
</code></pre>
<h2 id="troubleshooting-14"><a class="header" href="#troubleshooting-14">Troubleshooting</a></h2>
<h3 id="common-issues-9"><a class="header" href="#common-issues-9">Common Issues</a></h3>
<ol>
<li>
<p><strong>Certificate chain issues</strong></p>
<pre><code class="language-bash"># Verify certificate chain
openssl verify -CAfile ca-cert.pem server-cert.pem
</code></pre>
</li>
<li>
<p><strong>SAN mismatch</strong></p>
<pre><code class="language-bash"># Check certificate SAN entries
openssl x509 -in server-cert.pem -text -noout | grep -A1 "Subject Alternative Name"
</code></pre>
</li>
<li>
<p><strong>TLS handshake failures</strong></p>
<pre><code class="language-bash"># Test TLS connection
openssl s_client -connect provider-service:9443 -cert client-cert.pem -key client-key.pem -CAfile ca-cert.pem
</code></pre>
</li>
<li>
<p><strong>Clock skew issues</strong></p>
<pre><code class="language-bash"># Ensure time synchronization
ntpdate -s time.nist.gov
</code></pre>
</li>
</ol>
<h3 id="debug-commands"><a class="header" href="#debug-commands">Debug Commands</a></h3>
<pre><code class="language-bash"># Check certificate validity
kubectl get secret provider-tls -o yaml | grep tls.crt | base64 -d | openssl x509 -text -noout

# Monitor certificate expiration
kubectl get certificates

# Check provider logs for TLS errors
kubectl logs deployment/provider-deployment | grep -i tls
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="external-secrets-management"><a class="header" href="#external-secrets-management">External Secrets Management</a></h1>
<p>This guide covers integrating VirtRigaud providers with external secret management systems using ExternalSecrets operators and best practices for credential security.</p>
<h2 id="overview-16"><a class="header" href="#overview-16">Overview</a></h2>
<p>External secret management provides secure, centralized credential storage and automatic secret rotation. Supported systems include:</p>
<ul>
<li><strong>HashiCorp Vault</strong>: Enterprise secret management with dynamic secrets</li>
<li><strong>AWS Secrets Manager</strong>: Cloud-native secret storage with automatic rotation</li>
<li><strong>Azure Key Vault</strong>: Azure-integrated secret management</li>
<li><strong>Google Secret Manager</strong>: GCP secret storage service</li>
<li><strong>Kubernetes External Secrets</strong>: Generic external secret integration</li>
</ul>
<h2 id="external-secrets-operator-setup"><a class="header" href="#external-secrets-operator-setup">External Secrets Operator Setup</a></h2>
<h3 id="installation-1"><a class="header" href="#installation-1">Installation</a></h3>
<pre><code class="language-bash"># Install External Secrets Operator
helm repo add external-secrets https://charts.external-secrets.io
helm repo update

helm install external-secrets external-secrets/external-secrets \
  --namespace external-secrets-system \
  --create-namespace \
  --set installCRDs=true
</code></pre>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-yaml"># ServiceAccount for External Secrets Operator
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-secrets
  namespace: virtrigaud-system
  annotations:
    # For AWS IRSA (IAM Roles for Service Accounts)
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/external-secrets-role

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: external-secrets
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "update", "patch", "delete", "get", "list", "watch"]
  - apiGroups: ["external-secrets.io"]
    resources: ["*"]
    verbs: ["*"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: external-secrets
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-secrets
subjects:
  - kind: ServiceAccount
    name: external-secrets
    namespace: virtrigaud-system
</code></pre>
<h2 id="hashicorp-vault-integration"><a class="header" href="#hashicorp-vault-integration">HashiCorp Vault Integration</a></h2>
<h3 id="vault-secretstore"><a class="header" href="#vault-secretstore">Vault SecretStore</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: vault-secret-store
  namespace: virtrigaud-system
spec:
  provider:
    vault:
      server: "https://vault.example.com:8200"
      path: "secret"
      version: "v2"
      auth:
        # Use Kubernetes service account for authentication
        kubernetes:
          mountPath: "kubernetes"
          role: "virtrigaud-role"
          serviceAccountRef:
            name: "external-secrets"

---
# For multi-namespace access
apiVersion: external-secrets.io/v1beta1
kind: ClusterSecretStore
metadata:
  name: vault-cluster-store
spec:
  provider:
    vault:
      server: "https://vault.example.com:8200"
      path: "secret"
      version: "v2"
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "virtrigaud-cluster-role"
          serviceAccountRef:
            name: "external-secrets"
            namespace: "virtrigaud-system"
</code></pre>
<h3 id="vault-policy-configuration"><a class="header" href="#vault-policy-configuration">Vault Policy Configuration</a></h3>
<pre><code class="language-hcl"># Vault policy for VirtRigaud secrets
path "secret/data/virtrigaud/*" {
  capabilities = ["read"]
}

path "secret/data/providers/*" {
  capabilities = ["read"]
}

# Dynamic database credentials
path "database/creds/readonly" {
  capabilities = ["read"]
}

# PKI for TLS certificates
path "pki/issue/virtrigaud" {
  capabilities = ["create", "update"]
}
</code></pre>
<h3 id="vsphere-credentials-from-vault"><a class="header" href="#vsphere-credentials-from-vault">vSphere Credentials from Vault</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: vsphere-credentials
  namespace: vsphere-providers
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-secret-store
    kind: SecretStore
  target:
    name: vsphere-credentials
    creationPolicy: Owner
    template:
      type: Opaque
      data:
        username: "{{ .username }}"
        password: "{{ .password }}"
        server: "{{ .server }}"
        # Optional: TLS certificate
        ca.crt: "{{ .ca_cert | b64dec }}"
  data:
    - secretKey: username
      remoteRef:
        key: secret/data/providers/vsphere
        property: username
    - secretKey: password
      remoteRef:
        key: secret/data/providers/vsphere
        property: password
    - secretKey: server
      remoteRef:
        key: secret/data/providers/vsphere
        property: server
    - secretKey: ca_cert
      remoteRef:
        key: secret/data/providers/vsphere
        property: ca_cert
</code></pre>
<h2 id="aws-secrets-manager-integration"><a class="header" href="#aws-secrets-manager-integration">AWS Secrets Manager Integration</a></h2>
<h3 id="aws-secretstore-with-irsa"><a class="header" href="#aws-secretstore-with-irsa">AWS SecretStore with IRSA</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: virtrigaud-system
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-west-2
      auth:
        # Use IAM Roles for Service Accounts (IRSA)
        serviceAccount:
          name: external-secrets
          namespace: virtrigaud-system

---
# IAM Policy for the IRSA role
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "secretsmanager:GetSecretValue",
        "secretsmanager:DescribeSecret"
      ],
      "Resource": [
        "arn:aws:secretsmanager:us-west-2:ACCOUNT:secret:virtrigaud/*"
      ]
    }
  ]
}
</code></pre>
<h3 id="aws-secret-configuration"><a class="header" href="#aws-secret-configuration">AWS Secret Configuration</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: aws-provider-credentials
  namespace: provider-namespace
spec:
  refreshInterval: 15m
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: provider-credentials
    creationPolicy: Owner
  data:
    - secretKey: credentials.json
      remoteRef:
        key: "virtrigaud/provider-credentials"
        property: "credentials"
    - secretKey: api-key
      remoteRef:
        key: "virtrigaud/api-keys"
        property: "provider-api-key"
</code></pre>
<h2 id="azure-key-vault-integration"><a class="header" href="#azure-key-vault-integration">Azure Key Vault Integration</a></h2>
<h3 id="azure-secretstore"><a class="header" href="#azure-secretstore">Azure SecretStore</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: azure-key-vault
  namespace: virtrigaud-system
spec:
  provider:
    azurekv:
      vaultUrl: "https://virtrigaud-vault.vault.azure.net/"
      authType: "ManagedIdentity"
      # Or use Service Principal:
      # authType: "ServicePrincipal"
      # authSecretRef:
      #   clientId:
      #     name: azure-secret
      #     key: client-id
      #   clientSecret:
      #     name: azure-secret
      #     key: client-secret
      tenantId: "tenant-id-here"

---
# Managed Identity setup (ARM template or Terraform)
apiVersion: v1
kind: Secret
metadata:
  name: azure-config
  namespace: virtrigaud-system
type: Opaque
data:
  # Base64 encoded values
  tenant-id: dGVuYW50LWlkLWhlcmU=
  client-id: Y2xpZW50LWlkLWhlcmU=
  client-secret: Y2xpZW50LXNlY3JldC1oZXJl
</code></pre>
<h3 id="azure-key-vault-secret"><a class="header" href="#azure-key-vault-secret">Azure Key Vault Secret</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: azure-provider-secrets
  namespace: provider-namespace
spec:
  refreshInterval: 30m
  secretStoreRef:
    name: azure-key-vault
    kind: SecretStore
  target:
    name: provider-secrets
    creationPolicy: Owner
  data:
    - secretKey: subscription-id
      remoteRef:
        key: "azure-subscription-id"
    - secretKey: resource-group
      remoteRef:
        key: "azure-resource-group"
    - secretKey: client-certificate
      remoteRef:
        key: "azure-client-cert"
</code></pre>
<h2 id="google-secret-manager-integration"><a class="header" href="#google-secret-manager-integration">Google Secret Manager Integration</a></h2>
<h3 id="gcp-secretstore"><a class="header" href="#gcp-secretstore">GCP SecretStore</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: gcp-secret-manager
  namespace: virtrigaud-system
spec:
  provider:
    gcpsm:
      projectId: "your-gcp-project"
      auth:
        # Use Workload Identity
        workloadIdentity:
          clusterLocation: us-central1
          clusterName: virtrigaud-cluster
          serviceAccountRef:
            name: external-secrets
            namespace: virtrigaud-system

---
# Workload Identity binding
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-secrets
  namespace: virtrigaud-system
  annotations:
    iam.gke.io/gcp-service-account: virtrigaud-secrets@PROJECT.iam.gserviceaccount.com
</code></pre>
<h3 id="gcp-secret-configuration"><a class="header" href="#gcp-secret-configuration">GCP Secret Configuration</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: gcp-provider-secrets
  namespace: provider-namespace
spec:
  refreshInterval: 20m
  secretStoreRef:
    name: gcp-secret-manager
    kind: SecretStore
  target:
    name: gcp-provider-credentials
    creationPolicy: Owner
  data:
    - secretKey: service-account.json
      remoteRef:
        key: "virtrigaud-service-account"
        version: "latest"
    - secretKey: project-id
      remoteRef:
        key: "gcp-project-id"
        version: "latest"
</code></pre>
<h2 id="provider-specific-configurations"><a class="header" href="#provider-specific-configurations">Provider-Specific Configurations</a></h2>
<h3 id="vsphere-provider-with-dynamic-credentials"><a class="header" href="#vsphere-provider-with-dynamic-credentials">vSphere Provider with Dynamic Credentials</a></h3>
<pre><code class="language-yaml"># Vault configuration for vSphere dynamic credentials
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: vsphere-dynamic-credentials
  namespace: vsphere-providers
spec:
  refreshInterval: 15m  # Short refresh for dynamic credentials
  secretStoreRef:
    name: vault-secret-store
    kind: SecretStore
  target:
    name: vsphere-dynamic-creds
    creationPolicy: Owner
    template:
      type: Opaque
      data:
        username: "{{ .username }}"
        password: "{{ .password }}"
        server: "{{ .server }}"
        session_ttl: "{{ .lease_duration }}"
  data:
    - secretKey: username
      remoteRef:
        key: "vsphere/creds/dynamic-role"
        property: "username"
    - secretKey: password
      remoteRef:
        key: "vsphere/creds/dynamic-role"
        property: "password"
    - secretKey: server
      remoteRef:
        key: "secret/data/vsphere/static"
        property: "server"
    - secretKey: lease_duration
      remoteRef:
        key: "vsphere/creds/dynamic-role"
        property: "lease_duration"

---
# Provider deployment using dynamic credentials
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vsphere-provider
  namespace: vsphere-providers
spec:
  template:
    spec:
      containers:
        - name: provider
          env:
            - name: VSPHERE_USERNAME
              valueFrom:
                secretKeyRef:
                  name: vsphere-dynamic-creds
                  key: username
            - name: VSPHERE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: vsphere-dynamic-creds
                  key: password
            - name: VSPHERE_SERVER
              valueFrom:
                secretKeyRef:
                  name: vsphere-dynamic-creds
                  key: server
</code></pre>
<h3 id="libvirt-provider-with-ssh-keys"><a class="header" href="#libvirt-provider-with-ssh-keys">Libvirt Provider with SSH Keys</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: libvirt-ssh-keys
  namespace: libvirt-providers
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-secret-store
    kind: SecretStore
  target:
    name: libvirt-ssh-credentials
    creationPolicy: Owner
    template:
      type: kubernetes.io/ssh-auth
      data:
        ssh-privatekey: "{{ .private_key }}"
        ssh-publickey: "{{ .public_key }}"
        known_hosts: "{{ .known_hosts }}"
  data:
    - secretKey: private_key
      remoteRef:
        key: "secret/data/libvirt/ssh"
        property: "private_key"
    - secretKey: public_key
      remoteRef:
        key: "secret/data/libvirt/ssh"
        property: "public_key"
    - secretKey: known_hosts
      remoteRef:
        key: "secret/data/libvirt/ssh"
        property: "known_hosts"

---
# Mount SSH keys in provider
apiVersion: apps/v1
kind: Deployment
metadata:
  name: libvirt-provider
spec:
  template:
    spec:
      containers:
        - name: provider
          volumeMounts:
            - name: ssh-keys
              mountPath: /home/provider/.ssh
              readOnly: true
          env:
            - name: SSH_AUTH_SOCK
              value: "/tmp/ssh-agent.sock"
      volumes:
        - name: ssh-keys
          secret:
            secretName: libvirt-ssh-credentials
            defaultMode: 0600
</code></pre>
<h2 id="tls-certificate-management"><a class="header" href="#tls-certificate-management">TLS Certificate Management</a></h2>
<h3 id="automatic-tls-with-external-secrets"><a class="header" href="#automatic-tls-with-external-secrets">Automatic TLS with External Secrets</a></h3>
<pre><code class="language-yaml">apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: provider-tls-certs
  namespace: provider-namespace
spec:
  refreshInterval: 24h
  secretStoreRef:
    name: vault-secret-store
    kind: SecretStore
  target:
    name: provider-tls
    creationPolicy: Owner
    template:
      type: kubernetes.io/tls
      data:
        tls.crt: "{{ .certificate }}"
        tls.key: "{{ .private_key }}"
        ca.crt: "{{ .ca_certificate }}"
  data:
    - secretKey: certificate
      remoteRef:
        key: "pki/issue/virtrigaud"
        property: "certificate"
    - secretKey: private_key
      remoteRef:
        key: "pki/issue/virtrigaud"
        property: "private_key"
    - secretKey: ca_certificate
      remoteRef:
        key: "pki/issue/virtrigaud"
        property: "issuing_ca"

---
# Vault PKI configuration (run in Vault)
# vault write pki/roles/virtrigaud \
#   allowed_domains="virtrigaud.local,provider-service" \
#   allow_subdomains=true \
#   max_ttl="8760h" \
#   generate_lease=true
</code></pre>
<h2 id="monitoring-and-alerting-1"><a class="header" href="#monitoring-and-alerting-1">Monitoring and Alerting</a></h2>
<h3 id="externalsecret-monitoring"><a class="header" href="#externalsecret-monitoring">ExternalSecret Monitoring</a></h3>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: external-secrets-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: external-secrets
  endpoints:
    - port: metrics
      interval: 30s

---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: external-secrets-alerts
  namespace: monitoring
spec:
  groups:
    - name: external-secrets.rules
      rules:
        - alert: ExternalSecretSyncFailure
          expr: increase(external_secrets_sync_calls_error[5m]) &gt; 0
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "External secret sync failure"
            description: "ExternalSecret {{ $labels.name }} in namespace {{ $labels.namespace }} failed to sync"
        
        - alert: ExternalSecretStale
          expr: (time() - external_secrets_sync_calls_total) &gt; 3600
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "External secret not refreshed"
            description: "ExternalSecret {{ $labels.name }} has not been refreshed for over 1 hour"
</code></pre>
<h3 id="custom-monitoring"><a class="header" href="#custom-monitoring">Custom Monitoring</a></h3>
<pre><code class="language-go">package monitoring

import (
    "context"
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

var (
    secretAge = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "virtrigaud_secret_age_seconds",
            Help: "Age of provider secrets in seconds",
        },
        []string{"secret_name", "namespace", "provider"},
    )
    
    secretRotationCount = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "virtrigaud_secret_rotations_total",
            Help: "Total number of secret rotations",
        },
        []string{"secret_name", "namespace", "provider"},
    )
)

type SecretMonitor struct {
    client kubernetes.Interface
}

func (sm *SecretMonitor) MonitorSecrets(ctx context.Context) {
    ticker := time.NewTicker(60 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case &lt;-ctx.Done():
            return
        case &lt;-ticker.C:
            sm.updateSecretMetrics()
        }
    }
}

func (sm *SecretMonitor) updateSecretMetrics() {
    secrets, err := sm.client.CoreV1().Secrets("").List(context.TODO(), metav1.ListOptions{
        LabelSelector: "app.kubernetes.io/managed-by=external-secrets",
    })
    if err != nil {
        return
    }
    
    for _, secret := range secrets.Items {
        provider := secret.Labels["provider"]
        if provider == "" {
            continue
        }
        
        age := time.Since(secret.CreationTimestamp.Time).Seconds()
        secretAge.WithLabelValues(secret.Name, secret.Namespace, provider).Set(age)
    }
}
</code></pre>
<h2 id="security-best-practices-3"><a class="header" href="#security-best-practices-3">Security Best Practices</a></h2>
<h3 id="1-least-privilege-access"><a class="header" href="#1-least-privilege-access">1. Least Privilege Access</a></h3>
<pre><code class="language-yaml"># Minimal Vault policy for specific provider
path "secret/data/providers/vsphere/{{ identity.entity.aliases.auth_kubernetes_*.metadata.service_account_namespace }}" {
  capabilities = ["read"]
}

# Time-bound secrets
path "vsphere/creds/readonly" {
  capabilities = ["read"]
  allowed_parameters = {
    "ttl" = ["15m", "30m", "1h"]
  }
}
</code></pre>
<h3 id="2-secret-rotation-automation"><a class="header" href="#2-secret-rotation-automation">2. Secret Rotation Automation</a></h3>
<pre><code class="language-yaml">apiVersion: batch/v1
kind: CronJob
metadata:
  name: rotate-provider-secrets
  namespace: virtrigaud-system
spec:
  schedule: "0 2 * * 0"  # Weekly on Sunday at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: secret-rotator
              image: virtrigaud/secret-rotator:latest
              command:
                - /bin/sh
                - -c
                - |
                  # Force refresh of all external secrets
                  kubectl annotate externalsecret --all \
                    force-sync="$(date +%s)" \
                    --namespace=vsphere-providers
                  
                  # Restart provider deployments to pick up new secrets
                  kubectl rollout restart deployment \
                    --selector=app.kubernetes.io/name=virtrigaud-provider-runtime \
                    --namespace=vsphere-providers
          restartPolicy: OnFailure
          serviceAccountName: secret-rotator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-rotator
rules:
  - apiGroups: ["external-secrets.io"]
    resources: ["externalsecrets"]
    verbs: ["get", "list", "patch"]
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "patch"]
</code></pre>
<h3 id="3-audit-logging"><a class="header" href="#3-audit-logging">3. Audit Logging</a></h3>
<pre><code class="language-yaml"># Vault audit configuration
vault audit enable file file_path=/vault/logs/audit.log

# Example audit log entry structure
{
  "time": "2023-12-01T10:30:00Z",
  "type": "request",
  "auth": {
    "client_token": "hvs.xxx",
    "accessor": "hmac-sha256:xxx",
    "display_name": "kubernetes-virtrigaud-system-external-secrets",
    "policies": ["virtrigaud-policy"],
    "metadata": {
      "service_account_name": "external-secrets",
      "service_account_namespace": "virtrigaud-system"
    }
  },
  "request": {
    "id": "request-id",
    "operation": "read",
    "path": "secret/data/providers/vsphere",
    "data": null,
    "remote_address": "10.0.0.100"
  }
}
</code></pre>
<h3 id="4-emergency-procedures"><a class="header" href="#4-emergency-procedures">4. Emergency Procedures</a></h3>
<pre><code class="language-bash">#!/bin/bash
# emergency-secret-rotation.sh

echo "=== Emergency Secret Rotation ==="

# 1. Revoke all active leases for a provider
vault lease revoke -prefix vsphere/creds/

# 2. Force refresh all external secrets
kubectl get externalsecret --all-namespaces -o name | \
  xargs -I {} kubectl annotate {} force-sync="$(date +%s)"

# 3. Restart all provider deployments
kubectl get deployments --all-namespaces \
  -l app.kubernetes.io/name=virtrigaud-provider-runtime \
  -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}{"\n"}{end}' | \
  while read deployment; do
    kubectl rollout restart deployment $deployment
  done

# 4. Monitor rollout status
kubectl get deployments --all-namespaces \
  -l app.kubernetes.io/name=virtrigaud-provider-runtime \
  -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}{"\n"}{end}' | \
  while read deployment; do
    kubectl rollout status deployment $deployment --timeout=300s
  done

echo "Emergency rotation completed"
</code></pre>
<h3 id="5-secret-validation"><a class="header" href="#5-secret-validation">5. Secret Validation</a></h3>
<pre><code class="language-go">package validation

import (
    "crypto/x509"
    "encoding/pem"
    "fmt"
    "time"
)

func ValidateSecret(secretData map[string][]byte, secretType string) error {
    switch secretType {
    case "tls":
        return validateTLSSecret(secretData)
    case "ssh":
        return validateSSHSecret(secretData)
    case "credential":
        return validateCredentialSecret(secretData)
    }
    return nil
}

func validateTLSSecret(data map[string][]byte) error {
    cert, ok := data["tls.crt"]
    if !ok {
        return fmt.Errorf("missing tls.crt")
    }
    
    key, ok := data["tls.key"]
    if !ok {
        return fmt.Errorf("missing tls.key")
    }
    
    // Parse certificate
    block, _ := pem.Decode(cert)
    if block == nil {
        return fmt.Errorf("failed to parse certificate PEM")
    }
    
    parsedCert, err := x509.ParseCertificate(block.Bytes)
    if err != nil {
        return fmt.Errorf("failed to parse certificate: %w", err)
    }
    
    // Check expiration
    if time.Now().After(parsedCert.NotAfter) {
        return fmt.Errorf("certificate expired on %v", parsedCert.NotAfter)
    }
    
    if time.Now().Add(24*time.Hour).After(parsedCert.NotAfter) {
        return fmt.Errorf("certificate expires soon on %v", parsedCert.NotAfter)
    }
    
    // Validate key
    block, _ = pem.Decode(key)
    if block == nil {
        return fmt.Errorf("failed to parse private key PEM")
    }
    
    return nil
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="network-policies-for-provider-security"><a class="header" href="#network-policies-for-provider-security">Network Policies for Provider Security</a></h1>
<p>This guide covers Kubernetes NetworkPolicy configurations to secure communication between VirtRigaud components and provider services.</p>
<h2 id="overview-17"><a class="header" href="#overview-17">Overview</a></h2>
<p>NetworkPolicies provide network-level security by controlling traffic flow between pods, namespaces, and external endpoints. For VirtRigaud providers, this includes:</p>
<ul>
<li><strong>Ingress Control</strong>: Restricting which services can communicate with providers</li>
<li><strong>Egress Control</strong>: Limiting provider access to external hypervisor endpoints</li>
<li><strong>Namespace Isolation</strong>: Preventing cross-tenant communication</li>
<li><strong>External Access</strong>: Controlling access to hypervisor management interfaces</li>
</ul>
<h2 id="basic-networkpolicy-template"><a class="header" href="#basic-networkpolicy-template">Basic NetworkPolicy Template</a></h2>
<h3 id="provider-ingress-policy"><a class="header" href="#provider-ingress-policy">Provider Ingress Policy</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: provider-ingress
  namespace: provider-namespace
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider
  policyTypes:
    - Ingress
  ingress:
    # Allow from VirtRigaud manager
    - from:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: virtrigaud-manager
      ports:
        - protocol: TCP
          port: 9443  # gRPC provider port
    
    # Allow health checks from monitoring
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 8080  # Health/metrics port
    
    # Allow from same namespace (for debugging)
    - from:
        - podSelector: {}
      ports:
        - protocol: TCP
          port: 8080
</code></pre>
<h3 id="provider-egress-policy"><a class="header" href="#provider-egress-policy">Provider Egress Policy</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: provider-egress
  namespace: provider-namespace
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider
  policyTypes:
    - Egress
  egress:
    # Allow DNS resolution
    - to: []
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    
    # Allow HTTPS to Kubernetes API
    - to:
        - namespaceSelector:
            matchLabels:
              name: kube-system
      ports:
        - protocol: TCP
          port: 443
    
    # Allow access to hypervisor management interfaces
    - to: []
      ports:
        - protocol: TCP
          port: 443  # vCenter HTTPS
        - protocol: TCP
          port: 80   # vCenter HTTP (if needed)
    
    # For libvirt providers - allow access to hypervisor nodes
    - to:
        - podSelector:
            matchLabels:
              node-role.kubernetes.io/worker: "true"
      ports:
        - protocol: TCP
          port: 16509  # libvirt daemon
</code></pre>
<h2 id="environment-specific-policies"><a class="header" href="#environment-specific-policies">Environment-Specific Policies</a></h2>
<h3 id="vsphere-provider-5"><a class="header" href="#vsphere-provider-5">vSphere Provider</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vsphere-provider-policy
  namespace: vsphere-providers
  labels:
    provider: vsphere
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider-runtime
      provider: vsphere
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Manager access
    - from:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
      ports:
        - protocol: TCP
          port: 9443
    
    # Monitoring access
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8080

  egress:
    # DNS
    - to: []
      ports:
        - protocol: UDP
          port: 53
    
    # vCenter access (specific IP ranges)
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
            except:
              - 10.244.0.0/16  # Exclude pod network
      ports:
        - protocol: TCP
          port: 443
    
    - to:
        - ipBlock:
            cidr: 192.168.0.0/16
      ports:
        - protocol: TCP
          port: 443
    
    # ESXi host access for direct operations
    - to:
        - ipBlock:
            cidr: 10.1.0.0/24  # ESXi management network
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 902   # vCenter agent
</code></pre>
<h3 id="libvirt-provider-4"><a class="header" href="#libvirt-provider-4">Libvirt Provider</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: libvirt-provider-policy
  namespace: libvirt-providers
  labels:
    provider: libvirt
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider-runtime
      provider: libvirt
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Manager access
    - from:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
      ports:
        - protocol: TCP
          port: 9443
    
    # Monitoring access
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8080

  egress:
    # DNS
    - to: []
      ports:
        - protocol: UDP
          port: 53
    
    # Access to hypervisor nodes
    - to: []
      ports:
        - protocol: TCP
          port: 16509  # libvirt daemon
        - protocol: TCP
          port: 22     # SSH for remote libvirt
    
    # Access to shared storage (NFS, iSCSI, etc.)
    - to:
        - ipBlock:
            cidr: 10.2.0.0/24  # Storage network
      ports:
        - protocol: TCP
          port: 2049  # NFS
        - protocol: TCP
          port: 3260  # iSCSI
        - protocol: UDP
          port: 111   # RPC portmapper
</code></pre>
<h3 id="mock-provider-development"><a class="header" href="#mock-provider-development">Mock Provider (Development)</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mock-provider-policy
  namespace: development
  labels:
    provider: mock
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider-runtime
      provider: mock
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Allow from manager and other development pods
    - from:
        - namespaceSelector:
            matchLabels:
              environment: development
      ports:
        - protocol: TCP
          port: 9443
        - protocol: TCP
          port: 8080

  egress:
    # Allow all egress for development environment
    - to: []
</code></pre>
<h2 id="multi-tenant-isolation"><a class="header" href="#multi-tenant-isolation">Multi-Tenant Isolation</a></h2>
<h3 id="tenant-namespace-policies"><a class="header" href="#tenant-namespace-policies">Tenant Namespace Policies</a></h3>
<pre><code class="language-yaml"># Template for tenant-specific policies
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-isolation
  namespace: tenant-{{TENANT_NAME}}
  labels:
    tenant: "{{TENANT_NAME}}"
spec:
  podSelector: {}  # Apply to all pods in namespace
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Allow from same tenant namespace
    - from:
        - namespaceSelector:
            matchLabels:
              tenant: "{{TENANT_NAME}}"
    
    # Allow from VirtRigaud system namespace
    - from:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
    
    # Allow from monitoring namespace
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring

  egress:
    # Allow to same tenant namespace
    - to:
        - namespaceSelector:
            matchLabels:
              tenant: "{{TENANT_NAME}}"
    
    # Allow to VirtRigaud system namespace
    - to:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
    
    # DNS resolution
    - to: []
      ports:
        - protocol: UDP
          port: 53
    
    # External hypervisor access (tenant-specific IP ranges)
    - to:
        - ipBlock:
            cidr: "{{TENANT_HYPERVISOR_CIDR}}"
      ports:
        - protocol: TCP
          port: 443
</code></pre>
<h3 id="cross-tenant-communication-prevention"><a class="header" href="#cross-tenant-communication-prevention">Cross-Tenant Communication Prevention</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-cross-tenant
  namespace: tenant-production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Explicitly deny from other tenant namespaces
    - from: []
      # Empty from selector with explicit namespace exclusions
  
  egress:
    # Explicitly deny to other tenant namespaces
    - to:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
    - to:
        - namespaceSelector:
            matchLabels:
              name: monitoring
    - to:
        - namespaceSelector:
            matchLabels:
              tenant: production
    # Deny all other namespace access
</code></pre>
<h2 id="advanced-policies"><a class="header" href="#advanced-policies">Advanced Policies</a></h2>
<h3 id="time-based-access-control"><a class="header" href="#time-based-access-control">Time-Based Access Control</a></h3>
<pre><code class="language-yaml"># Use external controllers like OPA Gatekeeper for time-based policies
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: timerestriction
spec:
  crd:
    spec:
      names:
        kind: TimeRestriction
      validation:
        type: object
        properties:
          allowedHours:
            type: array
            items:
              type: integer
            description: "Allowed hours (0-23) for network access"
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package timerestriction
        
        violation[{"msg": msg}] {
          current_hour := time.now_ns() / 1000000000 / 3600 % 24
          not current_hour in input.parameters.allowedHours
          msg := sprintf("Network access not allowed at hour %v", [current_hour])
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: TimeRestriction
metadata:
  name: business-hours-only
spec:
  match:
    kinds:
      - apiGroups: ["networking.k8s.io"]
        kinds: ["NetworkPolicy"]
    namespaces: ["production"]
  parameters:
    allowedHours: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17]  # 8 AM - 5 PM
</code></pre>
<h3 id="dynamic-ip-allow-listing"><a class="header" href="#dynamic-ip-allow-listing">Dynamic IP Allow-listing</a></h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: dynamic-hypervisor-access
  namespace: provider-namespace
  annotations:
    # Use external controllers to update IP blocks dynamically
    network-policy-controller/update-interval: "300s"
    network-policy-controller/ip-source: "configmap:hypervisor-ips"
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud-provider
  policyTypes:
    - Egress
  egress:
    # Will be dynamically updated by controller
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
    # Static rules remain
    - to: []
      ports:
        - protocol: UDP
          port: 53
</code></pre>
<h2 id="monitoring-and-troubleshooting"><a class="header" href="#monitoring-and-troubleshooting">Monitoring and Troubleshooting</a></h2>
<h3 id="networkpolicy-monitoring"><a class="header" href="#networkpolicy-monitoring">NetworkPolicy Monitoring</a></h3>
<pre><code class="language-yaml"># ServiceMonitor for network policy violations
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: networkpolicy-monitoring
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: networkpolicy-exporter
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics

---
# Example alerts for network policy violations
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: networkpolicy-alerts
  namespace: monitoring
spec:
  groups:
    - name: networkpolicy.rules
      rules:
        - alert: NetworkPolicyDeniedConnections
          expr: increase(networkpolicy_denied_connections_total[5m]) &gt; 10
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "High number of denied network connections"
            description: "{{ $labels.source_namespace }}/{{ $labels.source_pod }} had {{ $value }} denied connections to {{ $labels.dest_namespace }}/{{ $labels.dest_pod }}"
</code></pre>
<h3 id="debug-networkpolicies"><a class="header" href="#debug-networkpolicies">Debug NetworkPolicies</a></h3>
<pre><code class="language-bash">#!/bin/bash
# debug-networkpolicy.sh

NAMESPACE=${1:-default}
POD_NAME=${2}

echo "=== NetworkPolicy Debug for $NAMESPACE/$POD_NAME ==="

# List all NetworkPolicies in namespace
echo "NetworkPolicies in namespace $NAMESPACE:"
kubectl get networkpolicy -n $NAMESPACE

# Show specific NetworkPolicy details
echo -e "\nNetworkPolicy details:"
kubectl get networkpolicy -n $NAMESPACE -o yaml

# Test connectivity
if [ ! -z "$POD_NAME" ]; then
    echo -e "\nTesting connectivity from $POD_NAME:"
    
    # Test DNS resolution
    kubectl exec -n $NAMESPACE $POD_NAME -- nslookup kubernetes.default.svc.cluster.local
    
    # Test internal connectivity
    kubectl exec -n $NAMESPACE $POD_NAME -- wget -qO- --timeout=5 http://kubernetes.default.svc.cluster.local/api
    
    # Test external connectivity (adjust as needed)
    kubectl exec -n $NAMESPACE $POD_NAME -- wget -qO- --timeout=5 https://google.com
fi

# Check iptables rules (if accessible)
echo -e "\nIPTables rules (if accessible):"
kubectl get nodes -o wide
echo "Run the following on a node to see iptables:"
echo "sudo iptables -L -n | grep -E '(KUBE|Chain)'"
</code></pre>
<h3 id="cni-specific-troubleshooting"><a class="header" href="#cni-specific-troubleshooting">CNI-Specific Troubleshooting</a></h3>
<h4 id="calico"><a class="header" href="#calico">Calico</a></h4>
<pre><code class="language-bash"># Check Calico network policies
kubectl get networkpolicy --all-namespaces
kubectl get globalnetworkpolicy

# Check Calico endpoints
kubectl get endpoints --all-namespaces

# Debug Calico connectivity
kubectl exec -it -n kube-system &lt;calico-node-pod&gt; -- /bin/sh
calicoctl get wep --all-namespaces
calicoctl get netpol --all-namespaces
</code></pre>
<h4 id="cilium"><a class="header" href="#cilium">Cilium</a></h4>
<pre><code class="language-bash"># Check Cilium network policies
kubectl get cnp --all-namespaces  # Cilium Network Policies
kubectl get ccnp --all-namespaces # Cilium Cluster Network Policies

# Debug Cilium connectivity
kubectl exec -it -n kube-system &lt;cilium-pod&gt; -- cilium endpoint list
kubectl exec -it -n kube-system &lt;cilium-pod&gt; -- cilium policy get
</code></pre>
<h2 id="security-best-practices-4"><a class="header" href="#security-best-practices-4">Security Best Practices</a></h2>
<h3 id="1-principle-of-least-privilege"><a class="header" href="#1-principle-of-least-privilege">1. Principle of Least Privilege</a></h3>
<pre><code class="language-yaml"># Example: Minimal egress for a provider
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: minimal-egress-example
spec:
  podSelector:
    matchLabels:
      app: provider
  policyTypes:
    - Egress
  egress:
    # Only allow what's absolutely necessary
    - to: []
      ports:
        - protocol: UDP
          port: 53  # DNS only
    - to:
        - ipBlock:
            cidr: 10.1.1.100/32  # Specific vCenter IP only
      ports:
        - protocol: TCP
          port: 443  # HTTPS only
</code></pre>
<h3 id="2-default-deny-policies"><a class="header" href="#2-default-deny-policies">2. Default Deny Policies</a></h3>
<pre><code class="language-yaml"># Apply default deny to all namespaces
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  # Empty ingress/egress rules = deny all
</code></pre>
<h3 id="3-regular-policy-auditing"><a class="header" href="#3-regular-policy-auditing">3. Regular Policy Auditing</a></h3>
<pre><code class="language-bash">#!/bin/bash
# audit-networkpolicies.sh

echo "=== NetworkPolicy Audit Report ==="
echo "Generated: $(date)"
echo

# Check for namespaces without NetworkPolicies
echo "Namespaces without NetworkPolicies:"
for ns in $(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}'); do
    if [ $(kubectl get networkpolicy -n $ns --no-headers 2&gt;/dev/null | wc -l) -eq 0 ]; then
        echo "  - $ns (WARNING: No network policies)"
    fi
done

echo

# Check for overly permissive policies
echo "Potentially overly permissive policies:"
kubectl get networkpolicy --all-namespaces -o json | jq -r '
  .items[] | 
  select(
    (.spec.egress[]?.to // []) | length == 0 or
    (.spec.ingress[]?.from // []) | length == 0
  ) | 
  "\(.metadata.namespace)/\(.metadata.name) - Check for overly broad rules"
'

echo

# Check for unused NetworkPolicies
echo "NetworkPolicies with no matching pods:"
kubectl get networkpolicy --all-namespaces -o json | jq -r '
  .items[] as $np |
  $np.metadata.namespace as $ns |
  $np.spec.podSelector as $selector |
  if ($selector | keys | length) == 0 then
    "\($ns)/\($np.metadata.name) - Applies to all pods in namespace"
  else
    "\($ns)/\($np.metadata.name) - Check if pods match selector"
  end
'
</code></pre>
<h3 id="4-integration-with-service-mesh"><a class="header" href="#4-integration-with-service-mesh">4. Integration with Service Mesh</a></h3>
<pre><code class="language-yaml"># Example: Istio integration with NetworkPolicies
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: istio-compatible-policy
spec:
  podSelector:
    matchLabels:
      app: provider
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow Istio sidecar communication
    - from:
        - podSelector:
            matchLabels:
              app: istio-proxy
      ports:
        - protocol: TCP
          port: 15090  # Istio pilot
    # Your application ports
    - from:
        - namespaceSelector:
            matchLabels:
              name: virtrigaud-system
      ports:
        - protocol: TCP
          port: 9443
  egress:
    # Allow Istio control plane
    - to:
        - namespaceSelector:
            matchLabels:
              name: istio-system
      ports:
        - protocol: TCP
          port: 15010  # Pilot
        - protocol: TCP
          port: 15011  # Pilot secure
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cli-tools-reference"><a class="header" href="#cli-tools-reference">CLI Tools Reference</a></h1>
<p>VirtRigaud provides a comprehensive set of command-line tools for managing virtual machines, developing providers, running conformance tests, and performing load testing. This guide covers all available CLI tools and their usage.</p>
<h2 id="overview-18"><a class="header" href="#overview-18">Overview</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Tool</th><th>Purpose</th><th>Target Users</th></tr></thead><tbody>
<tr><td><a href="CLI.html#vrtg"><code>vrtg</code></a></td><td>Main CLI for VM management and operations</td><td>End users, DevOps teams, System administrators</td></tr>
<tr><td><a href="CLI.html#vcts"><code>vcts</code></a></td><td>Conformance testing suite</td><td>Provider developers, QA teams, CI/CD pipelines</td></tr>
<tr><td><a href="CLI.html#vrtg-provider"><code>vrtg-provider</code></a></td><td>Provider development toolkit</td><td>Provider developers, Contributors</td></tr>
<tr><td><a href="CLI.html#virtrigaud-loadgen"><code>virtrigaud-loadgen</code></a></td><td>Load testing and benchmarking</td><td>Performance engineers, SREs</td></tr>
</tbody></table>
</div>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<h3 id="from-github-releases"><a class="header" href="#from-github-releases">From GitHub Releases</a></h3>
<pre><code class="language-bash"># Download the latest release
export VIRTRIGAUD_VERSION="v0.2.1"
export PLATFORM="linux-amd64"  # or darwin-amd64, windows-amd64

# Install main CLI tool
curl -L "https://github.com/projectbeskar/virtrigaud/releases/download/${VIRTRIGAUD_VERSION}/vrtg-${PLATFORM}" -o vrtg
chmod +x vrtg
sudo mv vrtg /usr/local/bin/

# Install all CLI tools
curl -L "https://github.com/projectbeskar/virtrigaud/releases/download/${VIRTRIGAUD_VERSION}/virtrigaud-cli-${PLATFORM}.tar.gz" | tar xz
sudo mv vrtg vcts vrtg-provider virtrigaud-loadgen /usr/local/bin/
</code></pre>
<h3 id="from-source"><a class="header" href="#from-source">From Source</a></h3>
<pre><code class="language-bash">git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud

# Build all CLI tools
make build-cli

# Install to /usr/local/bin
sudo make install-cli

# Or install to custom location
make install-cli PREFIX=/usr/local
</code></pre>
<h3 id="using-go"><a class="header" href="#using-go">Using Go</a></h3>
<pre><code class="language-bash"># Install specific version
go install github.com/projectbeskar/virtrigaud/cmd/vrtg@v0.2.1
go install github.com/projectbeskar/virtrigaud/cmd/vcts@v0.2.1
go install github.com/projectbeskar/virtrigaud/cmd/vrtg-provider@v0.2.1
go install github.com/projectbeskar/virtrigaud/cmd/virtrigaud-loadgen@v0.2.1

# Install latest
go install github.com/projectbeskar/virtrigaud/cmd/vrtg@latest
</code></pre>
<h3 id="completion"><a class="header" href="#completion">Completion</a></h3>
<p>Enable shell completion for enhanced productivity:</p>
<pre><code class="language-bash"># Bash
vrtg completion bash &gt; /etc/bash_completion.d/vrtg
source /etc/bash_completion.d/vrtg

# Zsh
vrtg completion zsh &gt; "${fpath[1]}/_vrtg"

# Fish
vrtg completion fish &gt; ~/.config/fish/completions/vrtg.fish

# PowerShell
vrtg completion powershell | Out-String | Invoke-Expression
</code></pre>
<hr />
<h2 id="vrtg"><a class="header" href="#vrtg">vrtg</a></h2>
<p>The main CLI tool for managing VirtRigaud resources and virtual machines.</p>
<h3 id="global-flags"><a class="header" href="#global-flags">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file (default: $KUBECONFIG or ~/.kube/config)
--namespace string    Kubernetes namespace (default: "default")
--output string       Output format: table, json, yaml (default: "table")
--timeout duration    Operation timeout (default: 30s)
--verbose             Enable verbose output
-h, --help           Help for vrtg
</code></pre>
<h3 id="commands"><a class="header" href="#commands">Commands</a></h3>
<h4 id="vm---virtual-machine-management"><a class="header" href="#vm---virtual-machine-management">vm - Virtual Machine Management</a></h4>
<p>Manage virtual machines with comprehensive lifecycle operations.</p>
<pre><code class="language-bash"># List virtual machines
vrtg vm list [flags]

# Describe a virtual machine
vrtg vm describe &lt;name&gt; [flags]

# Show VM events
vrtg vm events &lt;name&gt; [flags]

# Get VM console URL
vrtg vm console-url &lt;name&gt; [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--all-namespaces</code>: List VMs across all namespaces</li>
<li><code>--label-selector</code>: Filter by labels (e.g., <code>app=web,env=prod</code>)</li>
<li><code>--field-selector</code>: Filter by fields (e.g., <code>spec.powerState=On</code>)</li>
<li><code>--sort-by</code>: Sort output by column (name, namespace, powerState, provider)</li>
<li><code>--watch</code>: Watch for changes</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># List all VMs in table format
vrtg vm list

# List VMs with custom output format
vrtg vm list --output json --namespace production

# List VMs across all namespaces
vrtg vm list --all-namespaces

# Filter VMs by labels
vrtg vm list --label-selector environment=production,tier=web

# Watch VM status changes
vrtg vm list --watch

# Get detailed VM information
vrtg vm describe my-vm --output yaml

# Get VM console URL
vrtg vm console-url my-vm

# Show recent VM events
vrtg vm events my-vm
</code></pre>
<h4 id="provider---provider-management"><a class="header" href="#provider---provider-management">provider - Provider Management</a></h4>
<p>Manage provider configurations and monitor their health.</p>
<pre><code class="language-bash"># List providers
vrtg provider list [flags]

# Show provider status
vrtg provider status &lt;name&gt; [flags]

# Show provider logs
vrtg provider logs &lt;name&gt; [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--follow</code>: Follow log output (for logs command)</li>
<li><code>--tail</code>: Number of lines to show from end of logs (default: 100)</li>
<li><code>--since</code>: Show logs since timestamp (e.g., 1h, 30m)</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># List all providers
vrtg provider list

# Check provider status
vrtg provider status vsphere-provider

# View provider logs
vrtg provider logs vsphere-provider --tail 50

# Follow provider logs in real-time
vrtg provider logs vsphere-provider --follow

# Show logs from last hour
vrtg provider logs vsphere-provider --since 1h
</code></pre>
<h4 id="snapshot---snapshot-management"><a class="header" href="#snapshot---snapshot-management">snapshot - Snapshot Management</a></h4>
<p>Manage VM snapshots for backup and recovery.</p>
<pre><code class="language-bash"># Create a VM snapshot
vrtg snapshot create &lt;vm-name&gt; &lt;snapshot-name&gt; [flags]

# List snapshots
vrtg snapshot list [vm-name] [flags]

# Revert VM to snapshot
vrtg snapshot revert &lt;vm-name&gt; &lt;snapshot-name&gt; [flags]
</code></pre>
<p><strong>Flags for create:</strong></p>
<ul>
<li><code>--description</code>: Snapshot description</li>
<li><code>--include-memory</code>: Include memory state in snapshot</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Create a simple snapshot
vrtg snapshot create my-vm pre-upgrade

# Create snapshot with description and memory
vrtg snapshot create my-vm pre-maintenance \
  --description "Before maintenance window" \
  --include-memory

# List all snapshots
vrtg snapshot list

# List snapshots for specific VM
vrtg snapshot list my-vm

# Revert to a snapshot
vrtg snapshot revert my-vm pre-upgrade
</code></pre>
<h4 id="clone---vm-cloning"><a class="header" href="#clone---vm-cloning">clone - VM Cloning</a></h4>
<p>Clone virtual machines for rapid provisioning.</p>
<pre><code class="language-bash"># Clone a virtual machine
vrtg clone run &lt;source-vm&gt; &lt;target-vm&gt; [flags]

# List clone operations
vrtg clone list [flags]
</code></pre>
<p><strong>Flags for run:</strong></p>
<ul>
<li><code>--linked</code>: Create linked clone (faster, space-efficient)</li>
<li><code>--target-namespace</code>: Namespace for target VM</li>
<li><code>--customize</code>: Apply customization during clone</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Simple VM clone
vrtg clone run template-vm new-vm

# Linked clone for development
vrtg clone run production-vm dev-vm --linked

# Clone to different namespace
vrtg clone run template-vm test-vm --target-namespace testing

# List clone operations
vrtg clone list
</code></pre>
<h4 id="conformance---provider-testing"><a class="header" href="#conformance---provider-testing">conformance - Provider Testing</a></h4>
<p>Run conformance tests against providers.</p>
<pre><code class="language-bash"># Run conformance tests
vrtg conformance run &lt;provider&gt; [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--output-dir</code>: Directory for test results</li>
<li><code>--skip-tests</code>: Comma-separated list of tests to skip</li>
<li><code>--timeout</code>: Test timeout (default: 30m)</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Run conformance tests
vrtg conformance run vsphere-provider

# Run tests with custom timeout
vrtg conformance run vsphere-provider --timeout 1h

# Skip specific tests
vrtg conformance run vsphere-provider --skip-tests "test-large-vms,test-network"
</code></pre>
<h4 id="diag---diagnostics"><a class="header" href="#diag---diagnostics">diag - Diagnostics</a></h4>
<p>Diagnostic tools for troubleshooting.</p>
<pre><code class="language-bash"># Create diagnostic bundle
vrtg diag bundle [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--output</code>: Output file path (default: virtrigaud-diag-&lt;timestamp&gt;.tar.gz)</li>
<li><code>--include-logs</code>: Include provider logs in bundle</li>
<li><code>--since</code>: Collect logs since timestamp</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Create diagnostic bundle
vrtg diag bundle

# Create bundle with logs from last 2 hours
vrtg diag bundle --include-logs --since 2h

# Custom output location
vrtg diag bundle --output /tmp/debug-bundle.tar.gz
</code></pre>
<h4 id="init---installation"><a class="header" href="#init---installation">init - Installation</a></h4>
<p>Initialize VirtRigaud in a Kubernetes cluster.</p>
<pre><code class="language-bash"># Initialize virtrigaud
vrtg init [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--chart-version</code>: Helm chart version to install</li>
<li><code>--namespace</code>: Installation namespace (default: virtrigaud-system)</li>
<li><code>--values</code>: Values file for Helm chart</li>
<li><code>--dry-run</code>: Show what would be installed</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Basic installation
vrtg init

# Install specific version
vrtg init --chart-version v0.2.1

# Install with custom values
vrtg init --values custom-values.yaml

# Dry run to see what would be installed
vrtg init --dry-run
</code></pre>
<hr />
<h2 id="vcts"><a class="header" href="#vcts">vcts</a></h2>
<p>VirtRigaud Conformance Test Suite - runs standardized tests against providers.</p>
<h3 id="usage-1"><a class="header" href="#usage-1">Usage</a></h3>
<pre><code class="language-bash">vcts [command] [flags]
</code></pre>
<h3 id="global-flags-1"><a class="header" href="#global-flags-1">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file
--namespace string    Kubernetes namespace (default: "virtrigaud-system")
--provider string     Provider name to test
--output-dir string   Output directory for test results (default: "./conformance-results")
--skip-tests strings  Comma-separated list of tests to skip
--timeout duration    Test timeout (default: 30m)
--parallel int        Number of parallel test executions (default: 1)
--verbose             Enable verbose output
</code></pre>
<h3 id="commands-1"><a class="header" href="#commands-1">Commands</a></h3>
<h4 id="run---execute-tests"><a class="header" href="#run---execute-tests">run - Execute Tests</a></h4>
<pre><code class="language-bash"># Run all conformance tests
vcts run --provider vsphere-provider

# Run with custom settings
vcts run --provider vsphere-provider \
  --timeout 1h \
  --parallel 3 \
  --output-dir /tmp/test-results

# Skip specific tests
vcts run --provider libvirt-provider \
  --skip-tests "test-snapshots,test-linked-clones"

# Verbose output for debugging
vcts run --provider proxmox-provider --verbose
</code></pre>
<h4 id="list---list-available-tests"><a class="header" href="#list---list-available-tests">list - List Available Tests</a></h4>
<pre><code class="language-bash"># List all available tests
vcts list

# List tests for specific capability
vcts list --capability snapshots
</code></pre>
<h4 id="validate---validate-provider"><a class="header" href="#validate---validate-provider">validate - Validate Provider</a></h4>
<pre><code class="language-bash"># Validate provider configuration
vcts validate --provider vsphere-provider

# Check provider connectivity
vcts validate --provider vsphere-provider --check-connectivity
</code></pre>
<h3 id="test-categories"><a class="header" href="#test-categories">Test Categories</a></h3>
<ol>
<li><strong>Basic Operations</strong>: VM creation, deletion, power operations</li>
<li><strong>Lifecycle Management</strong>: Start, stop, restart, suspend operations</li>
<li><strong>Resource Management</strong>: CPU, memory, disk operations</li>
<li><strong>Networking</strong>: Network configuration and connectivity</li>
<li><strong>Storage</strong>: Disk operations, resizing, multiple disks</li>
<li><strong>Snapshots</strong>: Create, list, revert, delete snapshots</li>
<li><strong>Cloning</strong>: VM cloning and linked clones</li>
<li><strong>Error Handling</strong>: Provider error scenarios</li>
<li><strong>Performance</strong>: Basic performance benchmarks</li>
</ol>
<h3 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h3>
<p>Test results are available in multiple formats:</p>
<ul>
<li><strong>JUnit XML</strong>: For CI/CD integration</li>
<li><strong>JSON</strong>: Machine-readable format</li>
<li><strong>HTML</strong>: Human-readable report</li>
<li><strong>TAP</strong>: Test Anything Protocol</li>
</ul>
<hr />
<h2 id="vrtg-provider"><a class="header" href="#vrtg-provider">vrtg-provider</a></h2>
<p>Provider development toolkit for creating and managing VirtRigaud providers.</p>
<h3 id="usage-2"><a class="header" href="#usage-2">Usage</a></h3>
<pre><code class="language-bash">vrtg-provider [command] [flags]
</code></pre>
<h3 id="global-flags-2"><a class="header" href="#global-flags-2">Global Flags</a></h3>
<pre><code class="language-bash">--verbose     Enable verbose output
--help        Help for vrtg-provider
</code></pre>
<h3 id="commands-2"><a class="header" href="#commands-2">Commands</a></h3>
<h4 id="init---initialize-provider"><a class="header" href="#init---initialize-provider">init - Initialize Provider</a></h4>
<p>Bootstrap a new provider project with scaffolding.</p>
<pre><code class="language-bash">vrtg-provider init &lt;provider-name&gt; [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--template</code>: Template to use (grpc, rest, hybrid)</li>
<li><code>--output-dir</code>: Output directory (default: current directory)</li>
<li><code>--module</code>: Go module name</li>
<li><code>--author</code>: Author name for generated files</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Create basic gRPC provider
vrtg-provider init my-provider --template grpc

# Create with custom module
vrtg-provider init my-provider \
  --template grpc \
  --module github.com/myorg/my-provider \
  --author "John Doe &lt;john@example.com&gt;"

# Create in specific directory
vrtg-provider init my-provider \
  --output-dir /path/to/providers \
  --template grpc
</code></pre>
<h4 id="generate---code-generation"><a class="header" href="#generate---code-generation">generate - Code Generation</a></h4>
<p>Generate boilerplate code for provider implementation.</p>
<pre><code class="language-bash">vrtg-provider generate [type] [flags]
</code></pre>
<p><strong>Types:</strong></p>
<ul>
<li><code>client</code>: Generate client code</li>
<li><code>server</code>: Generate server implementation</li>
<li><code>tests</code>: Generate test scaffolding</li>
<li><code>docs</code>: Generate documentation templates</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Generate client code
vrtg-provider generate client --provider my-provider

# Generate test scaffolding
vrtg-provider generate tests --provider my-provider

# Generate documentation
vrtg-provider generate docs --provider my-provider
</code></pre>
<h4 id="verify---verification"><a class="header" href="#verify---verification">verify - Verification</a></h4>
<p>Verify provider implementation and compliance.</p>
<pre><code class="language-bash">vrtg-provider verify [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--provider-dir</code>: Provider directory to verify</li>
<li><code>--check-interface</code>: Verify interface compliance</li>
<li><code>--check-docs</code>: Verify documentation completeness</li>
<li><code>--check-tests</code>: Verify test coverage</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Basic verification
vrtg-provider verify --provider-dir ./my-provider

# Comprehensive check
vrtg-provider verify \
  --provider-dir ./my-provider \
  --check-interface \
  --check-docs \
  --check-tests
</code></pre>
<h4 id="publish---publishing"><a class="header" href="#publish---publishing">publish - Publishing</a></h4>
<p>Prepare provider for publishing and distribution.</p>
<pre><code class="language-bash">vrtg-provider publish [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--provider-dir</code>: Provider directory</li>
<li><code>--version</code>: Version to publish</li>
<li><code>--registry</code>: Container registry</li>
<li><code>--chart-repo</code>: Helm chart repository</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Publish provider
vrtg-provider publish \
  --provider-dir ./my-provider \
  --version v1.0.0 \
  --registry ghcr.io/myorg

# Publish with Helm chart
vrtg-provider publish \
  --provider-dir ./my-provider \
  --version v1.0.0 \
  --registry ghcr.io/myorg \
  --chart-repo https://charts.myorg.com
</code></pre>
<h3 id="provider-template-structure"><a class="header" href="#provider-template-structure">Provider Template Structure</a></h3>
<pre><code>my-provider/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ provider/
‚îÇ       ‚îî‚îÄ‚îÄ main.go              # Provider entry point
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ provider/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.go           # gRPC server implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.go           # Provider client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.go            # Provider-specific types
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îî‚îÄ‚îÄ config.go           # Configuration management
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îî‚îÄ‚îÄ api/                    # Public API interfaces
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ conformance/            # Conformance tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/            # Integration tests
‚îú‚îÄ‚îÄ deploy/
‚îÇ   ‚îú‚îÄ‚îÄ helm/                   # Helm charts
‚îÇ   ‚îî‚îÄ‚îÄ k8s/                    # Kubernetes manifests
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îú‚îÄ‚îÄ Dockerfile                  # Container image
‚îú‚îÄ‚îÄ Makefile                    # Build automation
‚îî‚îÄ‚îÄ README.md                   # Provider documentation
</code></pre>
<hr />
<h2 id="virtrigaud-loadgen"><a class="header" href="#virtrigaud-loadgen">virtrigaud-loadgen</a></h2>
<p>Load testing and benchmarking tool for VirtRigaud deployments.</p>
<h3 id="usage-3"><a class="header" href="#usage-3">Usage</a></h3>
<pre><code class="language-bash">virtrigaud-loadgen [command] [flags]
</code></pre>
<h3 id="global-flags-3"><a class="header" href="#global-flags-3">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file
--namespace string    Kubernetes namespace (default: "default")  
--output-dir string   Output directory for results (default: "./loadgen-results")
--config-file string  Load generation configuration file
--dry-run            Show what would be executed without running
--verbose            Enable verbose output
</code></pre>
<h3 id="commands-3"><a class="header" href="#commands-3">Commands</a></h3>
<h4 id="run---execute-load-test"><a class="header" href="#run---execute-load-test">run - Execute Load Test</a></h4>
<pre><code class="language-bash">virtrigaud-loadgen run [flags]
</code></pre>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--vms</code>: Number of VMs to create (default: 10)</li>
<li><code>--duration</code>: Test duration (default: 10m)</li>
<li><code>--ramp-up</code>: Ramp-up time (default: 2m)</li>
<li><code>--workers</code>: Number of concurrent workers (default: 5)</li>
<li><code>--provider</code>: Provider to test against</li>
<li><code>--vm-class</code>: VMClass to use for test VMs</li>
<li><code>--vm-image</code>: VMImage to use for test VMs</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Basic load test
virtrigaud-loadgen run --vms 50 --duration 15m

# Comprehensive load test
virtrigaud-loadgen run \
  --vms 100 \
  --duration 30m \
  --ramp-up 5m \
  --workers 10 \
  --provider vsphere-provider

# Test with specific configuration
virtrigaud-loadgen run --config-file loadtest-config.yaml
</code></pre>
<h4 id="config---configuration-management"><a class="header" href="#config---configuration-management">config - Configuration Management</a></h4>
<pre><code class="language-bash"># Generate sample configuration
virtrigaud-loadgen config generate --output sample-config.yaml

# Validate configuration
virtrigaud-loadgen config validate --config-file my-config.yaml
</code></pre>
<h3 id="configuration-file"><a class="header" href="#configuration-file">Configuration File</a></h3>
<pre><code class="language-yaml"># loadtest-config.yaml
metadata:
  name: "production-load-test"
  description: "Load test for production environment"

spec:
  # Test parameters
  vms: 100
  duration: "30m"
  rampUp: "5m"
  workers: 10
  
  # Target configuration
  provider: "vsphere-provider"
  namespace: "loadtest"
  
  # VM configuration
  vmClass: "standard-vm"
  vmImage: "ubuntu-22-04"
  
  # Test scenarios
  scenarios:
    - name: "vm-lifecycle"
      weight: 70
      operations:
        - create
        - start
        - stop
        - delete
    
    - name: "vm-operations"
      weight: 20
      operations:
        - snapshot
        - clone
        - reconfigure
    
    - name: "provider-stress"
      weight: 10
      operations:
        - rapid-create-delete
        - concurrent-operations

  # Reporting
  reporting:
    formats: ["json", "html", "csv"]
    metrics:
      - response-time
      - throughput
      - error-rate
      - resource-usage
</code></pre>
<h3 id="metrics-and-reporting"><a class="header" href="#metrics-and-reporting">Metrics and Reporting</a></h3>
<p>Load test results include:</p>
<ul>
<li><strong>Performance Metrics</strong>: Response times, throughput, latency percentiles</li>
<li><strong>Error Analysis</strong>: Error rates, failure patterns, error categorization</li>
<li><strong>Resource Usage</strong>: CPU, memory, network utilization</li>
<li><strong>Provider Metrics</strong>: Provider-specific performance indicators</li>
<li><strong>Trend Analysis</strong>: Performance over time, bottleneck identification</li>
</ul>
<h3 id="output-formats-1"><a class="header" href="#output-formats-1">Output Formats</a></h3>
<ul>
<li><strong>JSON</strong>: Machine-readable results for automation</li>
<li><strong>HTML</strong>: Interactive dashboard with charts and graphs</li>
<li><strong>CSV</strong>: Raw data for further analysis</li>
<li><strong>Prometheus</strong>: Metrics export for monitoring systems</li>
</ul>
<hr />
<h2 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h2>
<h3 id="automation-and-scripting"><a class="header" href="#automation-and-scripting">Automation and Scripting</a></h3>
<h4 id="bash-integration"><a class="header" href="#bash-integration">Bash Integration</a></h4>
<pre><code class="language-bash">#!/bin/bash
# VM management script

# Function to check VM status
check_vm_status() {
  local vm_name=$1
  vrtg vm describe "$vm_name" --output json | jq -r '.status.powerState'
}

# Wait for VM to be ready
wait_for_vm() {
  local vm_name=$1
  local timeout=300
  local count=0
  
  while [ $count -lt $timeout ]; do
    status=$(check_vm_status "$vm_name")
    if [ "$status" = "On" ]; then
      echo "VM $vm_name is ready"
      return 0
    fi
    sleep 5
    count=$((count + 5))
  done
  
  echo "Timeout waiting for VM $vm_name"
  return 1
}

# Create and wait for VM
vrtg vm create --file vm-config.yaml
wait_for_vm "my-vm"
</code></pre>
<h4 id="cicd-integration-1"><a class="header" href="#cicd-integration-1">CI/CD Integration</a></h4>
<pre><code class="language-yaml"># .github/workflows/vm-test.yml
name: VM Integration Test

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install vrtg CLI
        run: |
          curl -L "https://github.com/projectbeskar/virtrigaud/releases/latest/download/vrtg-linux-amd64" -o vrtg
          chmod +x vrtg
          sudo mv vrtg /usr/local/bin/
      
      - name: Setup kubeconfig
        run: echo "${{ secrets.KUBECONFIG }}" | base64 -d &gt; ~/.kube/config
      
      - name: Run conformance tests
        run: vcts run --provider test-provider --output-dir test-results
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: conformance-results
          path: test-results/
</code></pre>
<h3 id="configuration-management-1"><a class="header" href="#configuration-management-1">Configuration Management</a></h3>
<h4 id="environment-specific-configurations"><a class="header" href="#environment-specific-configurations">Environment-specific Configurations</a></h4>
<pre><code class="language-bash"># Development environment
export VRTG_KUBECONFIG=~/.kube/dev-config
export VRTG_NAMESPACE=development
export VRTG_OUTPUT=yaml

# Production environment  
export VRTG_KUBECONFIG=~/.kube/prod-config
export VRTG_NAMESPACE=production
export VRTG_OUTPUT=json

# Use environment-specific settings
vrtg vm list  # Uses environment variables
</code></pre>
<h4 id="configuration-files"><a class="header" href="#configuration-files">Configuration Files</a></h4>
<p>Create <code>~/.vrtg/config.yaml</code>:</p>
<pre><code class="language-yaml">contexts:
  development:
    kubeconfig: ~/.kube/dev-config
    namespace: development
    output: yaml
    timeout: 30s
  
  production:
    kubeconfig: ~/.kube/prod-config
    namespace: production
    output: json
    timeout: 60s

current-context: development

aliases:
  ls: vm list
  get: vm describe
  logs: provider logs
</code></pre>
<h3 id="troubleshooting-15"><a class="header" href="#troubleshooting-15">Troubleshooting</a></h3>
<h4 id="common-issues-10"><a class="header" href="#common-issues-10">Common Issues</a></h4>
<ol>
<li><strong>Connection Issues</strong></li>
</ol>
<pre><code class="language-bash"># Check cluster connectivity
vrtg provider list

# Validate kubeconfig
kubectl cluster-info

# Check provider logs
vrtg provider logs &lt;provider-name&gt; --tail 100
</code></pre>
<ol start="2">
<li><strong>Permission Issues</strong></li>
</ol>
<pre><code class="language-bash"># Check RBAC permissions
kubectl auth can-i create virtualmachines

# Get current user context
kubectl auth whoami
</code></pre>
<ol start="3">
<li><strong>Provider Issues</strong></li>
</ol>
<pre><code class="language-bash"># Check provider status
vrtg provider status &lt;provider-name&gt;

# Run diagnostics
vrtg diag bundle --include-logs
</code></pre>
<h4 id="debug-mode-1"><a class="header" href="#debug-mode-1">Debug Mode</a></h4>
<p>Enable debug output:</p>
<pre><code class="language-bash"># Global debug flag
vrtg --verbose vm list

# Provider-specific debugging
vrtg provider logs &lt;provider-name&gt; --follow --verbose

# Conformance test debugging
vcts run --provider &lt;provider-name&gt; --verbose
</code></pre>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="getting-started/quickstart.html">Getting Started Guide</a></li>
<li><a href="providers/development.html">Provider Development</a></li>
<li><a href="api-reference/">API Reference</a></li>
<li><a href="examples/">Examples</a></li>
<li><a href="RESILIENCE.html">Troubleshooting</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cli-reference"><a class="header" href="#cli-reference">CLI Reference</a></h1>
<p>VirtRigaud provides several command-line tools for managing virtual machines, testing providers, and developing new providers. All tools are available as part of VirtRigaud v0.2.0.</p>
<h2 id="overview-19"><a class="header" href="#overview-19">Overview</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Tool</th><th>Purpose</th><th>Target Users</th></tr></thead><tbody>
<tr><td><a href="api-reference/cli.html#vrtg"><code>vrtg</code></a></td><td>Main CLI for VM management</td><td>End users, DevOps teams</td></tr>
<tr><td><a href="api-reference/cli.html#vcts"><code>vcts</code></a></td><td>Conformance testing suite</td><td>Provider developers, QA teams</td></tr>
<tr><td><a href="api-reference/cli.html#vrtg-provider"><code>vrtg-provider</code></a></td><td>Provider development toolkit</td><td>Provider developers</td></tr>
<tr><td><a href="api-reference/cli.html#virtrigaud-loadgen"><code>virtrigaud-loadgen</code></a></td><td>Load testing and benchmarking</td><td>Performance engineers</td></tr>
</tbody></table>
</div>
<h2 id="installation-3"><a class="header" href="#installation-3">Installation</a></h2>
<h3 id="from-github-releases-1"><a class="header" href="#from-github-releases-1">From GitHub Releases</a></h3>
<pre><code class="language-bash"># Download the latest release
curl -L "https://github.com/projectbeskar/virtrigaud/releases/download/v0.2.0/vrtg-linux-amd64" -o vrtg
chmod +x vrtg
sudo mv vrtg /usr/local/bin/

# Install all CLI tools
curl -L "https://github.com/projectbeskar/virtrigaud/releases/download/v0.2.0/virtrigaud-cli-linux-amd64.tar.gz" | tar xz
sudo mv vrtg vcts vrtg-provider virtrigaud-loadgen /usr/local/bin/
</code></pre>
<h3 id="from-source-1"><a class="header" href="#from-source-1">From Source</a></h3>
<pre><code class="language-bash">git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud

# Build all CLI tools
make build-cli

# Install to /usr/local/bin
sudo make install-cli
</code></pre>
<h3 id="using-go-1"><a class="header" href="#using-go-1">Using Go</a></h3>
<pre><code class="language-bash">go install github.com/projectbeskar/virtrigaud/cmd/vrtg@v0.2.0
go install github.com/projectbeskar/virtrigaud/cmd/vcts@v0.2.0
go install github.com/projectbeskar/virtrigaud/cmd/vrtg-provider@v0.2.0
go install github.com/projectbeskar/virtrigaud/cmd/virtrigaud-loadgen@v0.2.0
</code></pre>
<h2 id="vrtg-1"><a class="header" href="#vrtg-1">vrtg</a></h2>
<p>The main CLI tool for managing VirtRigaud resources and virtual machines.</p>
<h3 id="global-flags-4"><a class="header" href="#global-flags-4">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file (default: $KUBECONFIG or ~/.kube/config)
--namespace string    Kubernetes namespace (default: "default")
--output string       Output format: table, json, yaml (default: "table")
--timeout duration    Operation timeout (default: 5m0s)
-h, --help           Help for vrtg
</code></pre>
<h3 id="commands-4"><a class="header" href="#commands-4">Commands</a></h3>
<h4 id="vm"><a class="header" href="#vm">vm</a></h4>
<p>Manage virtual machines.</p>
<pre><code class="language-bash"># List all VMs
vrtg vm list

# Get detailed VM information
vrtg vm get &lt;vm-name&gt;

# Create a VM from configuration
vrtg vm create --file vm.yaml

# Delete a VM
vrtg vm delete &lt;vm-name&gt;

# Power operations
vrtg vm start &lt;vm-name&gt;
vrtg vm stop &lt;vm-name&gt;
vrtg vm restart &lt;vm-name&gt;

# Scale VMSet
vrtg vm scale &lt;vmset-name&gt; --replicas 5

# Get VM console URL
vrtg vm console &lt;vm-name&gt;

# Watch VM status changes
vrtg vm watch &lt;vm-name&gt;
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># List VMs with custom output
vrtg vm list --output json --namespace production

# Create VM with timeout
vrtg vm create --file my-vm.yaml --timeout 10m

# Power on all VMs in namespace
vrtg vm list --output json | jq -r '.items[].metadata.name' | xargs -I {} vrtg vm start {}
</code></pre>
<h4 id="provider-1"><a class="header" href="#provider-1">provider</a></h4>
<p>Manage provider configurations.</p>
<pre><code class="language-bash"># List providers
vrtg provider list

# Get provider details
vrtg provider get &lt;provider-name&gt;

# Check provider connectivity
vrtg provider validate &lt;provider-name&gt;

# Get provider capabilities
vrtg provider capabilities &lt;provider-name&gt;

# View provider logs
vrtg provider logs &lt;provider-name&gt;

# Test provider functionality
vrtg provider test &lt;provider-name&gt;
</code></pre>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># Validate all providers
vrtg provider list --output json | jq -r '.items[].metadata.name' | xargs -I {} vrtg provider validate {}

# Get detailed provider status
vrtg provider get vsphere-prod --output yaml
</code></pre>
<h4 id="image"><a class="header" href="#image">image</a></h4>
<p>Manage VM images and templates.</p>
<pre><code class="language-bash"># List available images
vrtg image list

# Get image details
vrtg image get &lt;image-name&gt;

# Prepare an image
vrtg image prepare &lt;image-name&gt;

# Delete an image
vrtg image delete &lt;image-name&gt;
</code></pre>
<h4 id="snapshot"><a class="header" href="#snapshot">snapshot</a></h4>
<p>Manage VM snapshots.</p>
<pre><code class="language-bash"># List snapshots for a VM
vrtg snapshot list --vm &lt;vm-name&gt;

# Create a snapshot
vrtg snapshot create &lt;vm-name&gt; --name "pre-upgrade"

# Restore from snapshot
vrtg snapshot restore &lt;vm-name&gt; &lt;snapshot-name&gt;

# Delete a snapshot
vrtg snapshot delete &lt;vm-name&gt; &lt;snapshot-name&gt;
</code></pre>
<h4 id="completion-1"><a class="header" href="#completion-1">completion</a></h4>
<p>Generate shell completion scripts.</p>
<pre><code class="language-bash"># Bash
vrtg completion bash &gt; /etc/bash_completion.d/vrtg

# Zsh
vrtg completion zsh &gt; "${fpath[1]}/_vrtg"

# Fish
vrtg completion fish &gt; ~/.config/fish/completions/vrtg.fish

# PowerShell
vrtg completion powershell &gt; vrtg.ps1
</code></pre>
<h3 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h3>
<p>vrtg uses the same kubeconfig as kubectl. Configuration precedence:</p>
<ol>
<li><code>--kubeconfig</code> flag</li>
<li><code>KUBECONFIG</code> environment variable</li>
<li><code>~/.kube/config</code></li>
</ol>
<h4 id="config-file"><a class="header" href="#config-file">Config File</a></h4>
<p>Create <code>~/.vrtg/config.yaml</code> for default settings:</p>
<pre><code class="language-yaml">defaults:
  namespace: "virtrigaud-system"
  timeout: "10m"
  output: "table"
providers:
  preferred: "vsphere-prod"
output:
  colors: true
  timestamps: true
</code></pre>
<h2 id="vcts-1"><a class="header" href="#vcts-1">vcts</a></h2>
<p>VirtRigaud Conformance Test Suite for validating provider implementations.</p>
<h3 id="global-flags-5"><a class="header" href="#global-flags-5">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file
--namespace string    Test namespace (default: "vcts")
--provider string     Provider to test
--output-dir string   Directory for test results
--timeout duration    Test timeout (default: 30m)
--parallel int        Number of parallel tests (default: 1)
--skip strings        Tests to skip (comma-separated)
--verbose             Verbose output
-h, --help           Help for vcts
</code></pre>
<h3 id="commands-5"><a class="header" href="#commands-5">Commands</a></h3>
<h4 id="run"><a class="header" href="#run">run</a></h4>
<p>Run conformance tests against a provider.</p>
<pre><code class="language-bash"># Run all tests
vcts run --provider vsphere-prod

# Run specific test suites
vcts run --provider vsphere-prod --suites core,storage

# Run with custom configuration
vcts run --provider libvirt-test --config test-config.yaml

# Skip specific tests
vcts run --provider vsphere-prod --skip "test-large-vm,test-snapshot-memory"

# Generate detailed report
vcts run --provider vsphere-prod --output-dir ./test-results --verbose
</code></pre>
<h4 id="list"><a class="header" href="#list">list</a></h4>
<p>List available test suites and tests.</p>
<pre><code class="language-bash"># List all test suites
vcts list suites

# List tests in a suite
vcts list tests --suite core

# List supported providers
vcts list providers
</code></pre>
<h4 id="validate"><a class="header" href="#validate">validate</a></h4>
<p>Validate test configuration.</p>
<pre><code class="language-bash"># Validate configuration file
vcts validate --config test-config.yaml

# Validate provider setup
vcts validate --provider vsphere-prod
</code></pre>
<h3 id="test-suites"><a class="header" href="#test-suites">Test Suites</a></h3>
<h4 id="core-suite"><a class="header" href="#core-suite">Core Suite</a></h4>
<ul>
<li>Basic VM lifecycle (create, start, stop, delete)</li>
<li>Provider connectivity and authentication</li>
<li>Resource allocation and management</li>
</ul>
<h4 id="storage-suite"><a class="header" href="#storage-suite">Storage Suite</a></h4>
<ul>
<li>Disk creation and attachment</li>
<li>Volume expansion operations</li>
<li>Storage pool management</li>
</ul>
<h4 id="network-suite"><a class="header" href="#network-suite">Network Suite</a></h4>
<ul>
<li>Network interface management</li>
<li>IP address allocation</li>
<li>Network connectivity tests</li>
</ul>
<h4 id="snapshot-suite"><a class="header" href="#snapshot-suite">Snapshot Suite</a></h4>
<ul>
<li>Snapshot creation and deletion</li>
<li>Snapshot restoration</li>
<li>Memory state preservation</li>
</ul>
<h4 id="performance-suite"><a class="header" href="#performance-suite">Performance Suite</a></h4>
<ul>
<li>VM creation performance</li>
<li>Resource utilization benchmarks</li>
<li>Concurrent operation handling</li>
</ul>
<h3 id="test-configuration"><a class="header" href="#test-configuration">Test Configuration</a></h3>
<p>Create <code>test-config.yaml</code>:</p>
<pre><code class="language-yaml">provider:
  name: "vsphere-prod"
  type: "vsphere"
  
tests:
  core:
    enabled: true
    timeout: "15m"
  storage:
    enabled: true
    testDiskSize: "10Gi"
  network:
    enabled: false  # Skip network tests
    
resources:
  vmClass: "test-small"
  vmImage: "ubuntu-22-04"
  
cleanup:
  enabled: true
  timeout: "10m"
</code></pre>
<h2 id="vrtg-provider-1"><a class="header" href="#vrtg-provider-1">vrtg-provider</a></h2>
<p>Development toolkit for creating and maintaining VirtRigaud providers.</p>
<h3 id="global-flags-6"><a class="header" href="#global-flags-6">Global Flags</a></h3>
<pre><code class="language-bash">--verbose            Enable verbose output
-h, --help          Help for vrtg-provider
</code></pre>
<h3 id="commands-6"><a class="header" href="#commands-6">Commands</a></h3>
<h4 id="init"><a class="header" href="#init">init</a></h4>
<p>Initialize a new provider project.</p>
<pre><code class="language-bash"># Create a new provider
vrtg-provider init --name hyperv --type hyperv --output ./hyperv-provider

# Create with custom options
vrtg-provider init --name aws-ec2 --type aws \
  --capabilities snapshots,linked-clones \
  --output ./aws-provider
</code></pre>
<p><strong>Options:</strong></p>
<ul>
<li><code>--name</code>: Provider name</li>
<li><code>--type</code>: Provider type</li>
<li><code>--capabilities</code>: Comma-separated capabilities list</li>
<li><code>--output</code>: Output directory</li>
<li><code>--remote</code>: Generate remote provider (default: true)</li>
</ul>
<h4 id="generate"><a class="header" href="#generate">generate</a></h4>
<p>Generate code for provider components.</p>
<pre><code class="language-bash"># Generate API types
vrtg-provider generate api --provider-type vsphere

# Generate client code
vrtg-provider generate client --provider-type vsphere --api-version v1

# Generate test suite
vrtg-provider generate tests --provider-type vsphere

# Generate documentation
vrtg-provider generate docs --provider-type vsphere
</code></pre>
<h4 id="verify"><a class="header" href="#verify">verify</a></h4>
<p>Verify provider implementation.</p>
<pre><code class="language-bash"># Verify provider structure
vrtg-provider verify structure --path ./my-provider

# Verify capabilities
vrtg-provider verify capabilities --path ./my-provider

# Verify API compatibility
vrtg-provider verify api --path ./my-provider --api-version v1beta1
</code></pre>
<h4 id="publish"><a class="header" href="#publish">publish</a></h4>
<p>Publish provider artifacts.</p>
<pre><code class="language-bash"># Build and publish provider image
vrtg-provider publish --path ./my-provider --registry ghcr.io/myorg

# Publish with specific tag
vrtg-provider publish --path ./my-provider --tag v1.0.0

# Dry run publication
vrtg-provider publish --path ./my-provider --dry-run
</code></pre>
<h3 id="provider-structure"><a class="header" href="#provider-structure">Provider Structure</a></h3>
<pre><code>my-provider/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îî‚îÄ‚îÄ provider-mytype/
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îî‚îÄ‚îÄ main.go
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îî‚îÄ‚îÄ provider/
‚îÇ       ‚îú‚îÄ‚îÄ provider.go
‚îÇ       ‚îú‚îÄ‚îÄ capabilities.go
‚îÇ       ‚îî‚îÄ‚îÄ provider_test.go
‚îú‚îÄ‚îÄ deploy/
‚îÇ   ‚îú‚îÄ‚îÄ provider.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ deployment.yaml
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îî‚îÄ‚îÄ Makefile
</code></pre>
<h2 id="virtrigaud-loadgen-1"><a class="header" href="#virtrigaud-loadgen-1">virtrigaud-loadgen</a></h2>
<p>Load testing and performance benchmarking tool for VirtRigaud providers.</p>
<h3 id="global-flags-7"><a class="header" href="#global-flags-7">Global Flags</a></h3>
<pre><code class="language-bash">--kubeconfig string   Path to kubeconfig file
--namespace string    Test namespace (default: "loadgen")
--output-dir string   Output directory for results
--config-file string  Load generation configuration file
--dry-run            Show what would be created without executing
--verbose            Verbose output
-h, --help          Help for virtrigaud-loadgen
</code></pre>
<h3 id="commands-7"><a class="header" href="#commands-7">Commands</a></h3>
<h4 id="run-1"><a class="header" href="#run-1">run</a></h4>
<p>Execute load generation scenarios.</p>
<pre><code class="language-bash"># Run default load test
virtrigaud-loadgen run --config loadtest.yaml

# Run with custom settings
virtrigaud-loadgen run --config loadtest.yaml --workers 50 --duration 10m

# Run specific scenario
virtrigaud-loadgen run --scenario vm-creation --vms 100

# Generate performance report
virtrigaud-loadgen run --config loadtest.yaml --output-dir ./perf-results
</code></pre>
<h4 id="scenarios"><a class="header" href="#scenarios">scenarios</a></h4>
<p>Manage load testing scenarios.</p>
<pre><code class="language-bash"># List available scenarios
virtrigaud-loadgen scenarios list

# Show scenario details
virtrigaud-loadgen scenarios get vm-lifecycle

# Validate scenario configuration
virtrigaud-loadgen scenarios validate --config custom-scenario.yaml
</code></pre>
<h4 id="analyze"><a class="header" href="#analyze">analyze</a></h4>
<p>Analyze load test results.</p>
<pre><code class="language-bash"># Generate performance report
virtrigaud-loadgen analyze --input ./perf-results

# Compare test runs
virtrigaud-loadgen analyze --compare run1.csv,run2.csv

# Generate charts
virtrigaud-loadgen analyze --input ./perf-results --charts
</code></pre>
<h3 id="load-test-configuration"><a class="header" href="#load-test-configuration">Load Test Configuration</a></h3>
<p>Create <code>loadtest.yaml</code>:</p>
<pre><code class="language-yaml">metadata:
  name: "vm-creation-load-test"
  description: "Test VM creation performance"

scenarios:
  - name: "vm-creation"
    type: "vm-lifecycle"
    workers: 20
    duration: "5m"
    resources:
      vmClass: "small"
      vmImage: "ubuntu-22-04"
      provider: "vsphere-prod"
    
  - name: "vm-scaling"
    type: "vmset-scaling"
    workers: 5
    iterations: 10
    scaling:
      min: 1
      max: 50
      step: 5

providers:
  - name: "vsphere-prod"
    type: "vsphere"
  - name: "libvirt-test"
    type: "libvirt"

output:
  format: ["csv", "json"]
  metrics: ["latency", "throughput", "errors"]
  
cleanup:
  enabled: true
  timeout: "15m"
</code></pre>
<h3 id="performance-scenarios"><a class="header" href="#performance-scenarios">Performance Scenarios</a></h3>
<h4 id="vm-lifecycle-1"><a class="header" href="#vm-lifecycle-1">VM Lifecycle</a></h4>
<ul>
<li>Create, start, stop, delete operations</li>
<li>Measures end-to-end VM management performance</li>
</ul>
<h4 id="burst-creation"><a class="header" href="#burst-creation">Burst Creation</a></h4>
<ul>
<li>Rapid VM creation under load</li>
<li>Tests provider scaling capabilities</li>
</ul>
<h4 id="vmset-scaling"><a class="header" href="#vmset-scaling">VMSet Scaling</a></h4>
<ul>
<li>Scale VMSets up and down</li>
<li>Measures horizontal scaling performance</li>
</ul>
<h4 id="provider-stress"><a class="header" href="#provider-stress">Provider Stress</a></h4>
<ul>
<li>High concurrent operations</li>
<li>Tests provider reliability under stress</li>
</ul>
<h3 id="results-analysis"><a class="header" href="#results-analysis">Results Analysis</a></h3>
<p>Load test results include:</p>
<ul>
<li><strong>Latency metrics</strong>: P50, P95, P99 response times</li>
<li><strong>Throughput</strong>: Operations per second</li>
<li><strong>Error rates</strong>: Failed operations percentage</li>
<li><strong>Resource usage</strong>: CPU, memory, network utilization</li>
<li><strong>Provider metrics</strong>: API call statistics</li>
</ul>
<p>Example output:</p>
<pre><code class="language-csv">timestamp,scenario,operation,latency_ms,status,provider
2025-01-15T10:00:01Z,vm-creation,create,2500,success,vsphere-prod
2025-01-15T10:00:03Z,vm-creation,create,2800,success,vsphere-prod
2025-01-15T10:00:05Z,vm-creation,create,failed,timeout,vsphere-prod
</code></pre>
<h2 id="best-practices-10"><a class="header" href="#best-practices-10">Best Practices</a></h2>
<h3 id="using-vrtg"><a class="header" href="#using-vrtg">Using vrtg</a></h3>
<ol>
<li><strong>Use namespaces</strong> to organize resources</li>
<li><strong>Set timeouts</strong> appropriately for your environment</li>
<li><strong>Use dry-run</strong> options for validation before execution</li>
<li><strong>Monitor operations</strong> with watch commands</li>
</ol>
<h3 id="testing-with-vcts"><a class="header" href="#testing-with-vcts">Testing with vcts</a></h3>
<ol>
<li><strong>Run core tests first</strong> to validate basic functionality</li>
<li><strong>Use separate namespaces</strong> for different test runs</li>
<li><strong>Clean up resources</strong> after testing</li>
<li><strong>Document test results</strong> for compliance tracking</li>
</ol>
<h3 id="developing-with-vrtg-provider"><a class="header" href="#developing-with-vrtg-provider">Developing with vrtg-provider</a></h3>
<ol>
<li><strong>Start with init</strong> to create proper structure</li>
<li><strong>Implement core capabilities</strong> before advanced features</li>
<li><strong>Test thoroughly</strong> with vcts before publishing</li>
<li><strong>Follow naming conventions</strong> for consistency</li>
</ol>
<h3 id="load-testing-with-virtrigaud-loadgen"><a class="header" href="#load-testing-with-virtrigaud-loadgen">Load Testing with virtrigaud-loadgen</a></h3>
<ol>
<li><strong>Start small</strong> and gradually increase load</li>
<li><strong>Monitor system resources</strong> during tests</li>
<li><strong>Use realistic scenarios</strong> that match production workloads</li>
<li><strong>Analyze results</strong> to identify bottlenecks</li>
</ol>
<h2 id="support-4"><a class="header" href="#support-4">Support</a></h2>
<ul>
<li><strong>Documentation</strong>: <a href="https://projectbeskar.github.io/virtrigaud/">VirtRigaud Docs</a></li>
<li><strong>Issues</strong>: <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub Issues</a></li>
<li><strong>Discussions</strong>: <a href="https://github.com/projectbeskar/virtrigaud/discussions">GitHub Discussions</a></li>
<li><strong>Community</strong>: <a href="https://discord.gg/projectbeskar">Discord</a></li>
</ul>
<h2 id="version-information-1"><a class="header" href="#version-information-1">Version Information</a></h2>
<p>This documentation covers VirtRigaud CLI tools v0.2.0.</p>
<p>For older versions, see the <a href="https://github.com/projectbeskar/virtrigaud/releases">releases page</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metrics-catalog-1"><a class="header" href="#metrics-catalog-1">Metrics Catalog</a></h1>
<p>VirtRigaud exposes comprehensive metrics for monitoring and observability. All metrics are available at the <code>/metrics</code> endpoint on port 8080.</p>
<h2 id="manager-metrics-1"><a class="header" href="#manager-metrics-1">Manager Metrics</a></h2>
<h3 id="reconciliation-metrics"><a class="header" href="#reconciliation-metrics">Reconciliation Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_manager_reconcile_total</code></td><td>Counter</td><td><code>kind</code>, <code>outcome</code></td><td>Total number of reconcile loops</td></tr>
<tr><td><code>virtrigaud_manager_reconcile_duration_seconds</code></td><td>Histogram</td><td><code>kind</code></td><td>Time spent in reconcile loops</td></tr>
<tr><td><code>virtrigaud_queue_depth</code></td><td>Gauge</td><td><code>kind</code></td><td>Current queue depth for each resource kind</td></tr>
</tbody></table>
</div>
<h3 id="vm-operation-metrics-1"><a class="header" href="#vm-operation-metrics-1">VM Operation Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_vm_operations_total</code></td><td>Counter</td><td><code>operation</code>, <code>provider_type</code>, <code>provider</code>, <code>outcome</code></td><td>Total VM operations</td></tr>
<tr><td><code>virtrigaud_vm_reconfigure_total</code></td><td>Counter</td><td><code>provider_type</code>, <code>outcome</code></td><td>Total VM reconfiguration operations</td></tr>
<tr><td><code>virtrigaud_vm_snapshot_total</code></td><td>Counter</td><td><code>action</code>, <code>provider_type</code>, <code>outcome</code></td><td>Total VM snapshot operations</td></tr>
<tr><td><code>virtrigaud_vm_clone_total</code></td><td>Counter</td><td><code>linked</code>, <code>provider_type</code>, <code>outcome</code></td><td>Total VM clone operations</td></tr>
<tr><td><code>virtrigaud_vm_image_prepare_total</code></td><td>Counter</td><td><code>provider_type</code>, <code>outcome</code></td><td>Total VM image preparation operations</td></tr>
</tbody></table>
</div>
<h3 id="build-information"><a class="header" href="#build-information">Build Information</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_build_info</code></td><td>Gauge</td><td><code>version</code>, <code>git_sha</code>, <code>go_version</code></td><td>Build information</td></tr>
</tbody></table>
</div>
<h2 id="provider-metrics-1"><a class="header" href="#provider-metrics-1">Provider Metrics</a></h2>
<h3 id="grpc-metrics"><a class="header" href="#grpc-metrics">gRPC Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_provider_rpc_requests_total</code></td><td>Counter</td><td><code>provider_type</code>, <code>method</code>, <code>code</code></td><td>Total gRPC requests</td></tr>
<tr><td><code>virtrigaud_provider_rpc_latency_seconds</code></td><td>Histogram</td><td><code>provider_type</code>, <code>method</code></td><td>gRPC request latency</td></tr>
<tr><td><code>virtrigaud_provider_tasks_inflight</code></td><td>Gauge</td><td><code>provider_type</code>, <code>provider</code></td><td>Number of inflight tasks</td></tr>
</tbody></table>
</div>
<h3 id="provider-specific-metrics"><a class="header" href="#provider-specific-metrics">Provider-Specific Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_ip_discovery_duration_seconds</code></td><td>Histogram</td><td><code>provider_type</code></td><td>Time to discover VM IP addresses</td></tr>
</tbody></table>
</div>
<h3 id="error-metrics-1"><a class="header" href="#error-metrics-1">Error Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_errors_total</code></td><td>Counter</td><td><code>reason</code>, <code>component</code></td><td>Total errors by reason and component</td></tr>
</tbody></table>
</div>
<h2 id="label-definitions"><a class="header" href="#label-definitions">Label Definitions</a></h2>
<h3 id="common-labels"><a class="header" href="#common-labels">Common Labels</a></h3>
<ul>
<li><code>provider_type</code>: The type of provider (<code>vsphere</code>, <code>libvirt</code>)</li>
<li><code>provider</code>: The name of the provider instance</li>
<li><code>outcome</code>: The result of an operation (<code>success</code>, <code>failure</code>, <code>error</code>)</li>
<li><code>kind</code>: The Kubernetes resource kind (<code>VirtualMachine</code>, <code>VMClass</code>, etc.)</li>
<li><code>component</code>: The component generating the metric (<code>manager</code>, <code>provider</code>)</li>
</ul>
<h3 id="operation-specific-labels"><a class="header" href="#operation-specific-labels">Operation-Specific Labels</a></h3>
<ul>
<li><code>operation</code>: Type of VM operation (<code>Create</code>, <code>Delete</code>, <code>Power</code>, <code>Describe</code>, <code>Reconfigure</code>)</li>
<li><code>method</code>: gRPC method name (<code>CreateVM</code>, <code>DeleteVM</code>, <code>PowerVM</code>, etc.)</li>
<li><code>code</code>: gRPC status code (<code>OK</code>, <code>INVALID_ARGUMENT</code>, <code>DEADLINE_EXCEEDED</code>, etc.)</li>
<li><code>action</code>: Snapshot action (<code>create</code>, <code>delete</code>, <code>revert</code>)</li>
<li><code>linked</code>: Whether a clone is linked (<code>true</code>, <code>false</code>)</li>
<li><code>reason</code>: Error reason (<code>ConnectionFailed</code>, <code>AuthenticationError</code>, etc.)</li>
</ul>
<h2 id="histogram-buckets"><a class="header" href="#histogram-buckets">Histogram Buckets</a></h2>
<p>Duration histograms use the following buckets (in seconds):</p>
<pre><code>0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30, 60, 120, 300
</code></pre>
<h2 id="example-queries"><a class="header" href="#example-queries">Example Queries</a></h2>
<h3 id="prometheus-queries"><a class="header" href="#prometheus-queries">Prometheus Queries</a></h3>
<h4 id="error-rate"><a class="header" href="#error-rate">Error Rate</a></h4>
<pre><code class="language-promql"># Overall error rate
rate(virtrigaud_vm_operations_total{outcome="failure"}[5m]) /
rate(virtrigaud_vm_operations_total[5m])

# Provider-specific error rate
rate(virtrigaud_provider_rpc_requests_total{code!="OK"}[5m]) /
rate(virtrigaud_provider_rpc_requests_total[5m])
</code></pre>
<h4 id="latency"><a class="header" href="#latency">Latency</a></h4>
<pre><code class="language-promql"># 95th percentile VM creation time
histogram_quantile(0.95, 
  rate(virtrigaud_vm_operations_duration_seconds_bucket{operation="Create"}[5m])
)

# gRPC request latency by method
histogram_quantile(0.95,
  rate(virtrigaud_provider_rpc_latency_seconds_bucket[5m])
) by (method)
</code></pre>
<h4 id="throughput"><a class="header" href="#throughput">Throughput</a></h4>
<pre><code class="language-promql"># VM operations per second
rate(virtrigaud_vm_operations_total[5m])

# Operations by provider
rate(virtrigaud_vm_operations_total[5m]) by (provider_type, provider)
</code></pre>
<h4 id="queue-depth"><a class="header" href="#queue-depth">Queue Depth</a></h4>
<pre><code class="language-promql"># Current queue depth
virtrigaud_queue_depth

# Average queue depth over time
avg_over_time(virtrigaud_queue_depth[5m])
</code></pre>
<h4 id="inflight-tasks"><a class="header" href="#inflight-tasks">Inflight Tasks</a></h4>
<pre><code class="language-promql"># Current inflight tasks
virtrigaud_provider_tasks_inflight

# Inflight tasks by provider
virtrigaud_provider_tasks_inflight by (provider_type, provider)
</code></pre>
<h3 id="grafana-dashboard-queries"><a class="header" href="#grafana-dashboard-queries">Grafana Dashboard Queries</a></h3>
<h4 id="vm-creation-success-rate-panel"><a class="header" href="#vm-creation-success-rate-panel">VM Creation Success Rate Panel</a></h4>
<pre><code class="language-promql">sum(rate(virtrigaud_vm_operations_total{operation="Create",outcome="success"}[5m])) /
sum(rate(virtrigaud_vm_operations_total{operation="Create"}[5m])) * 100
</code></pre>
<h4 id="provider-health-panel"><a class="header" href="#provider-health-panel">Provider Health Panel</a></h4>
<pre><code class="language-promql">up{job="virtrigaud-provider"}
</code></pre>
<h4 id="error-rate-by-provider-panel"><a class="header" href="#error-rate-by-provider-panel">Error Rate by Provider Panel</a></h4>
<pre><code class="language-promql">sum(rate(virtrigaud_errors_total[5m])) by (component, provider_type)
</code></pre>
<h2 id="servicemonitor-configuration"><a class="header" href="#servicemonitor-configuration">ServiceMonitor Configuration</a></h2>
<p>Example ServiceMonitor for Prometheus Operator:</p>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: virtrigaud-manager
  namespace: virtrigaud-system
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud
      app.kubernetes.io/component: manager
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: virtrigaud-providers
  namespace: virtrigaud-system
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: virtrigaud
      app.kubernetes.io/component: provider
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
</code></pre>
<h2 id="alert-rules"><a class="header" href="#alert-rules">Alert Rules</a></h2>
<p>Example PrometheusRule for common alerts:</p>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: virtrigaud-alerts
  namespace: virtrigaud-system
spec:
  groups:
  - name: virtrigaud.rules
    rules:
    - alert: VirtrigaudProviderDown
      expr: up{job="virtrigaud-provider"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Virtrigaud provider is down"
        description: "Provider {{ $labels.instance }} has been down for more than 5 minutes"

    - alert: VirtrigaudHighErrorRate
      expr: |
        rate(virtrigaud_vm_operations_total{outcome="failure"}[5m]) /
        rate(virtrigaud_vm_operations_total[5m]) &gt; 0.1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High error rate in VM operations"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.provider }}"

    - alert: VirtrigaudSlowVMCreation
      expr: |
        histogram_quantile(0.95,
          rate(virtrigaud_vm_operations_duration_seconds_bucket{operation="Create"}[5m])
        ) &gt; 600
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Slow VM creation times"
        description: "95th percentile VM creation time is {{ $value }}s"

    - alert: VirtrigaudQueueBacklog
      expr: virtrigaud_queue_depth &gt; 100
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Queue backlog detected"
        description: "Queue depth for {{ $labels.kind }} is {{ $value }}"
</code></pre>
<h2 id="custom-metrics"><a class="header" href="#custom-metrics">Custom Metrics</a></h2>
<p>Providers can expose additional custom metrics specific to their implementation:</p>
<h3 id="vsphere-provider-metrics"><a class="header" href="#vsphere-provider-metrics">vSphere Provider Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_vsphere_sessions_total</code></td><td>Counter</td><td><code>datacenter</code></td><td>Total vSphere sessions created</td></tr>
<tr><td><code>virtrigaud_vsphere_api_calls_total</code></td><td>Counter</td><td><code>method</code>, <code>datacenter</code></td><td>Total vSphere API calls</td></tr>
</tbody></table>
</div>
<h3 id="libvirt-provider-metrics"><a class="header" href="#libvirt-provider-metrics">Libvirt Provider Metrics</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Metric Name</th><th>Type</th><th>Labels</th><th>Description</th></tr></thead><tbody>
<tr><td><code>virtrigaud_libvirt_connections_total</code></td><td>Counter</td><td><code>host</code></td><td>Total Libvirt connections</td></tr>
<tr><td><code>virtrigaud_libvirt_domains_total</code></td><td>Gauge</td><td><code>host</code>, <code>state</code></td><td>Current number of domains by state</td></tr>
</tbody></table>
</div>
<h2 id="metric-collection-best-practices"><a class="header" href="#metric-collection-best-practices">Metric Collection Best Practices</a></h2>
<ol>
<li><strong>Scrape Interval</strong>: Use 30s interval for most metrics</li>
<li><strong>Retention</strong>: Keep metrics for at least 30 days for trending</li>
<li><strong>High Cardinality</strong>: Be careful with VM names and IDs in labels</li>
<li><strong>Aggregation</strong>: Use recording rules for frequently queried metrics</li>
<li><strong>Alerting</strong>: Set up alerts for SLI/SLO violations</li>
</ol>
<h2 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h2>
<ul>
<li><a href="api-reference/../admin-guides/monitoring.html">Monitoring Guide</a></li>
<li><a href="api-reference/../admin-guides/dashboards.html">Grafana Dashboards</a></li>
<li><a href="api-reference/../admin-guides/alerts.html">Alert Runbooks</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provider-catalog"><a class="header" href="#provider-catalog">Provider Catalog</a></h1>
<p><em>Last updated: 2025-08-26T14:30:00Z</em></p>
<p>The VirtRigaud Provider Catalog lists all verified and community providers available for the VirtRigaud virtualization management platform. All providers in this catalog have been tested for conformance and compatibility.</p>
<h2 id="provider-overview"><a class="header" href="#provider-overview">Provider Overview</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Description</th><th>Capabilities</th><th>Conformance</th><th>Maintainer</th><th>License</th></tr></thead><tbody>
<tr><td><strong>Mock Provider</strong></td><td>A mock provider for testing and demonstrations</td><td>core, snapshot, clone, image-prepare, advanced</td><td><img src="https://img.shields.io/badge/conformance-pass-green" alt="Conformance" /></td><td>virtrigaud@projectbeskar.com</td><td>Apache-2.0</td></tr>
<tr><td><strong>vSphere Provider</strong></td><td>VMware vSphere provider for VirtRigaud</td><td>core, snapshot, clone, advanced</td><td><img src="https://img.shields.io/badge/conformance-pass-green" alt="Conformance" /></td><td>virtrigaud@projectbeskar.com</td><td>Apache-2.0</td></tr>
<tr><td><strong>Libvirt Provider</strong></td><td>Libvirt/KVM provider for VirtRigaud</td><td>core, snapshot, clone</td><td><img src="https://img.shields.io/badge/conformance-partial-yellow" alt="Conformance" /></td><td>virtrigaud@projectbeskar.com</td><td>Apache-2.0</td></tr>
</tbody></table>
</div>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<h3 id="installing-a-provider"><a class="header" href="#installing-a-provider">Installing a Provider</a></h3>
<p>To install a provider in your Kubernetes cluster, use the VirtRigaud provider runtime Helm chart:</p>
<pre><code class="language-bash"># Add the VirtRigaud Helm repository
helm repo add virtrigaud https://projectbeskar.github.io/virtrigaud
helm repo update

# Install a provider using the runtime chart
helm install my-vsphere-provider virtrigaud/virtrigaud-provider-runtime \
  --namespace vsphere-providers \
  --create-namespace \
  --set image.repository=ghcr.io/projectbeskar/virtrigaud/provider-vsphere \
  --set image.tag=0.1.1 \
  --set env[0].name=VSPHERE_SERVER \
  --set env[0].value=vcenter.example.com
</code></pre>
<h3 id="provider-discovery"><a class="header" href="#provider-discovery">Provider Discovery</a></h3>
<p>Once installed, providers automatically register with the VirtRigaud manager. You can list available providers:</p>
<pre><code class="language-bash">kubectl get providers -n virtrigaud-system
</code></pre>
<h2 id="provider-details"><a class="header" href="#provider-details">Provider Details</a></h2>
<h3 id="mock-provider"><a class="header" href="#mock-provider">Mock Provider</a></h3>
<ul>
<li><strong>Image:</strong> <code>ghcr.io/projectbeskar/virtrigaud/provider-mock:0.1.1</code></li>
<li><strong>Repository:</strong> <a href="https://github.com/projectbeskar/virtrigaud">https://github.com/projectbeskar/virtrigaud</a></li>
<li><strong>Maturity:</strong> stable</li>
<li><strong>Tags:</strong> testing, development, demo</li>
<li><strong>Documentation:</strong> <a href="https://projectbeskar.github.io/virtrigaud/providers/mock/">https://projectbeskar.github.io/virtrigaud/providers/mock/</a></li>
</ul>
<p>The mock provider is perfect for:</p>
<ul>
<li>Testing VirtRigaud functionality</li>
<li>Development and CI/CD pipelines</li>
<li>Learning provider concepts</li>
<li>Demonstrating VirtRigaud capabilities</li>
</ul>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">helm install mock-provider virtrigaud/virtrigaud-provider-runtime \
  --namespace development \
  --create-namespace \
  --set image.repository=ghcr.io/projectbeskar/virtrigaud/provider-mock \
  --set image.tag=0.1.1 \
  --set env[0].name=LOG_LEVEL \
  --set env[0].value=debug
</code></pre>
<h3 id="vsphere-provider-6"><a class="header" href="#vsphere-provider-6">vSphere Provider</a></h3>
<ul>
<li><strong>Image:</strong> <code>ghcr.io/projectbeskar/virtrigaud/provider-vsphere:0.1.1</code></li>
<li><strong>Repository:</strong> <a href="https://github.com/projectbeskar/virtrigaud">https://github.com/projectbeskar/virtrigaud</a></li>
<li><strong>Maturity:</strong> beta</li>
<li><strong>Tags:</strong> vmware, vsphere, enterprise</li>
<li><strong>Documentation:</strong> <a href="https://projectbeskar.github.io/virtrigaud/providers/vsphere/">https://projectbeskar.github.io/virtrigaud/providers/vsphere/</a></li>
</ul>
<p>The vSphere provider enables VirtRigaud to manage VMware vSphere environments, including:</p>
<ul>
<li>VM lifecycle management (create, update, delete)</li>
<li>Power operations (on, off, restart, suspend)</li>
<li>Snapshot management</li>
<li>VM cloning and templates</li>
<li>Resource allocation and configuration</li>
</ul>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>VMware vSphere 6.7 or later</li>
<li>vCenter Server credentials</li>
<li>Network connectivity to vCenter API</li>
</ul>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash"># Create secret for vSphere credentials
kubectl create secret generic vsphere-credentials \
  --namespace vsphere-providers \
  --from-literal=username=your-username \
  --from-literal=password=your-password

# Install provider
helm install vsphere-provider virtrigaud/virtrigaud-provider-runtime \
  --namespace vsphere-providers \
  --create-namespace \
  --set image.repository=ghcr.io/projectbeskar/virtrigaud/provider-vsphere \
  --set image.tag=0.1.1 \
  --set env[0].name=VSPHERE_SERVER \
  --set env[0].value=vcenter.example.com \
  --set env[1].name=VSPHERE_USERNAME \
  --set env[1].valueFrom.secretKeyRef.name=vsphere-credentials \
  --set env[1].valueFrom.secretKeyRef.key=username \
  --set env[2].name=VSPHERE_PASSWORD \
  --set env[2].valueFrom.secretKeyRef.name=vsphere-credentials \
  --set env[2].valueFrom.secretKeyRef.key=password
</code></pre>
<h3 id="libvirt-provider-5"><a class="header" href="#libvirt-provider-5">Libvirt Provider</a></h3>
<ul>
<li><strong>Image:</strong> <code>ghcr.io/projectbeskar/virtrigaud/provider-libvirt:0.1.1</code></li>
<li><strong>Repository:</strong> <a href="https://github.com/projectbeskar/virtrigaud">https://github.com/projectbeskar/virtrigaud</a></li>
<li><strong>Maturity:</strong> beta</li>
<li><strong>Tags:</strong> libvirt, kvm, qemu, open-source</li>
<li><strong>Documentation:</strong> <a href="https://projectbeskar.github.io/virtrigaud/providers/libvirt/">https://projectbeskar.github.io/virtrigaud/providers/libvirt/</a></li>
</ul>
<p>The libvirt provider manages KVM/QEMU virtual machines through libvirt, supporting:</p>
<ul>
<li>VM lifecycle management</li>
<li>Power state control</li>
<li>Snapshot operations</li>
<li>Basic cloning capabilities</li>
<li>Local and remote libvirt connections</li>
</ul>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Libvirt daemon running on target hosts</li>
<li>SSH access for remote connections</li>
<li>Shared storage for multi-host deployments</li>
</ul>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">helm install libvirt-provider virtrigaud/virtrigaud-provider-runtime \
  --namespace libvirt-providers \
  --create-namespace \
  --set image.repository=ghcr.io/projectbeskar/virtrigaud/provider-libvirt \
  --set image.tag=0.1.1 \
  --set env[0].name=LIBVIRT_URI \
  --set env[0].value=qemu:///system \
  --set securityContext.runAsUser=0 \
  --set podSecurityContext.runAsUser=0
</code></pre>
<h2 id="capability-profiles"><a class="header" href="#capability-profiles">Capability Profiles</a></h2>
<p>VirtRigaud defines several capability profiles that providers can implement:</p>
<h3 id="core-profile"><a class="header" href="#core-profile">Core Profile</a></h3>
<p><strong>Required for all providers</strong></p>
<ul>
<li><code>vm.create</code> - Create virtual machines</li>
<li><code>vm.read</code> - Get virtual machine information</li>
<li><code>vm.update</code> - Update virtual machine configuration</li>
<li><code>vm.delete</code> - Delete virtual machines</li>
<li><code>vm.power</code> - Control power state (on/off/restart)</li>
<li><code>vm.list</code> - List virtual machines</li>
</ul>
<h3 id="snapshot-profile"><a class="header" href="#snapshot-profile">Snapshot Profile</a></h3>
<p><strong>Optional - for providers supporting VM snapshots</strong></p>
<ul>
<li><code>vm.snapshot.create</code> - Create VM snapshots</li>
<li><code>vm.snapshot.list</code> - List VM snapshots</li>
<li><code>vm.snapshot.delete</code> - Delete VM snapshots</li>
<li><code>vm.snapshot.restore</code> - Restore VM from snapshot</li>
</ul>
<h3 id="clone-profile"><a class="header" href="#clone-profile">Clone Profile</a></h3>
<p><strong>Optional - for providers supporting VM cloning</strong></p>
<ul>
<li><code>vm.clone</code> - Clone virtual machines</li>
<li><code>vm.template</code> - Create and manage VM templates</li>
</ul>
<h3 id="image-prepare-profile"><a class="header" href="#image-prepare-profile">Image Prepare Profile</a></h3>
<p><strong>Optional - for providers with image management</strong></p>
<ul>
<li><code>image.prepare</code> - Prepare VM images</li>
<li><code>image.list</code> - List available images</li>
<li><code>image.upload</code> - Upload custom images</li>
</ul>
<h3 id="advanced-profile"><a class="header" href="#advanced-profile">Advanced Profile</a></h3>
<p><strong>Optional - for advanced provider features</strong></p>
<ul>
<li><code>vm.migrate</code> - Live migrate VMs between hosts</li>
<li><code>vm.resize</code> - Dynamic resource allocation</li>
<li><code>vm.backup</code> - Backup and restore operations</li>
<li><code>vm.monitoring</code> - Advanced monitoring and metrics</li>
</ul>
<h2 id="contributing-a-provider"><a class="header" href="#contributing-a-provider">Contributing a Provider</a></h2>
<p>Want to add your provider to the catalog? Follow these steps:</p>
<h3 id="1-develop-your-provider"><a class="header" href="#1-develop-your-provider">1. Develop Your Provider</a></h3>
<p>Use the <a href="./providers/tutorial.html">Provider Developer Tutorial</a> to create your provider using the VirtRigaud SDK.</p>
<h3 id="2-ensure-conformance"><a class="header" href="#2-ensure-conformance">2. Ensure Conformance</a></h3>
<p>Run the VirtRigaud Conformance Test Suite (VCTS) to verify your provider meets the requirements:</p>
<pre><code class="language-bash"># Install the VCTS tool
go install github.com/projectbeskar/virtrigaud/cmd/vcts@latest

# Run conformance tests
vcts run --provider-endpoint=localhost:9443 --profile=core
</code></pre>
<h3 id="3-publish-to-catalog"><a class="header" href="#3-publish-to-catalog">3. Publish to Catalog</a></h3>
<p>Use the <code>vrtg-provider publish</code> command to submit your provider:</p>
<pre><code class="language-bash">vrtg-provider publish \
  --name your-provider \
  --image ghcr.io/yourorg/your-provider \
  --tag v1.0.0 \
  --repo https://github.com/yourorg/your-provider \
  --maintainer your-email@example.com \
  --license Apache-2.0
</code></pre>
<p>This will:</p>
<ol>
<li>Run conformance tests</li>
<li>Generate provider badges</li>
<li>Create a catalog entry</li>
<li>Open a pull request to add your provider</li>
</ol>
<h3 id="4-catalog-requirements"><a class="header" href="#4-catalog-requirements">4. Catalog Requirements</a></h3>
<p>To be included in the catalog, providers must:</p>
<ul>
<li>‚úÖ Pass VCTS core profile tests</li>
<li>‚úÖ Include comprehensive documentation</li>
<li>‚úÖ Provide Helm chart for deployment</li>
<li>‚úÖ Follow security best practices</li>
<li>‚úÖ Include proper error handling</li>
<li>‚úÖ Support health checks and metrics</li>
<li>‚úÖ Have active maintenance and support</li>
</ul>
<h2 id="provider-support-matrix"><a class="header" href="#provider-support-matrix">Provider Support Matrix</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Kubernetes</th><th>VirtRigaud</th><th>Go Version</th><th>Platforms</th></tr></thead><tbody>
<tr><td>Mock</td><td>1.25+</td><td>0.1.0+</td><td>1.23+</td><td>linux/amd64, linux/arm64</td></tr>
<tr><td>vSphere</td><td>1.25+</td><td>0.1.0+</td><td>1.23+</td><td>linux/amd64, linux/arm64</td></tr>
<tr><td>Libvirt</td><td>1.25+</td><td>0.1.0+</td><td>1.23+</td><td>linux/amd64</td></tr>
</tbody></table>
</div>
<h2 id="community-and-support"><a class="header" href="#community-and-support">Community and Support</a></h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://projectbeskar.github.io/virtrigaud/providers/">VirtRigaud Provider Docs</a></li>
<li><strong>Issues:</strong> <a href="https://github.com/projectbeskar/virtrigaud/issues">GitHub Issues</a></li>
<li><strong>Discussions:</strong> <a href="https://github.com/projectbeskar/virtrigaud/discussions">GitHub Discussions</a></li>
<li><strong>Slack:</strong> <a href="https://virtrigaud.slack.com">VirtRigaud Community</a></li>
</ul>
<h2 id="versioning-and-compatibility"><a class="header" href="#versioning-and-compatibility">Versioning and Compatibility</a></h2>
<p>Providers follow semantic versioning (SemVer) and maintain compatibility with VirtRigaud versions:</p>
<ul>
<li><strong>Major versions</strong> (1.0.0 ‚Üí 2.0.0): Breaking changes to APIs or behavior</li>
<li><strong>Minor versions</strong> (1.0.0 ‚Üí 1.1.0): New features, backward compatible</li>
<li><strong>Patch versions</strong> (1.0.0 ‚Üí 1.0.1): Bug fixes, security updates</li>
</ul>
<p><strong>Compatibility Policy:</strong></p>
<ul>
<li>Current VirtRigaud version supports providers from current major version</li>
<li>Providers should support at least 2 minor versions of VirtRigaud</li>
<li>Breaking changes require migration documentation</li>
</ul>
<h2 id="license-and-legal"><a class="header" href="#license-and-legal">License and Legal</a></h2>
<p>All providers in this catalog are open source and follow the licensing terms specified in their individual repositories. The catalog itself is maintained under the Apache 2.0 license.</p>
<p><strong>Trademark Notice:</strong> VMware and vSphere are trademarks of VMware, Inc. KVM and QEMU are trademarks of their respective owners. All trademarks are the property of their respective owners.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-github-actions-workflows-locally"><a class="header" href="#testing-github-actions-workflows-locally">Testing GitHub Actions Workflows Locally</a></h1>
<p>This guide explains how to test VirtRigaud‚Äôs GitHub Actions workflows locally before pushing to save on GitHub Actions costs and catch issues early.</p>
<h2 id="overview-20"><a class="header" href="#overview-20">Overview</a></h2>
<p>We provide several scripts to test workflows locally:</p>
<div class="table-wrapper"><table><thead><tr><th>Script</th><th>Purpose</th><th>Dependencies</th><th>Use Case</th></tr></thead><tbody>
<tr><td><code>hack/test-workflows-locally.sh</code></td><td><strong>Main orchestrator</strong> using <code>act</code></td><td><code>act</code>, <code>docker</code></td><td>Full GitHub Actions simulation</td></tr>
<tr><td><code>hack/test-lint-locally.sh</code></td><td><strong>Lint workflow</strong> replica</td><td><code>go</code>, <code>golangci-lint</code></td><td>Quick lint testing</td></tr>
<tr><td><code>hack/test-ci-locally.sh</code></td><td><strong>CI workflow</strong> replica</td><td><code>go</code>, <code>helm</code>, system deps</td><td>Comprehensive CI testing</td></tr>
<tr><td><code>hack/test-release-locally.sh</code></td><td><strong>Release workflow</strong> simulation</td><td><code>docker</code>, <code>helm</code>, <code>go</code></td><td>Release preparation testing</td></tr>
<tr><td><code>hack/test-helm-locally.sh</code></td><td><strong>Helm charts</strong> testing</td><td><code>helm</code>, <code>kind</code>, <code>kubectl</code></td><td>Chart validation and deployment</td></tr>
</tbody></table>
</div>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<h3 id="1-setup-first-time"><a class="header" href="#1-setup-first-time">1. Setup (First Time)</a></h3>
<pre><code class="language-bash"># Install dependencies and configure act
./hack/test-workflows-locally.sh setup
</code></pre>
<p>This will:</p>
<ul>
<li>Install <code>act</code> (GitHub Actions runner)</li>
<li>Create <code>.actrc</code> configuration</li>
<li>Create <code>.env.local</code> with environment variables</li>
<li>Create <code>.secrets</code> file (update with real values if needed)</li>
</ul>
<h3 id="2-quick-validation"><a class="header" href="#2-quick-validation">2. Quick Validation</a></h3>
<pre><code class="language-bash"># Fast syntax check of all workflows
./hack/test-workflows-locally.sh smoke
</code></pre>
<h3 id="3-test-individual-workflows"><a class="header" href="#3-test-individual-workflows">3. Test Individual Workflows</a></h3>
<pre><code class="language-bash"># Test lint workflow (fastest)
./hack/test-lint-locally.sh

# Test CI workflow (comprehensive)
./hack/test-ci-locally.sh

# Test Helm charts
./hack/test-helm-locally.sh

# Test release workflow (requires Docker)
./hack/test-release-locally.sh v0.2.0-test
</code></pre>
<h2 id="detailed-usage"><a class="header" href="#detailed-usage">Detailed Usage</a></h2>
<h3 id="lint-testing-test-lint-locallysh"><a class="header" href="#lint-testing-test-lint-locallysh">Lint Testing (<code>test-lint-locally.sh</code>)</a></h3>
<p>Replicates the <code>lint.yml</code> workflow:</p>
<pre><code class="language-bash"># Quick lint test
./hack/test-lint-locally.sh
</code></pre>
<p><strong>What it tests:</strong></p>
<ul>
<li>Go version compatibility</li>
<li>golangci-lint installation and execution</li>
<li>Comprehensive code linting (matching CI exactly)</li>
</ul>
<p><strong>Requirements:</strong></p>
<ul>
<li>Go 1.21+</li>
<li>Internet access (to download golangci-lint if needed)</li>
</ul>
<h3 id="ci-testing-test-ci-locallysh"><a class="header" href="#ci-testing-test-ci-locallysh">CI Testing (<code>test-ci-locally.sh</code>)</a></h3>
<p>Replicates the <code>ci.yml</code> workflow jobs:</p>
<pre><code class="language-bash"># Interactive mode (asks about optional jobs)
./hack/test-ci-locally.sh

# Quick essential tests only
./hack/test-ci-locally.sh quick

# Full CI replication including security scans
./hack/test-ci-locally.sh full
</code></pre>
<p><strong>Jobs tested:</strong></p>
<ul>
<li><strong>test</strong>: Go tests and coverage</li>
<li><strong>lint</strong>: Code linting with golangci-lint</li>
<li><strong>generate</strong>: Code and manifest generation</li>
<li><strong>build</strong>: Binary compilation</li>
<li><strong>build-tools</strong>: CLI tools compilation</li>
<li><strong>helm</strong>: Helm chart validation</li>
<li><strong>security</strong>: Security scanning (optional)</li>
</ul>
<p><strong>Requirements:</strong></p>
<ul>
<li>Go 1.23+</li>
<li>Helm 3.12+</li>
<li>System dependencies (libvirt-dev on Linux)</li>
<li>Python 3 (for YAML validation)</li>
</ul>
<h3 id="release-testing-test-release-locallysh"><a class="header" href="#release-testing-test-release-locallysh">Release Testing (<code>test-release-locally.sh</code>)</a></h3>
<p>Simulates the <code>release.yml</code> workflow:</p>
<pre><code class="language-bash"># Test with default tag
./hack/test-release-locally.sh

# Test with specific tag
./hack/test-release-locally.sh v0.3.0-rc.1

# Skip image building (faster)
./hack/test-release-locally.sh --no-images
</code></pre>
<p><strong>What it tests:</strong></p>
<ul>
<li>Container image building and pushing (to local registry)</li>
<li>Helm chart packaging with version updates</li>
<li>CLI tools building for multiple platforms</li>
<li>Changelog generation</li>
<li>Checksum creation</li>
<li>Container image smoke testing</li>
</ul>
<p><strong>Requirements:</strong></p>
<ul>
<li>Docker</li>
<li>Go 1.23+</li>
<li>Helm 3.12+</li>
<li>Local Docker registry (started automatically)</li>
</ul>
<h3 id="helm-testing-test-helm-locallysh"><a class="header" href="#helm-testing-test-helm-locallysh">Helm Testing (<code>test-helm-locally.sh</code>)</a></h3>
<p>Tests Helm charts with real Kubernetes:</p>
<pre><code class="language-bash"># Full helm test suite
./hack/test-helm-locally.sh

# Individual test types
./hack/test-helm-locally.sh lint     # Chart linting only
./hack/test-helm-locally.sh template # Template rendering only
./hack/test-helm-locally.sh crd      # CRD installation only
./hack/test-helm-locally.sh main     # Main chart installation
./hack/test-helm-locally.sh runtime  # Runtime chart installation

# Cleanup after testing
./hack/test-helm-locally.sh cleanup
</code></pre>
<p><strong>What it tests:</strong></p>
<ul>
<li>Helm chart linting (<code>helm lint</code>)</li>
<li>Template rendering with various value files</li>
<li>CRD installation and functionality</li>
<li>Chart installation in Kind cluster</li>
<li>Pod readiness and basic functionality</li>
</ul>
<p><strong>Requirements:</strong></p>
<ul>
<li>Helm 3.12+</li>
<li>Kind (Kubernetes in Docker)</li>
<li>kubectl</li>
<li>Docker</li>
</ul>
<h3 id="act-based-testing-test-workflows-locallysh"><a class="header" href="#act-based-testing-test-workflows-locallysh">Act-Based Testing (<code>test-workflows-locally.sh</code>)</a></h3>
<p>Uses <code>act</code> to run actual GitHub Actions workflows:</p>
<pre><code class="language-bash"># Setup first time
./hack/test-workflows-locally.sh setup

# Test individual workflows
./hack/test-workflows-locally.sh lint
./hack/test-workflows-locally.sh ci
./hack/test-workflows-locally.sh runtime

# Test all workflows (interactive)
./hack/test-workflows-locally.sh all

# Cleanup
./hack/test-workflows-locally.sh cleanup
</code></pre>
<p><strong>Advanced usage:</strong></p>
<ul>
<li>Supports secrets from <code>.secrets</code> file</li>
<li>Uses reusable containers for speed</li>
<li>Artifact handling with local storage</li>
<li>Environment variable injection</li>
</ul>
<h2 id="configuration-files-1"><a class="header" href="#configuration-files-1">Configuration Files</a></h2>
<h3 id="actrc"><a class="header" href="#actrc"><code>.actrc</code></a></h3>
<pre><code class="language-bash"># Act configuration for GitHub Actions simulation
-P ubuntu-latest=catthehacker/ubuntu:act-22.04
-P ubuntu-22.04=catthehacker/ubuntu:act-22.04  
-P ubuntu-24.04=catthehacker/ubuntu:act-22.04
--container-daemon-socket /var/run/docker.sock
--reuse
--rm
</code></pre>
<h3 id="envlocal"><a class="header" href="#envlocal"><code>.env.local</code></a></h3>
<pre><code class="language-bash"># Local environment variables
GO_VERSION=1.23
GOLANGCI_LINT_VERSION=v1.64.8
REGISTRY=localhost:5000
IMAGE_NAME_PREFIX=virtrigaud
GITHUB_ACTOR=local-user
GITHUB_REPOSITORY=projectbeskar/virtrigaud
# ... more environment variables
</code></pre>
<h3 id="secrets-optional"><a class="header" href="#secrets-optional"><code>.secrets</code> (optional)</a></h3>
<pre><code class="language-bash"># GitHub token for release workflows
GITHUB_TOKEN=your_github_token_here
REGISTRY=localhost:5000
</code></pre>
<h2 id="workflow-specific-notes"><a class="header" href="#workflow-specific-notes">Workflow-Specific Notes</a></h2>
<h3 id="lint-workflow-lintyml"><a class="header" href="#lint-workflow-lintyml">Lint Workflow (<code>lint.yml</code>)</a></h3>
<ul>
<li><strong>Fast</strong>: Usually completes in 1-2 minutes</li>
<li><strong>Requirements</strong>: Minimal (Go + golangci-lint)</li>
<li><strong>Run before</strong>: Every commit</li>
<li><strong>Catches</strong>: Code style, syntax, and simple errors</li>
</ul>
<h3 id="ci-workflow-ciyml"><a class="header" href="#ci-workflow-ciyml">CI Workflow (<code>ci.yml</code>)</a></h3>
<ul>
<li><strong>Comprehensive</strong>: Tests building, testing, security</li>
<li><strong>Duration</strong>: 10-20 minutes for full run</li>
<li><strong>Platform deps</strong>: LibVirt requires Linux for full testing</li>
<li><strong>Run before</strong>: Pull requests and major changes</li>
</ul>
<h3 id="release-workflow-releaseyml"><a class="header" href="#release-workflow-releaseyml">Release Workflow (<code>release.yml</code>)</a></h3>
<ul>
<li><strong>Complex</strong>: Multi-platform builds, signing, publishing</li>
<li><strong>Duration</strong>: 20-30 minutes</li>
<li><strong>Local only</strong>: Uses local registry, no real publishing</li>
<li><strong>Run before</strong>: Creating releases</li>
</ul>
<h3 id="runtime-chart-workflow-runtime-chartyml"><a class="header" href="#runtime-chart-workflow-runtime-chartyml">Runtime Chart Workflow (<code>runtime-chart.yml</code>)</a></h3>
<ul>
<li><strong>Kubernetes focused</strong>: Tests provider runtime charts</li>
<li><strong>Requirements</strong>: Kind cluster</li>
<li><strong>Duration</strong>: 5-10 minutes</li>
<li><strong>Run before</strong>: Chart changes</li>
</ul>
<h2 id="best-practices-11"><a class="header" href="#best-practices-11">Best Practices</a></h2>
<h3 id="daily-development-workflow"><a class="header" href="#daily-development-workflow">Daily Development Workflow</a></h3>
<pre><code class="language-bash"># Before committing
./hack/test-lint-locally.sh

# Before pushing feature branch
./hack/test-ci-locally.sh quick

# Before creating PR
./hack/test-ci-locally.sh full
</code></pre>
<h3 id="pre-release-workflow"><a class="header" href="#pre-release-workflow">Pre-Release Workflow</a></h3>
<pre><code class="language-bash"># Test release preparation
./hack/test-release-locally.sh v0.2.0-rc.1

# Test chart deployment
./hack/test-helm-locally.sh full

# Test with act for full simulation
./hack/test-workflows-locally.sh all
</code></pre>
<h3 id="troubleshooting-16"><a class="header" href="#troubleshooting-16">Troubleshooting</a></h3>
<h4 id="common-issues-11"><a class="header" href="#common-issues-11">Common Issues</a></h4>
<ol>
<li>
<p><strong>Docker permission denied</strong></p>
<pre><code class="language-bash">sudo usermod -aG docker $USER
# Then logout/login
</code></pre>
</li>
<li>
<p><strong>LibVirt dependencies missing</strong></p>
<pre><code class="language-bash"># Ubuntu/Debian
sudo apt-get install libvirt-dev pkg-config

# Skip libvirt tests on non-Linux
./hack/test-ci-locally.sh quick
</code></pre>
</li>
<li>
<p><strong>Kind cluster creation fails</strong></p>
<pre><code class="language-bash"># Clean up and retry
kind delete cluster --name virtrigaud-test
./hack/test-helm-locally.sh
</code></pre>
</li>
<li>
<p><strong>Act fails with container errors</strong></p>
<pre><code class="language-bash"># Clean up act containers
docker ps -a | grep "act-" | awk '{print $1}' | xargs docker rm -f

# Rebuild without cache
./hack/test-workflows-locally.sh cleanup
./hack/test-workflows-locally.sh setup
</code></pre>
</li>
</ol>
<h4 id="debugging-tips"><a class="header" href="#debugging-tips">Debugging Tips</a></h4>
<ul>
<li><strong>Check logs</strong>: All scripts provide detailed logging</li>
<li><strong>Use dry-run</strong>: Most scripts support <code>--help</code> for options</li>
<li><strong>Incremental testing</strong>: Start with <code>lint</code>, then <code>ci quick</code>, then full tests</li>
<li><strong>Docker cleanup</strong>: Regular <code>docker system prune</code> helps with space</li>
</ul>
<h3 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h3>
<ol>
<li><strong>Use quick modes</strong> for daily development</li>
<li><strong>Skip expensive jobs</strong> like security scans during iteration</li>
<li><strong>Reuse Kind clusters</strong> with <code>./hack/test-helm-locally.sh</code></li>
<li><strong>Use local registry</strong> for container testing</li>
<li><strong>Run parallel tests</strong> when possible</li>
</ol>
<h2 id="integration-with-development"><a class="header" href="#integration-with-development">Integration with Development</a></h2>
<h3 id="git-hooks"><a class="header" href="#git-hooks">Git Hooks</a></h3>
<p>Add to <code>.git/hooks/pre-push</code>:</p>
<pre><code class="language-bash">#!/bin/bash
echo "Running local lint check before push..."
./hack/test-lint-locally.sh
</code></pre>
<h3 id="ide-integration"><a class="header" href="#ide-integration">IDE Integration</a></h3>
<p>Many IDEs can run these scripts as build tasks:</p>
<p><strong>VS Code</strong> (<code>.vscode/tasks.json</code>):</p>
<pre><code class="language-json">{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Test Lint Locally",
      "type": "shell", 
      "command": "./hack/test-lint-locally.sh",
      "group": "test",
      "presentation": {
        "echo": true,
        "reveal": "always",
        "focus": false,
        "panel": "shared"
      }
    }
  ]
}
</code></pre>
<h3 id="ci-cost-optimization"><a class="header" href="#ci-cost-optimization">CI Cost Optimization</a></h3>
<p>By testing locally first:</p>
<ul>
<li><strong>Reduce failed CI runs</strong> by ~80%</li>
<li><strong>Save GitHub Actions minutes</strong></li>
<li><strong>Faster feedback</strong> (local runs are often faster)</li>
<li><strong>Better debugging</strong> (local environment is easier to inspect)</li>
</ul>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>These local testing scripts allow you to:</p>
<p>‚úÖ <strong>Catch issues early</strong> before they reach GitHub Actions<br />
‚úÖ <strong>Save costs</strong> by reducing failed CI runs<br />
‚úÖ <strong>Debug faster</strong> with local environment access<br />
‚úÖ <strong>Test thoroughly</strong> with multiple approaches<br />
‚úÖ <strong>Iterate quickly</strong> during development</p>
<p>Start with the lint script for daily use, and gradually incorporate the full test suite for comprehensive validation before releases.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributing-to-virtrigaud"><a class="header" href="#contributing-to-virtrigaud">Contributing to VirtRigaud</a></h1>
<p>Thank you for your interest in contributing to VirtRigaud! This document provides guidelines and information for contributors.</p>
<h2 id="development-setup-1"><a class="header" href="#development-setup-1">Development Setup</a></h2>
<h3 id="prerequisites-8"><a class="header" href="#prerequisites-8">Prerequisites</a></h3>
<ul>
<li>Go 1.23+</li>
<li>Docker</li>
<li>Kubernetes cluster (kind, k3s, or remote)</li>
<li>kubectl</li>
<li>Helm 3.x</li>
<li>make</li>
</ul>
<h3 id="clone-and-setup"><a class="header" href="#clone-and-setup">Clone and Setup</a></h3>
<pre><code class="language-bash">git clone https://github.com/projectbeskar/virtrigaud.git
cd virtrigaud

# Install development dependencies
make dev-setup

# Install pre-commit hooks (optional but recommended)
pip install pre-commit
pre-commit install
</code></pre>
<h2 id="development-workflow"><a class="header" href="#development-workflow">Development Workflow</a></h2>
<h3 id="1-making-changes"><a class="header" href="#1-making-changes">1. Making Changes</a></h3>
<h4 id="api-changes"><a class="header" href="#api-changes">API Changes</a></h4>
<p>When modifying API types:</p>
<pre><code class="language-bash"># Edit API types
vim api/infra.virtrigaud.io/v1beta1/virtualmachine_types.go

# Generate code and sync CRDs
make generate
make sync-helm-crds

# Verify everything is in sync
make verify-helm-crds
</code></pre>
<h4 id="code-changes"><a class="header" href="#code-changes">Code Changes</a></h4>
<p>For other code changes:</p>
<pre><code class="language-bash"># Run tests
make test

# Lint code
make lint

# Format code
make fmt
</code></pre>
<h3 id="2-crd-management"><a class="header" href="#2-crd-management">2. CRD Management</a></h3>
<p><strong>Important</strong>: Always keep CRDs synchronized between <code>config/crd/bases/</code> and <code>charts/virtrigaud/crds/</code>.</p>
<pre><code class="language-bash"># After API changes, sync CRDs to Helm chart
make sync-helm-crds

# Verify sync before committing
make verify-helm-crds
</code></pre>
<h3 id="3-testing"><a class="header" href="#3-testing">3. Testing</a></h3>
<pre><code class="language-bash"># Unit tests
make test

# Integration tests (requires cluster)
make test-integration

# End-to-end tests
make test-e2e

# Test specific provider
make test-provider-vsphere
</code></pre>
<h3 id="4-local-development"><a class="header" href="#4-local-development">4. Local Development</a></h3>
<pre><code class="language-bash"># Deploy to local cluster
make dev-deploy

# Watch for changes and auto-reload
make dev-watch

# Clean up
make dev-clean
</code></pre>
<h2 id="contribution-guidelines"><a class="header" href="#contribution-guidelines">Contribution Guidelines</a></h2>
<h3 id="pull-request-process"><a class="header" href="#pull-request-process">Pull Request Process</a></h3>
<ol>
<li><strong>Fork and branch</strong>: Create a feature branch from <code>main</code></li>
<li><strong>Make changes</strong>: Follow the development workflow above</li>
<li><strong>Test thoroughly</strong>: Run all relevant tests</li>
<li><strong>Update docs</strong>: Update documentation if needed</li>
<li><strong>CRD sync</strong>: Ensure CRDs are synchronized (CI will verify)</li>
<li><strong>Submit PR</strong>: Create a pull request with clear description</li>
</ol>
<h3 id="pr-requirements"><a class="header" href="#pr-requirements">PR Requirements</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
All tests pass</li>
<li><input disabled="" type="checkbox"/>
CRDs are in sync (verified by CI)</li>
<li><input disabled="" type="checkbox"/>
Code is formatted (<code>make fmt</code>)</li>
<li><input disabled="" type="checkbox"/>
Code is linted (<code>make lint</code>)</li>
<li><input disabled="" type="checkbox"/>
Documentation updated if needed</li>
<li><input disabled="" type="checkbox"/>
Changelog entry added (for user-facing changes)</li>
</ul>
<h3 id="commit-message-format"><a class="header" href="#commit-message-format">Commit Message Format</a></h3>
<p>Use conventional commit format:</p>
<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;

[optional body]

[optional footer(s)]
</code></pre>
<p>Types:</p>
<ul>
<li><code>feat</code>: New feature</li>
<li><code>fix</code>: Bug fix</li>
<li><code>docs</code>: Documentation changes</li>
<li><code>style</code>: Code style changes</li>
<li><code>refactor</code>: Code refactoring</li>
<li><code>test</code>: Test changes</li>
<li><code>chore</code>: Maintenance tasks</li>
</ul>
<p>Examples:</p>
<pre><code>feat(vsphere): add graceful shutdown support
fix(crd): resolve powerState validation conflict
docs(upgrade): add CRD synchronization guide
</code></pre>
<h2 id="code-style"><a class="header" href="#code-style">Code Style</a></h2>
<h3 id="go-code"><a class="header" href="#go-code">Go Code</a></h3>
<ul>
<li>Follow standard Go conventions</li>
<li>Use <code>gofmt</code> and <code>golangci-lint</code></li>
<li>Add meaningful comments for exported functions</li>
<li>Include unit tests for new functionality</li>
</ul>
<h3 id="yamlkubernetes"><a class="header" href="#yamlkubernetes">YAML/Kubernetes</a></h3>
<ul>
<li>Use 2-space indentation</li>
<li>Follow Kubernetes API conventions</li>
<li>Add descriptions to CRD fields</li>
<li>Include examples in documentation</li>
</ul>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<ul>
<li>Use clear, concise language</li>
<li>Include code examples</li>
<li>Update both API docs and user guides</li>
<li>Test documentation examples</li>
</ul>
<h2 id="testing-1"><a class="header" href="#testing-1">Testing</a></h2>
<h3 id="unit-tests"><a class="header" href="#unit-tests">Unit Tests</a></h3>
<pre><code class="language-bash"># Run all unit tests
make test

# Run tests for specific package
go test ./internal/controller/...

# Run with coverage
make test-coverage
</code></pre>
<h3 id="integration-tests"><a class="header" href="#integration-tests">Integration Tests</a></h3>
<pre><code class="language-bash"># Requires running Kubernetes cluster
export KUBECONFIG=~/.kube/config
make test-integration
</code></pre>
<h3 id="provider-tests"><a class="header" href="#provider-tests">Provider Tests</a></h3>
<pre><code class="language-bash"># Test specific provider (requires infrastructure)
make test-provider-vsphere
make test-provider-libvirt
make test-provider-proxmox
</code></pre>
<h2 id="release-process"><a class="header" href="#release-process">Release Process</a></h2>
<h3 id="for-maintainers"><a class="header" href="#for-maintainers">For Maintainers</a></h3>
<ol>
<li>
<p><strong>Prepare release</strong>:</p>
<pre><code class="language-bash"># Ensure CRDs are synced
make sync-helm-crds

# Update version in charts
vim charts/virtrigaud/Chart.yaml

# Update changelog
vim CHANGELOG.md
</code></pre>
</li>
<li>
<p><strong>Create release</strong>:</p>
<pre><code class="language-bash">git tag v0.2.1
git push origin v0.2.1
</code></pre>
</li>
<li>
<p><strong>CI handles</strong>:</p>
<ul>
<li>Building and pushing images</li>
<li>Creating GitHub release</li>
<li>Publishing Helm charts</li>
<li>Generating CLI binaries</li>
</ul>
</li>
</ol>
<h2 id="common-issues-12"><a class="header" href="#common-issues-12">Common Issues</a></h2>
<h3 id="crd-sync-issues"><a class="header" href="#crd-sync-issues">CRD Sync Issues</a></h3>
<p>If you see ‚ÄúHelm chart CRDs are out of sync‚Äù:</p>
<pre><code class="language-bash"># Fix with
make sync-helm-crds

# Verify
make verify-helm-crds
</code></pre>
<h3 id="test-failures"><a class="header" href="#test-failures">Test Failures</a></h3>
<pre><code class="language-bash"># Clean and retry
make clean
make test

# For libvirt-related failures
export SKIP_LIBVIRT_TESTS=true
make test
</code></pre>
<h3 id="development-environment-1"><a class="header" href="#development-environment-1">Development Environment</a></h3>
<pre><code class="language-bash"># Reset development environment
make dev-clean
make dev-deploy

# Check logs
kubectl logs -n virtrigaud-system deployment/virtrigaud-manager
</code></pre>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<ul>
<li><strong>GitHub Issues</strong>: Bug reports and feature requests</li>
<li><strong>GitHub Discussions</strong>: Questions and community support</li>
<li><strong>Documentation</strong>: Check docs/ directory</li>
<li><strong>Code Review</strong>: Maintainers will provide feedback on PRs</li>
</ul>
<h2 id="recognition-1"><a class="header" href="#recognition-1">Recognition</a></h2>
<p>Contributors are recognized in:</p>
<ul>
<li>CHANGELOG.md for significant contributions</li>
<li>README.md contributors section</li>
<li>GitHub contributor graphs</li>
</ul>
<p>Thank you for contributing to VirtRigaud! üöÄ</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtrigaud-examples"><a class="header" href="#virtrigaud-examples">VirtRigaud Examples</a></h1>
<p>This directory contains comprehensive examples for VirtRigaud v0.2.3+, showcasing all features and capabilities.</p>
<h2 id="quick-start-examples-1"><a class="header" href="#quick-start-examples-1">Quick Start Examples</a></h2>
<h3 id="basic-examples"><a class="header" href="#basic-examples">Basic Examples</a></h3>
<ul>
<li><strong><a href="examples/complete-example.yaml">complete-example.yaml</a></strong> - Complete end-to-end example with v0.2.1 features</li>
<li><strong><a href="examples/vm-ubuntu-small.yaml">vm-ubuntu-small.yaml</a></strong> - Simple Ubuntu VM with graceful shutdown</li>
<li><strong><a href="examples/vmclass-small.yaml">vmclass-small.yaml</a></strong> - Basic VMClass with hardware version support</li>
</ul>
<h3 id="provider-examples"><a class="header" href="#provider-examples">Provider Examples</a></h3>
<ul>
<li><strong><a href="examples/provider-vsphere.yaml">provider-vsphere.yaml</a></strong> - Basic vSphere provider configuration</li>
<li><strong><a href="examples/provider-libvirt.yaml">provider-libvirt.yaml</a></strong> - Basic LibVirt provider configuration</li>
</ul>
<h3 id="resource-examples"><a class="header" href="#resource-examples">Resource Examples</a></h3>
<ul>
<li><strong><a href="examples/vmimage-ubuntu.yaml">vmimage-ubuntu.yaml</a></strong> - VM image configuration</li>
<li><strong><a href="examples/vmnetwork-app.yaml">vmnetwork-app.yaml</a></strong> - Network attachment configuration</li>
<li><strong><a href="examples/vm-adoption-example.yaml">vm-adoption-example.yaml</a></strong> - VM adoption with filters</li>
</ul>
<h2 id="v021-feature-examples"><a class="header" href="#v021-feature-examples">v0.2.1 Feature Examples</a></h2>
<h3 id="new-in-v021"><a class="header" href="#new-in-v021">New in v0.2.1</a></h3>
<ul>
<li><strong><a href="examples/v021-feature-showcase.yaml">v021-feature-showcase.yaml</a></strong> - <strong>üåü COMPREHENSIVE DEMO</strong> - All v0.2.1 features in one example</li>
<li><strong><a href="examples/graceful-shutdown-examples.yaml">graceful-shutdown-examples.yaml</a></strong> - OffGraceful shutdown configurations</li>
<li><strong><a href="examples/vsphere-hardware-versions.yaml">vsphere-hardware-versions.yaml</a></strong> - Hardware version management</li>
<li><strong><a href="examples/disk-sizing-examples.yaml">disk-sizing-examples.yaml</a></strong> - Disk size configuration tests</li>
</ul>
<h3 id="advanced-provider-examples"><a class="header" href="#advanced-provider-examples">Advanced Provider Examples</a></h3>
<ul>
<li><strong><a href="examples/vsphere-advanced-example.yaml">vsphere-advanced-example.yaml</a></strong> - Advanced vSphere with v0.2.1 features</li>
<li><strong><a href="examples/libvirt-advanced-example.yaml">libvirt-advanced-example.yaml</a></strong> - Advanced LibVirt configuration</li>
<li><strong><a href="examples/proxmox-complete-example.yaml">proxmox-complete-example.yaml</a></strong> - Complete Proxmox setup</li>
</ul>
<h3 id="multi-provider-examples"><a class="header" href="#multi-provider-examples">Multi-Provider Examples</a></h3>
<ul>
<li><strong><a href="examples/multi-provider-example.yaml">multi-provider-example.yaml</a></strong> - Multiple providers in one cluster</li>
<li><strong><a href="examples/libvirt-complete-example.yaml">libvirt-complete-example.yaml</a></strong> - Complete LibVirt deployment</li>
</ul>
<h2 id="v023-feature-summary"><a class="header" href="#v023-feature-summary">v0.2.3 Feature Summary</a></h2>
<h3 id="-vm-reconfiguration-vsphere-libvirt-proxmox"><a class="header" href="#-vm-reconfiguration-vsphere-libvirt-proxmox">üîß VM Reconfiguration (vSphere, Libvirt, Proxmox)</a></h3>
<pre><code class="language-yaml"># Online resource changes (vSphere, Proxmox)
# Offline changes (Libvirt - requires restart)
spec:
  vmClassRef: medium  # Change from small to medium
  powerState: "On"
</code></pre>
<h3 id="-async-task-tracking-vsphere-proxmox"><a class="header" href="#-async-task-tracking-vsphere-proxmox">üìã Async Task Tracking (vSphere, Proxmox)</a></h3>
<pre><code class="language-yaml"># Automatic tracking of long-running operations
# Real-time progress and error reporting
</code></pre>
<h3 id="-console-access-vsphere-libvirt"><a class="header" href="#-console-access-vsphere-libvirt">üñ•Ô∏è Console Access (vSphere, Libvirt)</a></h3>
<pre><code class="language-yaml"># Web console URLs automatically generated
status:
  consoleURL: "https://vcenter.example.com/ui/app/vm..."  # vSphere
  # or
  consoleURL: "vnc://libvirt-host:5900"  # Libvirt VNC
</code></pre>
<h3 id="-guest-agent-integration-proxmox"><a class="header" href="#-guest-agent-integration-proxmox">üåê Guest Agent Integration (Proxmox)</a></h3>
<pre><code class="language-yaml"># Accurate IP detection via QEMU guest agent
status:
  ipAddresses:
    - 192.168.1.100
    - fd00::1234:5678:9abc:def0
</code></pre>
<h3 id="-vm-cloning-vsphere"><a class="header" href="#-vm-cloning-vsphere">üì¶ VM Cloning (vSphere)</a></h3>
<pre><code class="language-yaml"># Full and linked clones with automatic snapshot handling
spec:
  vmImageRef: source-vm
  cloneType: linked  # or "full"
</code></pre>
<h3 id="-previous-features-v021"><a class="header" href="#-previous-features-v021">üîÑ Previous Features (v0.2.1)</a></h3>
<ul>
<li><strong>Graceful Shutdown</strong>: OffGraceful power state with VMware Tools</li>
<li><strong>Hardware Version Management</strong>: vSphere hardware version control</li>
<li><strong>Proper Disk Sizing</strong>: Correct disk allocation across providers</li>
<li><strong>Enhanced Lifecycle Management</strong>: postStart/preStop hooks</li>
</ul>
<h2 id="usage-patterns"><a class="header" href="#usage-patterns">Usage Patterns</a></h2>
<h3 id="testing-v023-features"><a class="header" href="#testing-v023-features">Testing v0.2.3 Features</a></h3>
<ol>
<li>
<p><strong>Test VM reconfiguration</strong>:</p>
<pre><code class="language-bash"># Change VM class to trigger reconfiguration
kubectl patch virtualmachine my-vm --type='merge' \
  -p='{"spec":{"vmClassRef":"medium"}}'

# Watch the reconfiguration process
kubectl get vm my-vm -w
</code></pre>
</li>
<li>
<p><strong>Access VM console</strong>:</p>
<pre><code class="language-bash"># Get console URL from VM status
kubectl get vm my-vm -o jsonpath='{.status.consoleURL}'

# For VNC (Libvirt): Use any VNC client
vncviewer $(kubectl get vm my-vm -o jsonpath='{.status.consoleURL}' | sed 's/vnc:\/\///')
</code></pre>
</li>
<li>
<p><strong>Monitor async tasks</strong> (vSphere, Proxmox):</p>
<pre><code class="language-bash"># Watch task progress in provider logs
kubectl logs -f deployment/virtrigaud-provider-vsphere
</code></pre>
</li>
<li>
<p><strong>Verify guest agent</strong> (Proxmox):</p>
<pre><code class="language-bash"># Check IP addresses from guest agent
kubectl get vm my-vm -o jsonpath='{.status.ipAddresses}'
</code></pre>
</li>
<li>
<p><strong>Test VM cloning</strong> (vSphere):</p>
<pre><code class="language-bash"># Create a clone of existing VM
kubectl apply -f - &lt;&lt;EOF
apiVersion: infra.virtrigaud.io/v1beta1
kind: VirtualMachine
metadata:
  name: web-server-clone
spec:
  vmClassRef: small
  vmImageRef: web-server-01
  cloneType: linked
EOF
</code></pre>
</li>
</ol>
<h3 id="development-workflow-1"><a class="header" href="#development-workflow-1">Development Workflow</a></h3>
<ol>
<li><strong>Choose base example</strong> based on your use case</li>
<li><strong>Customize</strong> provider, class, and VM specifications</li>
<li><strong>Test locally</strong> with your infrastructure</li>
<li><strong>Iterate</strong> based on your requirements</li>
</ol>
<h3 id="production-deployment"><a class="header" href="#production-deployment">Production Deployment</a></h3>
<ol>
<li><strong>Start with complete-example.yaml</strong></li>
<li><strong>Add security configurations</strong> from security/ subdirectory</li>
<li><strong>Configure secrets</strong> from secrets/ subdirectory</li>
<li><strong>Apply advanced patterns</strong> from advanced/ subdirectory</li>
</ol>
<h2 id="file-organization"><a class="header" href="#file-organization">File Organization</a></h2>
<pre><code>docs/examples/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ complete-example.yaml             # Complete setup guide
‚îú‚îÄ‚îÄ v021-feature-showcase.yaml        # üåü v0.2.1 comprehensive demo
‚îú‚îÄ‚îÄ vm-ubuntu-small.yaml             # Simple VM example
‚îú‚îÄ‚îÄ vmclass-small.yaml               # Basic VMClass
‚îú‚îÄ‚îÄ provider-*.yaml                  # Provider configurations
‚îú‚îÄ‚îÄ graceful-shutdown-examples.yaml  # OffGraceful demos
‚îú‚îÄ‚îÄ vsphere-hardware-versions.yaml   # Hardware version examples
‚îú‚îÄ‚îÄ disk-sizing-examples.yaml        # Disk sizing tests
‚îú‚îÄ‚îÄ advanced/                        # Complex scenarios
‚îú‚îÄ‚îÄ secrets/                         # Secret management
‚îî‚îÄ‚îÄ security/                        # Security configurations
</code></pre>
<h2 id="version-compatibility"><a class="header" href="#version-compatibility">Version Compatibility</a></h2>
<ul>
<li><strong>v0.2.3+</strong>: All examples with v0.2.3 features (Reconfigure, Clone, TaskStatus, ConsoleURL, Guest Agent)</li>
<li><strong>v0.2.2</strong>: Nested virtualization, TPM support, snapshot management</li>
<li><strong>v0.2.1</strong>: Graceful shutdown, hardware version, disk sizing fixes</li>
<li><strong>v0.2.0</strong>: Initial production-ready providers</li>
<li><strong>v0.1.x</strong>: Legacy examples in git history</li>
</ul>
<h2 id="need-help-1"><a class="header" href="#need-help-1">Need Help?</a></h2>
<ul>
<li>üìñ <strong>Documentation</strong>: <a href="examples/../README.html">../README.md</a></li>
<li>üöÄ <strong>Quick Start</strong>: <a href="examples/../getting-started/quickstart.html">../getting-started/quickstart.md</a></li>
<li>üîß <strong>CLI Tools</strong>: <a href="examples/../CLI.html">../CLI.md</a></li>
<li>üìã <strong>Upgrade Guide</strong>: <a href="examples/../UPGRADE.html">../UPGRADE.md</a></li>
<li>üèóÔ∏è <strong>Contributing</strong>: <a href="examples/../../CONTRIBUTING.html">../../CONTRIBUTING.md</a></li>
</ul>
<hr />
<p><strong>Pro Tip</strong>: Start with <code>v021-feature-showcase.yaml</code> to see all v0.2.1 capabilities in action! üöÄ</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/page-toc.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
